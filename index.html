<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-27T00:00:00Z">2024-03-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mini-Gemini: Mining the Potential of Multi-modality Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce Mini-Gemini, a simple and effective framework
enhancing multi-modality Vision Language Models (VLMs). Despite the
advancements in VLMs facilitating basic visual dialog and reasoning, a
performance gap persists compared to advanced models like GPT-4 and Gemini. We
try to narrow the gap by mining the potential of VLMs for better performance
and any-to-any workflow from three aspects, i.e., high-resolution visual
tokens, high-quality data, and VLM-guided generation. To enhance visual tokens,
we propose to utilize an additional visual encoder for high-resolution
refinement without increasing the visual token count. We further construct a
high-quality dataset that promotes precise image comprehension and
reasoning-based generation, expanding the operational scope of current VLMs. In
general, Mini-Gemini further mines the potential of VLMs and empowers current
frameworks with image understanding, reasoning, and generation simultaneously.
Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs)
from 2B to 34B. It is demonstrated to achieve leading performance in several
zero-shot benchmarks and even surpasses the developed private models. Code and
models are available at https://github.com/dvlab-research/MiniGemini.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at
  https://github.com/dvlab-research/MiniGemini</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth
  Estimation <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Patni, Aradhye Agarwal, Chetan Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the absence of parallax cues, a learning-based single image depth
estimation (SIDE) model relies heavily on shading and contextual cues in the
image. While this simplicity is attractive, it is necessary to train such
models on large and varied datasets, which are difficult to capture. It has
been shown that using embeddings from pre-trained foundational models, such as
CLIP, improves zero shot transfer in several applications. Taking inspiration
from this, in our paper we explore the use of global image priors generated
from a pre-trained ViT model to provide more detailed contextual information.
We argue that the embedding vector from a ViT model, pre-trained on a large
dataset, captures greater relevant information for SIDE than the usual route of
generating pseudo image captions, followed by CLIP based text embeddings. Based
on this idea, we propose a new SIDE model using a diffusion backbone which is
conditioned on ViT embeddings. Our proposed design establishes a new
state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of
0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on
KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to
0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model
trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)
over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,
18%, 45%, 9%) by ZoeDepth. The code is available at
https://github.com/Aradhye2002/EcoDepth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-form factuality in large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often generate content that contains factual
errors when responding to fact-seeking prompts on open-ended topics. To
benchmark a model's long-form factuality in open domains, we first use GPT-4 to
generate LongFact, a prompt set comprising thousands of questions spanning 38
topics. We then propose that LLM agents can be used as automated evaluators for
long-form factuality through a method which we call Search-Augmented Factuality
Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into
a set of individual facts and to evaluate the accuracy of each fact using a
multi-step reasoning process comprising sending search queries to Google Search
and determining whether a fact is supported by the search results. Furthermore,
we propose extending F1 score as an aggregated metric for long-form factuality.
To do so, we balance the percentage of supported facts in a response
(precision) with the percentage of provided facts relative to a hyperparameter
representing a user's preferred response length (recall).
  Empirically, we demonstrate that LLM agents can achieve superhuman rating
performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced
human annotators 72% of the time, and on a random subset of 100 disagreement
cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times
cheaper than human annotators. We also benchmark thirteen language models on
LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding
that larger language models generally achieve better long-form factuality.
LongFact, SAFE, and all experimental code are available at
https://github.com/google-deepmind/long-form-factuality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gamba: Marry Gaussian Splatting with Mamba for single view 3D
  reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuhong Shen, Xuanyu Yi, Zike Wu, Pan Zhou, Hanwang Zhang, Shuicheng Yan, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the challenge of efficiently reconstructing a 3D asset from a
single image with growing demands for automated 3D content creation pipelines.
Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural
Radiance Fields (NeRF). Despite their significant success, these approaches
encounter practical limitations due to lengthy optimization and considerable
memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D
reconstruction model from single-view images, emphasizing two main insights:
(1) 3D representation: leveraging a large number of 3D Gaussians for an
efficient 3D Gaussian splatting process; (2) Backbone design: introducing a
Mamba-based sequential network that facilitates context-dependent reasoning and
linear scalability with the sequence (token) length, accommodating a
substantial number of Gaussians. Gamba incorporates significant advancements in
data preprocessing, regularization design, and training methodologies. We
assessed Gamba against existing optimization-based and feed-forward 3D
generation approaches using the real-world scanned OmniObject3D dataset. Here,
Gamba demonstrates competitive generation capabilities, both qualitatively and
quantitatively, while achieving remarkable speed, approximately 0.6 second on a
single NVIDIA A100 GPU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImageNet-D: Benchmarking Neural Network Robustness on Diffusion
  Synthetic Object <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish rigorous benchmarks for visual perception robustness. Synthetic
images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific
type of evaluation over synthetic corruptions, backgrounds, and textures, yet
those robustness benchmarks are restricted in specified variations and have low
synthetic quality. In this work, we introduce generative model as a data source
for synthesizing hard images that benchmark deep models' robustness. Leveraging
diffusion models, we are able to generate images with more diversified
backgrounds, textures, and materials than any prior work, where we term this
benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a
significant accuracy drop to a range of vision models, from the standard ResNet
visual classifier to the latest foundation models like CLIP and MiniGPT-4,
significantly reducing their accuracy by up to 60\%. Our work suggests that
diffusion models can be an effective source to test vision models. The code and
dataset are available at https://github.com/chenshuang-zhang/imagenet_d.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superior Parallel Big Data Clustering through Competitive Stochastic
  Sample Size Optimization in Big-means 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rustam Mussabayev, Ravil Mussabayev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel K-means clustering algorithm, an advancement on
the conventional Big-means methodology. The proposed method efficiently
integrates parallel processing, stochastic sampling, and competitive
optimization to create a scalable variant designed for big data applications.
It addresses scalability and computation time challenges typically faced with
traditional techniques. The algorithm adjusts sample sizes dynamically for each
worker during execution, optimizing performance. Data from these sample sizes
are continually analyzed, facilitating the identification of the most efficient
configuration. By incorporating a competitive element among workers using
different sample sizes, efficiency within the Big-means algorithm is further
stimulated. In essence, the algorithm balances computational time and
clustering quality by employing a stochastic, competitive sampling strategy in
a parallel computing setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weidong Xie, Lun Luo, Nanfei Ye, Yi Ren, Shaoyi Du, Minhang Wang, Jintao Xu, Rui Ai, Weihao Gu, Xieyuanli Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Place recognition is an important task for robots and autonomous cars to
localize themselves and close loops in pre-built maps. While single-modal
sensor-based methods have shown satisfactory performance, cross-modal place
recognition that retrieving images from a point-cloud database remains a
challenging problem. Current cross-modal methods transform images into 3D
points using depth estimation for modality conversion, which are usually
computationally intensive and need expensive labeled data for depth
supervision. In this work, we introduce a fast and lightweight framework to
encode images and point clouds into place-distinctive descriptors. We propose
an effective Field of View (FoV) transformation module to convert point clouds
into an analogous modality as images. This module eliminates the necessity for
depth estimation and helps subsequent modules achieve real-time performance. We
further design a non-negative factorization-based encoder to extract mutually
consistent semantic features between point clouds and images. This encoder
yields more distinctive global descriptors for retrieval. Experimental results
on the KITTI dataset show that our proposed methods achieve state-of-the-art
performance while running in real time. Additional evaluation on the HAOMO
dataset covering a 17 km trajectory further shows the practical generalization
capabilities. We have released the implementation of our methods as open source
at: https://github.com/haomo-ai/ModaLink.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 11 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of subclinical atherosclerosis by image-based deep learning on
  chest x-ray 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guglielmo Gallone, Francesco Iodice, Alberto Presta, Davide Tore, Ovidio de Filippo, Michele Visciano, Carlo Alberto Barbano, Alessandro Serafini, Paola Gorrini, Alessandro Bruno, Walter Grosso Marra, James Hughes, Mario Iannaccone, Paolo Fonio, Attilio Fiandrotti, Alessandro Depaoli, Marco Grangetto, Gaetano Maria de Ferrari, Fabrizio D'Ascenzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aims. To develop a deep-learning based system for recognition of subclinical
atherosclerosis on a plain frontal chest x-ray. Methods and Results. A
deep-learning algorithm to predict coronary artery calcium (CAC) score (the
AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%
internal validation cohort) of primary prevention patients (58.4% male, median
age 63 [51-74] years) with available paired chest x-ray and chest computed
tomography (CT) indicated for any clinical reason and performed within 3
months. The CAC score calculated on chest CT was used as ground truth. The
model was validated on an temporally-independent cohort of 90 patients from the
same institution (external validation). The diagnostic accuracy of the AI-CAC
model assessed by the area under the curve (AUC) was the primary outcome.
Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.
AUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation
cohort and 0.77 in the external validation cohort. Sensitivity was consistently
above 92% in both cohorts. In the overall cohort (n=540), among patients with
AI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with
AI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events
(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to
accurately detect subclinical atherosclerosis on chest x-ray with elevated
sensitivity, and to predict ASCVD events with elevated negative predictive
value. Adoption of the AI-CAC model to refine CV risk stratification or as an
opportunistic screening tool requires prospective evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to European Heart Journal - Cardiovascular Imaging Added
  also the additional material 44 pages (30 main paper, 14 additional
  material), 14 figures (5 main manuscript, 9 additional material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Many-Objective Evolutionary Influence Maximization: Balancing Spread,
  Budget, Fairness, and Time <span class="chip">GECCO
  24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Influence Maximization (IM) problem seeks to discover the set of nodes in
a graph that can spread the information propagation at most. This problem is
known to be NP-hard, and it is usually studied by maximizing the influence
(spread) and, optionally, optimizing a second objective, such as minimizing the
seed set size or maximizing the influence fairness. However, in many practical
scenarios multiple aspects of the IM problem must be optimized at the same
time. In this work, we propose a first case study where several IM-specific
objective functions, namely budget, fairness, communities, and time, are
optimized on top of the maximization of influence and minimization of the seed
set size. To this aim, we introduce MOEIM (Many-Objective Evolutionary
Algorithm for Influence Maximization) a Multi-Objective Evolutionary Algorithm
(MOEA) based on NSGA-II incorporating graph-aware operators and a smart
initialization. We compare MOEIM in two experimental settings, including a
total of nine graph datasets, two heuristic methods, a related MOEA, and a
state-of-the-art Deep Learning approach. The experiments show that MOEIM
overall outperforms the competitors in most of the tested many-objective
settings. To conclude, we also investigate the correlation between the
objectives, leading to novel insights into the topic. The codebase is available
at https://github.com/eliacunegatti/MOEIM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Genetic and Evolutionary Computation Conference (GECCO
  24 Companion), July 14 18, 2024, Melbourne, VIC, Australia. ACM, New York,
  NY, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Learning Dynamics of Alignment with Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Im, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLMs) with human intentions has become a
critical task for safely deploying models in real-world systems. While existing
alignment approaches have seen empirical success, theoretically understanding
how these methods affect model behavior remains an open question. Our work
provides an initial attempt to theoretically analyze the learning dynamics of
human preference alignment. We formally show how the distribution of preference
datasets influences the rate of model updates and provide rigorous guarantees
on the training accuracy. Our theory also reveals an intricate phenomenon where
the optimization is prone to prioritizing certain behaviors with higher
preference distinguishability. We empirically validate our findings on
contemporary LLMs and alignment tasks, reinforcing our theoretical insights and
shedding light on considerations for future alignment approaches. Disclaimer:
This paper contains potentially offensive text; reader discretion is advised.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Manufacturing Quality Prediction Models through the
  Integration of Explainability Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker, Arnaud Gotlieb, Ricardo Knoblauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents a method that utilizes explainability techniques to
amplify the performance of machine learning (ML) models in forecasting the
quality of milling processes, as demonstrated in this paper through a
manufacturing use case. The methodology entails the initial training of ML
models, followed by a fine-tuning phase where irrelevant features identified
through explainability methods are eliminated. This procedural refinement
results in performance enhancements, paving the way for potential reductions in
manufacturing costs and a better understanding of the trained ML models. This
study highlights the usefulness of explainability techniques in both explaining
and optimizing predictive models in the manufacturing realm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Model Checking of Stochastic Reinforcement Learning
  Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a method to verify stochastic reinforcement learning (RL)
policies. This approach is compatible with any RL algorithm as long as the
algorithm and its corresponding environment collectively adhere to the Markov
property. In this setting, the future state of the environment should depend
solely on its current state and the action executed, independent of any
previous states or actions. Our method integrates a verification technique,
referred to as model checking, with RL, leveraging a Markov decision process, a
trained RL policy, and a probabilistic computation tree logic (PCTL) formula to
build a formal model that can be subsequently verified via the model checker
Storm. We demonstrate our method's applicability across multiple benchmarks,
comparing it to baseline methods called deterministic safety estimates and
naive monolithic model checking. Our results show that our method is suited to
verify stochastic RL policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Learning for Deep Causal Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing models that can answer questions of the form "How would $x$ change
if $y$ had been $z$?" is fundamental for advancing medical image analysis.
Training causal generative models that address such counterfactual questions,
though, currently requires that all relevant variables have been observed and
that corresponding labels are available in training data. However, clinical
data may not have complete records for all patients and state of the art causal
generative models are unable to take full advantage of this. We thus develop,
for the first time, a semi-supervised deep causal generative model that
exploits the causal relationships between variables to maximise the use of all
available data. We explore this in the setting where each sample is either
fully labelled or fully unlabelled, as well as the more clinically realistic
case of having different labels missing for each sample. We leverage techniques
from causal inference to infer missing values and subsequently generate
realistic counterfactuals, even for samples with incomplete labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations in Large Vision-Language Models with
  Instruction Contrastive Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintong Wang, Jingheng Pan, Liang Ding, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) are increasingly adept at generating
contextually detailed and coherent responses from visual inputs. However, their
application in multimodal decision-making and open-ended generation is hindered
by a notable rate of hallucinations, where generated text inaccurately
represents the visual contents. To address this issue, this paper introduces
the Instruction Contrastive Decoding (ICD) method, a novel approach designed to
reduce hallucinations during LVLM inference. Our method is inspired by our
observation that what we call disturbance instructions significantly exacerbate
hallucinations in multimodal fusion modules. ICD contrasts distributions from
standard and instruction disturbance, thereby increasing alignment uncertainty
and effectively subtracting hallucinated concepts from the original
distribution. Through comprehensive experiments on discriminative benchmarks
(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that
ICD significantly mitigates both object-level and attribute-level
hallucinations. Moreover, our method not only addresses hallucinations but also
significantly enhances the general perception and recognition capabilities of
LVLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable
  Transient-Free 3D reconstruction from Satellite Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camille Billouard, Dawa Derksen, Emmanuelle Sarrazin, Bruno Vallet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current stereo-vision pipelines produce high accuracy 3D reconstruction when
using multiple pairs or triplets of satellite images. However, these pipelines
are sensitive to the changes between images that can occur as a result of
multi-date acquisitions. Such variations are mainly due to variable shadows,
reflexions and transient objects (cars, vegetation). To take such changes into
account, Neural Radiance Fields (NeRF) have recently been applied to multi-date
satellite imagery. However, Neural methods are very compute-intensive, taking
dozens of hours to learn, compared with minutes for standard stereo-vision
pipelines. Following the ideas of Instant Neural Graphics Primitives we propose
to use an efficient sampling strategy and multi-resolution hash encoding to
accelerate the learning. Our model, Satellite Neural Graphics Primitives
(SAT-NGP) decreases the learning time to 15 minutes while maintaining the
quality of the 3D reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 1 table; Accepted to International Geoscience and
  Remote Sensing Symposium (IGARSS) 2024; Code available at
  https://github.com/Ellimac0/SAT-NGP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning with Orthonormal Anchors (CLOA) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on addressing the instability issues prevalent in
contrastive learning, specifically examining the InfoNCE loss function and its
derivatives. We reveal a critical observation that these loss functions exhibit
a restrictive behavior, leading to a convergence phenomenon where embeddings
tend to merge into a singular point. This "over-fusion" effect detrimentally
affects classification accuracy in subsequent supervised-learning tasks.
Through theoretical analysis, we demonstrate that embeddings, when equalized or
confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In
response to this challenge, our research introduces an innovative strategy that
leverages the same or fewer labeled data than typically used in the fine-tuning
phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to
disentangle embedding clusters, significantly enhancing the distinctiveness of
each embedding while simultaneously ensuring their aggregation into dense,
well-defined clusters. Our method demonstrates remarkable improvements with
just a fraction of the conventional label requirements, as evidenced by our
results on CIFAR10 and CIFAR100 datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annolid: Annotate, Segment, and Track Anything You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Yang, Thomas A. Cleland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annolid is a deep learning-based software package designed for the
segmentation, labeling, and tracking of research targets within video files,
focusing primarily on animal behavior analysis. Based on state-of-the-art
instance segmentation methods, Annolid now harnesses the Cutie video object
segmentation model to achieve resilient, markerless tracking of multiple
animals from single annotated frames, even in environments in which they may be
partially or entirely concealed by environmental features or by one another.
Our integration of Segment Anything and Grounding-DINO strategies additionally
enables the automatic masking and segmentation of recognizable animals and
objects by text command, removing the need for manual annotation. Annolid's
comprehensive approach to object segmentation flexibly accommodates a broad
spectrum of behavior analysis applications, enabling the classification of
diverse behavioral states such as freezing, digging, pup huddling, and social
interactions in addition to the tracking of animals and their body parts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransFusion: Contrastive Learning with <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel framework, TransFusion, designed to make the
process of contrastive learning more analytical and explainable. TransFusion
consists of attention blocks whose softmax being replaced by ReLU, and its
final block's weighted-sum operation is truncated to leave the adjacency matrix
as the output. The model is trained by minimizing the Jensen-Shannon Divergence
between its output and the target affinity matrix, which indicates whether each
pair of samples belongs to the same or different classes. The main contribution
of TransFusion lies in defining a theoretical limit for answering two
fundamental questions in the field: the maximum level of data augmentation and
the minimum batch size required for effective contrastive learning.
Furthermore, experimental results indicate that TransFusion successfully
extracts features that isolate clusters from complex real-world data, leading
to improved classification accuracy in downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aiming for Relevance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bar Eini Porat, Danny Eytan, Uri Shalit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vital signs are crucial in intensive care units (ICUs). They are used to
track the patient's state and to identify clinically significant changes.
Predicting vital sign trajectories is valuable for early detection of adverse
events. However, conventional machine learning metrics like RMSE often fail to
capture the true clinical relevance of such predictions. We introduce novel
vital sign prediction performance metrics that align with clinical contexts,
focusing on deviations from clinical norms, overall trends, and trend
deviations. These metrics are derived from empirical utility curves obtained in
a previous study through interviews with ICU clinicians. We validate the
metrics' usefulness using simulated and real clinical datasets (MIMIC and
eICU). Furthermore, we employ these metrics as loss functions for neural
networks, resulting in models that excel in predicting clinically significant
events. This research paves the way for clinically relevant machine learning
model evaluation and optimization, promising to improve ICU patient care. 10
pages, 9 figures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, AMIA Informatics 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INEXA: Interactive and Explainable Process Model Abstraction Through
  Object-Centric Process Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janik-Vasily Benzin, Gyunam Park, Juergen Mangler, Stefanie Rinderle-Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process events are recorded by multiple information systems at different
granularity levels. Based on the resulting event logs, process models are
discovered at different granularity levels, as well. Events stored at a
fine-grained granularity level, for example, may hinder the discovered process
model to be displayed due the high number of resulting model elements. The
discovered process model of a real-world manufacturing process, for example,
consists of 1,489 model elements and over 2,000 arcs. Existing process model
abstraction techniques could help reducing the size of the model, but would
disconnect it from the underlying event log. Existing event abstraction
techniques do neither support the analysis of mixed granularity levels, nor
interactive exploration of a suitable granularity level. To enable the
exploration of discovered process models at different granularity levels, we
propose INEXA, an interactive, explainable process model abstraction method
that keeps the link to the event log. As a starting point, INEXA aggregates
large process models to a "displayable" size, e.g., for the manufacturing use
case to a process model with 58 model elements. Then, the process analyst can
explore granularity levels interactively, while applied abstractions are
automatically traced in the event log for explainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic
  Learning over Low-power Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Fu, Gaolei Li, Jun Wu, Jianhua Li, Xi Lin, Kai Zhou, Yuchen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated neuromorphic learning (FedNL) leverages event-driven spiking neural
networks and federated learning frameworks to effectively execute intelligent
analysis tasks over amounts of distributed low-power devices but also perform
vulnerability to poisoning attacks. The threat of backdoor attacks on
traditional deep neural networks typically comes from time-invariant data.
However, in FedNL, unknown threats may be hidden in time-varying spike signals.
In this paper, we start to explore a novel vulnerability of FedNL-based systems
with the concept of time division multiplexing, termed Spikewhisper, which
allows attackers to evade detection as much as possible, as multiple malicious
clients can imperceptibly poison with different triggers at different
timeslices. In particular, the stealthiness of Spikewhisper is derived from the
time-domain divisibility of global triggers, in which each malicious client
pastes only one local trigger to a certain timeslice in the neuromorphic
sample, and also the polarity and motion of each local trigger can be
configured by attackers. Extensive experiments based on two different
neuromorphic datasets demonstrate that the attack success rate of Spikewispher
is higher than the temporally centralized attacks. Besides, it is validated
that the effect of Spikewispher is sensitive to the trigger duration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in
  Instructional Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Zare, Yulei Niu, Hammad Ayyubi, Shih-fu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedure Planning in instructional videos entails generating a sequence of
action steps based on visual observations of the initial and target states.
Despite the rapid progress in this task, there remain several critical
challenges to be solved: (1) Adaptive procedures: Prior works hold an
unrealistic assumption that the number of action steps is known and fixed,
leading to non-generalizable models in real-world scenarios where the sequence
length varies. (2) Temporal relation: Understanding the step temporal relation
knowledge is essential in producing reasonable and executable plans. (3)
Annotation cost: Annotating instructional videos with step-level labels (i.e.,
timestamp) or sequence-level labels (i.e., action category) is demanding and
labor-intensive, limiting its generalizability to large-scale datasets.In this
work, we propose a new and practical setting, called adaptive procedure
planning in instructional videos, where the procedure length is not fixed or
pre-determined. To address these challenges we introduce Retrieval-Augmented
Planner (RAP) model. Specifically, for adaptive procedures, RAP adaptively
determines the conclusion of actions using an auto-regressive model
architecture. For temporal relation, RAP establishes an external memory module
to explicitly retrieve the most relevant state-action pairs from the training
videos and revises the generated procedures. To tackle high annotation cost,
RAP utilizes a weakly-supervised learning manner to expand the training dataset
to other task-relevant, unannotated videos by generating pseudo labels for
action steps. Experiments on CrossTask and COIN benchmarks show the superiority
of RAP over traditional fixed-length models, establishing it as a strong
baseline solution for adaptive procedure planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote
  Sensing Image Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Run Shao, Zhaoyang Zhang, Chao Tao, Yunsheng Zhang, Chengli Peng, Haifeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tokenizer, as one of the fundamental components of large models, has long
been overlooked or even misunderstood in visual tasks. One key factor of the
great comprehension power of the large language model is that natural language
tokenizers utilize meaningful words or subwords as the basic elements of
language. In contrast, mainstream visual tokenizers, represented by patch-based
methods such as Patch Embed, rely on meaningless rectangular patches as basic
elements of vision, which cannot serve as effectively as words or subwords in
language. Starting from the essence of the tokenizer, we defined semantically
independent regions (SIRs) for vision. We designed a simple HOmogeneous visual
tOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception
Module (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity,
the OPM splits the image into 4*4 pixel seeds and then utilizes the attention
mechanism to perceive SIRs. The OVM employs cross-attention to merge seeds
within the same SIR. To achieve adaptability, the OVM defines a variable number
of learnable vectors as cross-attention queries, allowing for the adjustment of
token quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19
classification dataset, and GID5 segmentation dataset for sparse and dense
tasks. The results demonstrate that the visual tokens obtained by HOOK
correspond to individual objects, which demonstrates homogeneity. HOOK
outperformed Patch Embed by 6\% and 10\% in the two tasks and achieved
state-of-the-art performance compared to the baselines used for comparison.
Compared to Patch Embed, which requires more than one hundred tokens for one
image, HOOK requires only 6 and 8 tokens for sparse and dense tasks,
respectively, resulting in efficiency improvements of 1.5 to 2.8 times. The
code is available at https://github.com/GeoX-Lab/Hook.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Informed Graph Neural Networks for Water Distribution Systems <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inaam Ashraf, Janine Strotherm, Luca Hermes, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Water distribution systems (WDS) are an integral part of critical
infrastructure which is pivotal to urban development. As 70% of the world's
population will likely live in urban environments in 2050, efficient simulation
and planning tools for WDS play a crucial role in reaching UN's sustainable
developmental goal (SDG) 6 - "Clean water and sanitation for all". In this
realm, we propose a novel and efficient machine learning emulator, more
precisely, a physics-informed deep learning (DL) model, for hydraulic state
estimation in WDS. Using a recursive approach, our model only needs a few graph
convolutional neural network (GCN) layers and employs an innovative algorithm
based on message passing. Unlike conventional machine learning tasks, the model
uses hydraulic principles to infer two additional hydraulic state features in
the process of reconstructing the available ground truth feature in an
unsupervised manner. To the best of our knowledge, this is the first DL
approach to emulate the popular hydraulic simulator EPANET, utilizing no
additional information. Like most DL models and unlike the hydraulic simulator,
our model demonstrates vastly faster emulation times that do not increase
drastically with the size of the WDS. Moreover, we achieve high accuracy on the
ground truth and very similar results compared to the hydraulic simulator as
demonstrated through experiments on five real-world WDS datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper with the same title published at
  Proceedings of the AAAI Conference on Artificial Intelligence 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Zhao, Zhuomin Chai, Xun Jiang, Yibo Lin, Runsheng Wang, Ru Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  IR drop on the power delivery network (PDN) is closely related to PDN's
configuration and cell current consumption. As the integrated circuit (IC)
design is growing larger, dynamic IR drop simulation becomes computationally
unaffordable and machine learning based IR drop prediction has been explored as
a promising solution. Although CNN-based methods have been adapted to IR drop
prediction task in several works, the shortcomings of overlooking PDN
configuration is non-negligible. In this paper, we consider not only how to
properly represent cell-PDN relation, but also how to model IR drop following
its physical nature in the feature aggregation procedure. Thus, we propose a
novel graph structure, PDNGraph, to unify the representations of the PDN
structure and the fine-grained cell-PDN relation. We further propose a
dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN
branches to favorably capture the above features during the learning process.
Several key designs are presented to make the dynamic IR drop prediction highly
effective and interpretable. We are the first work to apply graph structure to
deep-learning based dynamic IR drop prediction method. Experiments show that
PDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3%
reduction in prediction error and achieves 545x speedup compared to the
commercial tool, which demonstrates the superiority of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Architecture Search for Sentence Classification with <span class="highlight-title">BERT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Sarah Schröder, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre training of language models on large text corpora is common practice in
Natural Language Processing. Following, fine tuning of these models is
performed to achieve the best results on a variety of tasks. In this paper we
question the common practice of only adding a single output layer as a
classification head on top of the network. We perform an AutoML search to find
architectures that outperform the current single layer at only a small compute
cost. We validate our classification architecture on a variety of NLP
benchmarks from the GLUE dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siang Chen, Wei Tang, Pengwei Xie, Wenming Yang, Guijin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fast and robust object grasping in clutter is a crucial component of
robotics. Most current works resort to the whole observed point cloud for 6-Dof
grasp generation, ignoring the guidance information excavated from global
semantics, thus limiting high-quality grasp generation and real-time
performance. In this work, we show that the widely used heatmaps are
underestimated in the efficiency of 6-Dof grasp generation. Therefore, we
propose an effective local grasp generator combined with grasp heatmaps as
guidance, which infers in a global-to-local semantic-to-point way.
Specifically, Gaussian encoding and the grid-based strategy are applied to
predict grasp heatmaps as guidance to aggregate local points into graspable
regions and provide global semantic information. Further, a novel non-uniform
anchor sampling mechanism is designed to improve grasp accuracy and diversity.
Benefiting from the high-efficiency encoding in the image space and focusing on
points in local graspable regions, our framework can perform high-quality grasp
detection in real-time and achieve state-of-the-art results. In addition, real
robot experiments demonstrate the effectiveness of our method with a success
rate of 94% and a clutter completion rate of 100%. Our code is available at
https://github.com/THU-VCLab/HGGD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extensive results on GraspNet-1B dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Path Towards Legal Autonomy: An interoperable and explainable approach
  to extracting, transforming, loading and computing legal information using
  large language models, expert systems and Bayesian networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Constant, Hannes Westermann, Bryan Wilson, Alex Kiefer, Ines Hipolito, Sylvain Pronovost, Steven Swanson, Mahault Albarracin, Maxwell J. D. Ramstead
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal autonomy - the lawful activity of artificial intelligence agents - can
be achieved in one of two ways. It can be achieved either by imposing
constraints on AI actors such as developers, deployers and users, and on AI
resources such as data, or by imposing constraints on the range and scope of
the impact that AI agents can have on the environment. The latter approach
involves encoding extant rules concerning AI driven devices into the software
of AI agents controlling those devices (e.g., encoding rules about limitations
on zones of operations into the agent software of an autonomous drone device).
This is a challenge since the effectivity of such an approach requires a method
of extracting, loading, transforming and computing legal information that would
be both explainable and legally interoperable, and that would enable AI agents
to reason about the law. In this paper, we sketch a proof of principle for such
a method using large language models (LLMs), expert legal systems known as
legal decision paths, and Bayesian networks. We then show how the proposed
method could be applied to extant regulation in matters of autonomous cars,
such as the California Vehicle Code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Behavior-Based Recommendation System for E-commerce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Barzegar Nozari, Mahdi Divsalar, Sepehr Akbarzadeh Abkenar, Mohammadreza Fadavi Amiri, Ali Divsalar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of existing recommender systems rely on user ratings, which are
limited by the lack of user collaboration and the sparsity problem. To address
these issues, this study proposes a behavior-based recommender system that
leverages customers' natural behaviors, such as browsing and clicking, on
e-commerce platforms. The proposed recommendation system involves clustering
active customers, determining neighborhoods, collecting similar users,
calculating product reputation based on similar users, and recommending
high-reputation products. To overcome the complexity of customer behaviors and
traditional clustering methods, an unsupervised clustering approach based on
product categories is developed to enhance the recommendation methodology. This
study makes notable contributions in several aspects. Firstly, a groundbreaking
behavior-based recommendation methodology is developed, incorporating customer
behavior to generate accurate and tailored recommendations leading to improved
customer satisfaction and engagement. Secondly, an original unsupervised
clustering method, focusing on product categories, enables more precise
clustering and facilitates accurate recommendations. Finally, an approach to
determine neighborhoods for active customers within clusters is established,
ensuring grouping of customers with similar behavioral patterns to enhance
recommendation accuracy and relevance. The proposed recommendation methodology
and clustering method contribute to improved recommendation performance,
offering valuable insights for researchers and practitioners in the field of
e-commerce recommendation systems. Additionally, the proposed method
outperforms benchmark methods in experiments conducted using a behavior dataset
from the well-known e-commerce site Alibaba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Line Search Methods for Large Scale Neural Network Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent studies, line search methods have shown significant improvements in
the performance of traditional stochastic gradient descent techniques,
eliminating the need for a specific learning rate schedule. In this paper, we
identify existing issues in state-of-the-art line search methods, propose
enhancements, and rigorously evaluate their effectiveness. We test these
methods on larger datasets and more complex data domains than before.
Specifically, we improve the Armijo line search by integrating the momentum
term from ADAM in its search direction, enabling efficient large-scale
training, a task that was previously prone to failure using Armijo line search
methods. Our optimization approach outperforms both the previous Armijo
implementation and tuned learning rate schedules for Adam. Our evaluation
focuses on Transformers and CNNs in the domains of NLP and image data. Our work
is publicly available as a Python package, which provides a hyperparameter free
Pytorch optimizer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faster Convergence for <span class="highlight-title">Transformer</span> Fine-tuning with Line Search Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Leonardo Galli, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that line search methods greatly increase performance
of traditional stochastic gradient descent methods on a variety of datasets and
architectures [1], [2]. In this work we succeed in extending line search
methods to the novel and highly popular Transformer architecture and dataset
domains in natural language processing. More specifically, we combine the
Armijo line search with the Adam optimizer and extend it by subdividing the
networks architecture into sensible units and perform the line search
separately on these local units. Our optimization method outperforms the
traditional Adam optimizer and achieves significant performance improvements
for small data sets or small training budgets, while performing equal or better
for other tested cases. Our work is publicly available as a python package,
which provides a hyperparameter-free pytorch optimizer that is compatible with
arbitrary network architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Employing Weather Forecast Data as Input to the Estimation of
  Evapotranspiration by Deep Neural Network Models <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro J. Vaz, Gabriela Schütz, Carlos Guerrero, Pedro J. S. Cardoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reference Evapotranspiration (ET0) is a key parameter for designing smart
irrigation scheduling, since it is related by a coefficient to the water needs
of a crop. The United Nations Food and Agriculture Organization, proposed a
standard method for ET0 computation (FAO56PM), based on the parameterization of
the Penman-Monteith equation, that is widely adopted in the literature. To
compute ET0 using the FAO56-PM method, four main weather parameters are needed:
temperature, humidity, wind, and solar radiation (SR). One way to make daily
ET0 estimations for future days is to use freely available weather forecast
services (WFSs), where many meteorological parameters are estimated up to the
next 15 days. A problem with this method is that currently, SR is not provided
as a free forecast parameter on most of those online services or, normally,
such forecasts present a financial cost penalty. For this reason, several ET0
estimation models using machine and deep learning were developed and presented
in the literature, that use as input features a reduced set of carefully
selected weather parameters, that are compatible with common freely available
WFSs. However, most studies on this topic have only evaluated model performance
using data from weather stations (WSs), without considering the effect of using
weather forecast data. In this study, the performance of authors' previous
models is evaluated when using weather forecast data from two online WFSs, in
the following scenarios: (i) direct ET0 estimation by an ANN model, and (ii)
estimate SR by ANN model, and then use that estimation for ET0 computation,
using the FAO56-PM method. Employing data collected from two WFSs and a WS
located in Vale do Lobo, Portugal, the latter approach achieved the best
result, with a coefficient of determination (R2) ranging between 0.893 and
0.667, when considering forecasts up to 15 days.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A partial version of the work submitted to ESRE/INTERNATIONAL
  CONFERENCE ON ENVIRONMENTAL SCIENCES AND RENEWABLE ENERGY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing EEG Signals from Event-Related Potential Paradigms with
  Conditional Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guido Klein, Pierre Guetschel, Gianluigi Silvestri, Michael Tangermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity in the brain-computer interface field can be alleviated through
the use of generative models, specifically diffusion models. While diffusion
models have previously been successfully applied to electroencephalogram (EEG)
data, existing models lack flexibility w.r.t.~sampling or require alternative
representations of the EEG data. To overcome these limitations, we introduce a
novel approach to conditional diffusion models that utilizes classifier-free
guidance to directly generate subject-, session-, and class-specific EEG data.
In addition to commonly used metrics, domain-specific metrics are employed to
evaluate the specificity of the generated samples. The results indicate that
the proposed model can generate EEG data that resembles real data for each
subject, session, and class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to 9th Graz BCI conference, 6 pages, 3 figures, first
  figure is split into two subfigures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain
  Adaptive Segmentation of 3D Point Clouds <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhimin Yuan, Wankang Zeng, Yanfei Su, Weiquan Liu, Ming Cheng, Yulan Guo, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to
annotating new domains. Self-training is a competitive approach for this task,
but its performance is limited by different sensor sampling patterns (i.e.,
variations in point density) and incomplete training strategies. In this work,
we propose a density-guided translator (DGT), which translates point density
between domains, and integrates it into a two-stage self-training pipeline
named DGT-ST. First, in contrast to existing works that simultaneously conduct
data generation and feature/output alignment within unstable adversarial
training, we employ the non-learnable DGT to bridge the domain gap at the input
level. Second, to provide a well-initialized model for self-training, we
propose a category-level adversarial network in stage one that utilizes the
prototype to prevent negative transfer. Finally, by leveraging the designs
above, a domain-mixed self-training method with source-aware consistency loss
is proposed in stage two to narrow the domain gap further. Experiments on two
synthetic-to-real segmentation tasks (SynLiDAR $\rightarrow$ semanticKITTI and
SynLiDAR $\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms
state-of-the-art methods, achieving 9.4$\%$ and 4.3$\%$ mIoU improvements,
respectively. Code is available at \url{https://github.com/yuan-zm/DGT-ST}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Ionova, Jan Kristof Behrens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assembly processes involving humans and robots are challenging scenarios
because the individual activities and access to shared workspace have to be
coordinated. Fixed robot programs leave no room to diverge from a fixed
protocol. Working on such a process can be stressful for the user and lead to
ineffective behavior or failure. We propose a novel approach of online
constraint-based scheduling in a reactive execution control framework
facilitating behavior trees called CoBOS. This allows the robot to adapt to
uncertain events such as delayed activity completions and activity selection
(by the human). The user will experience less stress as the robotic coworkers
adapt their behavior to best complement the human-selected activities to
complete the common task. In addition to the improved working conditions, our
algorithm leads to increased efficiency, even in highly uncertain scenarios. We
evaluate our algorithm using a probabilistic simulation study with 56000
experiments. We outperform all baselines by a margin of 4-10%. Initial real
robot experiments using a Franka Emika Panda robot and human tracking based on
HTC Vive VR gloves look promising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in
  Resource-Constrained CPS and IoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Hu, Jinhang Zuo, Alanis Zhao, Bob Iannucci, Carlee Joe-Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) emerge as a promising solution to harness distributed
and diverse environmental data by leveraging prior knowledge to understand the
complicated temporal and spatial correlations within heterogeneous datasets.
Unlike distributed learning frameworks such as federated learning, which often
struggle with multimodal data, FMs can transform diverse inputs into
embeddings. This process facilitates the integration of information from
various modalities and the application of prior learning to new domains.
However, deploying FMs in resource-constrained edge systems poses significant
challenges. To this end, we introduce CoRAST, a novel learning framework that
utilizes FMs for enhanced analysis of distributed, correlated heterogeneous
data. Utilizing a server-based FM, CoRAST can exploit existing environment
information to extract temporal, spatial, and cross-modal correlations among
sensor data. This enables CoRAST to offer context-aware insights for localized
client tasks through FM-powered global representation learning. Our evaluation
on real-world weather dataset demonstrates CoRAST's ability to exploit
correlated heterogeneous data through environmental representation learning to
reduce the forecast errors by up to 50.3% compared to the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted and to be published in 2024 IEEE International Workshop on
  Foundation Models for Cyber-Physical Systems & Internet of Things (FMSys)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Mitsouras, Eleftherios Tsonis, Paraskevi Tzouveli, Athanasios Voulodimos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable performance in text-to-image
synthesis, producing realistic and high resolution images that faithfully
adhere to the corresponding text-prompts. Despite their great success, they
still fall behind in sketch-to-image synthesis tasks, where in addition to
text-prompts, the spatial layout of the generated images has to closely follow
the outlines of certain reference sketches. Employing an MLP latent edge
predictor to guide the spatial layout of the synthesized image by predicting
edge maps at each denoising step has been recently proposed. Despite yielding
promising results, the pixel-wise operation of the MLP does not take into
account the spatial layout as a whole, and demands numerous denoising
iterations to produce satisfactory images, leading to time inefficiency. To
this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge
predictor, which is capable of efficiently capturing both local and global
features, as well as spatial correlations between pixels. Moreover, we propose
the addition of a sketch simplification network that offers the user the choice
of preprocessing and simplifying input sketches for enhanced outputs. The
experimental results, corroborated by user feedback, demonstrate that our
proposed U-Net latent edge predictor leads to more realistic results, that are
better aligned with the spatial outlines of the reference sketches, while
drastically reducing the number of required denoising steps and, consequently,
the overall execution time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, Christopher D. Manning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance
on a wide variety of biomedical NLP tasks. However, these models have hundreds
of billions of parameters, are computationally expensive to run, require users
to send their input data over the internet, and are trained on unknown data
sources. Can smaller, more targeted models compete? To address this question,
we build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive
model trained exclusively on PubMed abstracts and full articles. When
fine-tuned, BioMedLM can produce strong multiple-choice biomedical
question-answering results competitive with much larger models, such as
achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical
Genetics exam. BioMedLM can also be fine-tuned to produce useful answers to
patient questions on medical topics. This demonstrates that smaller models can
potentially serve as transparent, privacy-preserving, economical and
environmentally friendly foundations for particular NLP applications, such as
in biomedicine. The model is available on the Hugging Face Hub:
https://huggingface.co/stanford-crfm/BioMedLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is
  Critical for Semi-supervised Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Wu, Junbiao Pang, Baochang Zhang, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) is a practical challenge in computer vision.
Pseudo-label (PL) methods, e.g., FixMatch and FreeMatch, obtain the State Of
The Art (SOTA) performances in SSL. These approaches employ a
threshold-to-pseudo-label (T2L) process to generate PLs by truncating the
confidence scores of unlabeled data predicted by the self-training method.
However, self-trained models typically yield biased and high-variance
predictions, especially in the scenarios when a little labeled data are
supplied. To address this issue, we propose a lightweight channel-based
ensemble method to effectively consolidate multiple inferior PLs into the
theoretically guaranteed unbiased and low-variance one. Importantly, our
approach can be readily extended to any SSL framework, such as FixMatch or
FreeMatch. Experimental results demonstrate that our method significantly
outperforms state-of-the-art techniques on CIFAR10/100 in terms of
effectiveness and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering
  Using a VLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stimulated by the sophisticated reasoning capabilities of recent Large
Language Models (LLMs), a variety of strategies for bridging video modality
have been devised. A prominent strategy involves Video Language Models
(VideoLMs), which train a learnable interface with video data to connect
advanced vision encoders with LLMs. Recently, an alternative strategy has
surfaced, employing readily available foundation models, such as VideoLMs and
LLMs, across multiple stages for modality bridging. In this study, we introduce
a simple yet novel strategy where only a single Vision Language Model (VLM) is
utilized. Our starting point is the plain insight that a video comprises a
series of images, or frames, interwoven with temporal information. The essence
of video comprehension lies in adeptly managing the temporal aspects along with
the spatial details of each frame. Initially, we transform a video into a
single composite image by arranging multiple frames in a grid layout. The
resulting single image is termed as an image grid. This format, while
maintaining the appearance of a solitary image, effectively retains temporal
information within the grid structure. Therefore, the image grid approach
enables direct application of a single high-performance VLM without
necessitating any video-data training. Our extensive experimental analysis
across ten zero-shot video question answering benchmarks, including five
open-ended and five multiple-choice benchmarks, reveals that the proposed Image
Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out
of ten benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/imagegridworth/IG-VLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Models for Relevance Judgments in Legal Case
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengjie Ma, Chong Chen, Qi Chu, Jiaxin Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collecting relevant judgments for legal case retrieval is a challenging and
time-consuming task. Accurately judging the relevance between two legal cases
requires a considerable effort to read the lengthy text and a high level of
domain expertise to extract Legal Facts and make juridical judgments. With the
advent of advanced large language models, some recent studies have suggested
that it is promising to use LLMs for relevance judgment. Nonetheless, the
method of employing a general large language model for reliable relevance
judgments in legal case retrieval is yet to be thoroughly explored. To fill
this research gap, we devise a novel few-shot workflow tailored to the relevant
judgment of legal cases. The proposed workflow breaks down the annotation
process into a series of stages, imitating the process employed by human
annotators and enabling a flexible integration of expert reasoning to enhance
the accuracy of relevance judgments. By comparing the relevance judgments of
LLMs and human experts, we empirically show that we can obtain reliable
relevance judgments with the proposed workflow. Furthermore, we demonstrate the
capacity to augment existing legal case retrieval models through the synthesis
of data generated by the large language model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Colour and Brush Stroke Pattern Recognition in Abstract Art using
  Modified Deep Convolutional Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srinitish Srinivasan, Varenya Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract Art is an immensely popular, discussed form of art that often has
the ability to depict the emotions of an artist. Many researchers have made
attempts to study abstract art in the form of edge detection, brush stroke and
emotion recognition algorithms using machine and deep learning. This papers
describes the study of a wide distribution of abstract paintings using
Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and
reproduce a distribution enabling researchers and scientists to effectively
explore and study the generated image space. However, the challenge lies in
developing an efficient GAN architecture that overcomes common training
pitfalls. This paper addresses this challenge by introducing a modified-DCGAN
(mDCGAN) specifically designed for high-quality artwork generation. The
approach involves a thorough exploration of the modifications made, delving
into the intricate workings of DCGANs, optimisation techniques, and
regularisation methods aimed at improving stability and realism in art
generation enabling effective study of generated patterns. The proposed mDCGAN
incorporates meticulous adjustments in layer configurations and architectural
choices, offering tailored solutions to the unique demands of art generation
while effectively combating issues like mode collapse and gradient vanishing.
Further this paper explores the generated latent space by performing random
walks to understand vector relationships between brush strokes and colours in
the abstract art space and a statistical analysis of unstable outputs after a
certain period of GAN training and compare its significant difference. These
findings validate the effectiveness of the proposed approach, emphasising its
potential to revolutionise the field of digital art generation and digital art
ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Wu, Velibor Bojkovic, Bin Gu, Kun Suo, Kai Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient
computing compared with Artificial Neural Networks (ANNs), closely mirroring
biological neural processes. However, this potential comes with inherent
challenges in directly training SNNs through spatio-temporal backpropagation --
stemming from the temporal dynamics of spiking neurons and their discrete
signal processing -- which necessitates alternative ways of training, most
notably through ANN-SNN conversion. In this work, we introduce a lightweight
Forward Temporal Bias Correction (FTBC) technique, aimed at enhancing
conversion accuracy without the computational overhead. We ground our method on
provided theoretical findings that through proper temporal bias calibration the
expected error of ANN-SNN conversion can be reduced to be zero after each time
step. We further propose a heuristic algorithm for finding the temporal bias
only in the forward pass, thus eliminating the computational burden of
backpropagation and we evaluate our method on CIFAR-10/100 and ImageNet
datasets, achieving a notable increase in accuracy on all datasets. Codes are
released at a GitHub repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Multi-modal Models are Good Class-Incremental Learners <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xusheng Cao, Haori Lu, Linlan Huang, Xialei Liu, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic
forgetting caused by the classifier's bias towards the current task has long
posed a significant challenge. It is mainly caused by the characteristic of
discriminative models. With the growing popularity of the generative
multi-modal models, we would explore replacing discriminative models with
generative ones for CIL. However, transitioning from discriminative to
generative models requires addressing two key challenges. The primary challenge
lies in transferring the generated textual information into the classification
of distinct categories. Additionally, it requires formulating the task of CIL
within a generative framework. To this end, we propose a novel generative
multi-modal model (GMM) framework for class-incremental learning. Our approach
directly generates labels for images using an adapted generative model. After
obtaining the detailed text, we use a text encoder to extract text features and
employ feature matching to determine the most similar label as the
classification prediction. In the conventional CIL settings, we achieve
significantly better results in long-sequence task scenarios. Under the
Few-shot CIL setting, we have improved by at least 14\% accuracy over all the
current state-of-the-art methods with significantly less forgetting. Our code
is available at \url{https://github.com/DoubleClass/GMM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Attributed Text Generation of Large Language Models via
  Preference Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongfang Li, Zetian Sun, Baotian Hu, Zhenyu Liu, Xinshuo Hu, Xuebo Liu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have been widely adopted in natural language
processing, yet they face the challenge of generating unreliable content.
Recent works aim to reduce misinformation and hallucinations by resorting to
attribution as a means to provide evidence (i.e., citations). However, current
attribution methods usually focus on the retrieval stage and automatic
evaluation that neglect mirroring the citation mechanisms in human scholarly
writing to bolster credibility. In this paper, we address these challenges by
modelling the attribution task as preference learning and introducing an
Automatic Preference Optimization (APO) framework. First, we create a curated
collection for post-training with 6,330 examples by collecting and filtering
from existing datasets. Second, considering the high cost of labelling
preference data, we further propose an automatic method to synthesize
attribution preference data resulting in 95,263 pairs. Moreover, inspired by
the human citation process, we further propose a progressive preference
optimization method by leveraging fine-grained information. Extensive
experiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate
that APO achieves state-of-the-art citation F1 with higher answer quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 15 tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining
  Useful Life Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzai Ye, Li Feng, Jianlan Guo, Yuqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating the Remaining Useful Life (RUL) of lithium-ion
batteries is crucial for maintaining the safe and stable operation of
rechargeable battery management systems. However, this task is often
challenging due to the complex temporal dynamics involved. Recently,
attention-based networks, such as Transformers and Informer, have been the
popular architecture in time series forecasting. Despite their effectiveness,
these models with abundant parameters necessitate substantial training time to
unravel temporal patterns. To tackle these challenges, we propose a simple
MLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which
is an architecture based exclusively on multi-layer perceptrons (MLPs),
extracting information by mixing operations along both intra-patch and
inter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer
comprises parallel dual-head mixer layers: the intra-patch mixing MLP,
capturing local temporal patterns in the short-term period, and the inter-patch
mixing MLP, capturing global temporal patterns in the long-term period.
Notably, to address the varying importance of features in RUL prediction, we
introduce a weighted loss function in the MLP-Mixer-based architecture, marking
the first time such an approach has been employed. Our experiments demonstrate
that IIP-Mixer achieves competitive performance in battery RUL prediction,
outperforming other popular time-series frameworks
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salwa Mostafa, Mateus P. Mota, Alvaro Valcarce, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the problem of supporting Industrial Internet of Things user
equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and
random traffic arrival. A deep reinforcement learning (DRL) based centralized
dynamic scheduler for time-frequency resources is proposed to learn how to
schedule the available communication resources among the IIoT UEs. The proposed
scheduler leverages an RL framework to adapt to the dynamic changes in the
wireless communication system and traffic arrivals. Moreover, a graph-based
reduction scheme is proposed to reduce the state and action space of the RL
framework to allow fast convergence and a better learning strategy. Simulation
results demonstrate the effectiveness of the proposed intelligent scheduler in
guaranteeing the expressed intent of IIoT UEs compared to several traditional
scheduling schemes, such as round-robin, semi-static, and heuristic approaches.
The proposed scheduler also outperforms the contention-free and
contention-based schemes in maximizing the number of successfully computed
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Diverse Agricultural Data for Vision-Based Farming
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikolaj Cieslak, Umabharathi Govindarajan, Alejandro Garcia, Anuradha Chandrashekar, Torsten Hädrich, Aleksander Mendoza-Drosik, Dominik L. Michels, Sören Pirk, Chia-Chun Fu, Wojciech Pałubicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a specialized procedural model for generating synthetic
agricultural scenes, focusing on soybean crops, along with various weeds. This
model is capable of simulating distinct growth stages of these plants, diverse
soil conditions, and randomized field arrangements under varying lighting
conditions. The integration of real-world textures and environmental factors
into the procedural generation process enhances the photorealism and
applicability of the synthetic data. Our dataset includes 12,000 images with
semantic labels, offering a comprehensive resource for computer vision tasks in
precision agriculture, such as semantic segmentation for autonomous weed
control. We validate our model's effectiveness by comparing the synthetic data
against real agricultural images, demonstrating its potential to significantly
augment training data for machine learning models in agriculture. This approach
not only provides a cost-effective solution for generating high-quality,
diverse data but also addresses specific needs in agricultural vision tasks
that are not fully covered by general-purpose models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal
  Holes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanmoy Bandyopadhyay, Suman Kundu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection and analysis of the solar coronal holes (CHs) is an important
field of study in the domain of solar physics. Mainly, it is required for the
proper prediction of the geomagnetic storms which directly or indirectly affect
various space and ground-based systems. For the detection of CHs till date, the
solar scientist depends on manual hand-drawn approaches. However, with the
advancement of image processing technologies, some automated image segmentation
methods have been used for the detection of CHs. In-spite of this, fast and
accurate detection of CHs are till a major issues. Here in this work, a novel
quantum computing-based fast fuzzy c-mean technique has been developed for fast
detection of the CHs region. The task has been carried out in two stages, in
first stage the solar image has been segmented using a quantum computing based
fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted
out from the segmented image based on image morphological operation. In the
work, quantum computing has been used to optimize the cost function of the fast
fuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm
(QAOA) has been used to optimize the quadratic part of the cost function. The
proposed method has been tested for 193 \AA{} SDO/AIA full-disk solar image
datasets and has been compared with the existing techniques. The outcome shows
the comparable performance of the proposed method with the existing one within
a very lesser time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions
  with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxing Peng, Xusen Guo, Xianda Chen, Meixin Zhu, Kehua Chen,  Hao,  Yang, Xuesong Wang, Yinhai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To ensure safe driving in dynamic environments, autonomous vehicles should
possess the capability to accurately predict the lane change intentions of
surrounding vehicles in advance and forecast their future trajectories.
Existing motion prediction approaches have ample room for improvement,
particularly in terms of long-term prediction accuracy and interpretability. In
this paper, we address these challenges by proposing LC-LLM, an explainable
lane change prediction model that leverages the strong reasoning capabilities
and self-explanation abilities of Large Language Models (LLMs). Essentially, we
reformulate the lane change prediction task as a language modeling problem,
processing heterogeneous driving scenario information in natural language as
prompts for input into the LLM and employing a supervised fine-tuning technique
to tailor the LLM specifically for our lane change prediction task. This allows
us to utilize the LLM's powerful common sense reasoning abilities to understand
complex interactive information, thereby improving the accuracy of long-term
predictions. Furthermore, we incorporate explanatory requirements into the
prompts in the inference stage. Therefore, our LC-LLM model not only can
predict lane change intentions and trajectories but also provides explanations
for its predictions, enhancing the interpretability. Extensive experiments on
the large-scale highD dataset demonstrate the superior performance and
interpretability of our LC-LLM in lane change prediction task. To the best of
our knowledge, this is the first attempt to utilize LLMs for predicting lane
change behavior. Our study shows that LLMs can encode comprehensive interaction
information for driving behavior understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mAL<span class="highlight-title">BERT</span>: Is a Compact Multilingual <span class="highlight-title">BERT</span> Model Still Worth It? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christophe Servan, Sahar Ghannay, Sophie Rosset
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within the current trend of Pretained Language Models (PLM), emerge more and
more criticisms about the ethical andecological impact of such models. In this
article, considering these critical remarks, we propose to focus on
smallermodels, such as compact models like ALBERT, which are more ecologically
virtuous than these PLM. However,PLMs enable huge breakthroughs in Natural
Language Processing tasks, such as Spoken and Natural LanguageUnderstanding,
classification, Question--Answering tasks. PLMs also have the advantage of
being multilingual, and,as far as we know, a multilingual version of compact
ALBERT models does not exist. Considering these facts, wepropose the free
release of the first version of a multilingual compact ALBERT model,
pre-trained using Wikipediadata, which complies with the ethical aspect of such
a language model. We also evaluate the model against classicalmultilingual PLMs
in classical NLP tasks. Finally, this paper proposes a rare study on the
subword tokenizationimpact on language performances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2024 Joint International Conference on Computational Linguistics,
  Language Resources and Evaluation, May 2024, Torino, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLMs Converse Formally? Automatically Assessing LLMs in Translating
  and Interpreting Formal Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rushang Karia, Daksh Dobhal, Daniel Bramblett, Pulkit Verma, Siddharth Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stakeholders often describe system requirements using natural language which
are then converted to formal syntax by a domain-expert leading to increased
design costs. This paper assesses the capabilities of Large Language Models
(LLMs) in converting between natural language descriptions and formal
specifications. Existing work has evaluated the capabilities of LLMs in
generating formal syntax such as source code but such experiments are typically
hand-crafted and use problems that are likely to be in the training set of
LLMs, and often require human-annotated datasets. We propose an approach that
can use two copies of an LLM in conjunction with an off-the-shelf verifier to
automatically evaluate its translation abilities without any additional human
input. Our approach generates formal syntax using language grammars to
automatically generate a dataset. We conduct an empirical evaluation to measure
the accuracy of this translation task and show that SOTA LLMs cannot adequately
solve this task, limiting their current utility in the design of complex
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chinese Offensive Language Detection:Current Status and Future
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunze Xiao, Houda Bouamor, Wajdi Zaghouani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the considerable efforts being made to monitor and regulate
user-generated content on social media platforms, the pervasiveness of
offensive language, such as hate speech or cyberbullying, in the digital space
remains a significant challenge. Given the importance of maintaining a
civilized and respectful online environment, there is an urgent and growing
need for automatic systems capable of detecting offensive speech in real time.
However, developing effective systems for processing languages such as Chinese
presents a significant challenge, owing to the language's complex and nuanced
nature, which makes it difficult to process automatically. This paper provides
a comprehensive overview of offensive language detection in Chinese, examining
current benchmarks and approaches and highlighting specific models and tools
for addressing the unique challenges of detecting offensive language in this
complex language. The primary objective of this survey is to explore the
existing techniques and identify potential avenues for further research that
can address the cultural and linguistic complexities of Chinese.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A thermodynamically consistent physics-informed deep learning material
  model for short fiber/polymer nanocomposites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Betim Bahtiri, Behrouz Arash, Sven Scheffler, Maximilian Jux, Raimund Rolfes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a physics-informed deep learning (PIDL)-based constitutive
model for investigating the viscoelastic-viscoplastic behavior of short
fiber-reinforced nanoparticle-filled epoxies under various ambient conditions.
The deep-learning model is trained to enforce thermodynamic principles, leading
to a thermodynamically consistent constitutive model. To accomplish this, a
long short-term memory network is combined with a feed-forward neural network
to predict internal variables required for characterizing the internal
dissipation of the nanocomposite materials. In addition, another feed-forward
neural network is used to indicate the free-energy function, which enables
defining the thermodynamic state of the entire system. The PIDL model is
initially developed for the three-dimensional case by generating synthetic data
from a classical constitutive model. The model is then trained by extracting
the data directly from cyclic loading-unloading experimental tests. Numerical
examples show that the PIDL model can accurately predict the mechanical
behavior of epoxy-based nanocomposites for different volume fractions of fibers
and nanoparticles under various hygrothermal conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2305.08102</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Recommender System for NFT Collectibles with Item Feature <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjoo Choi, Seonmi Kim, Yejin Kim, Youngbin Lee, Joohwan Hong, Yongjae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have been actively studied and applied in various domains
to deal with information overload. Although there are numerous studies on
recommender systems for movies, music, and e-commerce, comparatively less
attention has been paid to the recommender system for NFTs despite the
continuous growth of the NFT market. This paper presents a recommender system
for NFTs that utilizes a variety of data sources, from NFT transaction records
to external item features, to generate precise recommendations that cater to
individual preferences. We develop a data-efficient graph-based recommender
system to efficiently capture the complex relationship between each item and
users and generate node(item) embeddings which incorporate both node feature
information and graph structure. Furthermore, we exploit inputs beyond
user-item interactions, such as image feature, text feature, and price feature.
Numerical experiments verify the performance of the graph-based recommender
system improves significantly after utilizing all types of item features as
side information, thereby outperforming all other baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the AAAI 2023 Bridge on AI for Financial Services
  (https://sites.google.com/view/aaai-ai-fin/home)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrinivas Ramasubramanian, Harsh Rangwani, Sho Takemori, Kunal Samanta, Yuhei Umeda, Venkatesh Babu Radhakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise in internet usage has led to the generation of massive amounts of
data, resulting in the adoption of various supervised and semi-supervised
machine learning algorithms, which can effectively utilize the colossal amount
of data to train models. However, before deploying these models in the real
world, these must be strictly evaluated on performance measures like worst-case
recall and satisfy constraints such as fairness. We find that current
state-of-the-art empirical techniques offer sub-optimal performance on these
practical, non-decomposable performance objectives. On the other hand, the
theoretical techniques necessitate training a new model from scratch for each
performance objective. To bridge the gap, we propose SelMix, a selective
mixup-based inexpensive fine-tuning technique for pre-trained models, to
optimize for the desired objective. The core idea of our framework is to
determine a sampling distribution to perform a mixup of features between
samples from particular classes such that it optimizes the given objective. We
comprehensively evaluate our technique against the existing empirical and
theoretically principled methods on standard benchmark datasets for imbalanced
classification. We find that proposed SelMix fine-tuning significantly improves
the performance for various practical non-decomposable objectives across
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 SpotLight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic
  Communication Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhang Zheng, Kechao Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to semantic communication tasks rely on the knowledge
of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these
methods necessitate training under specific SNR conditions, entailing
considerable time and computational resources. In this paper, we propose GeNet,
a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at
combating noise, thereby facilitating Task-Oriented Communication (TOC). We
propose a novel approach where we first transform the input data image into
graph structures. Then we leverage a GNN-based encoder to extract semantic
information from the source data. This extracted semantic information is then
transmitted through the channel. At the receiver's end, a GNN-based decoder is
utilized to reconstruct the relevant semantic information from the source data
for TOC. Through experimental evaluation, we show GeNet's effectiveness in
anti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet's
performance by varying the number of nodes, revealing its versatility as a new
paradigm for semantic communication. Additionally, we show GeNet's robustness
to geometric transformations by testing it with different rotation angles,
without resorting to data augmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Recalibration of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Lisa Li, Urvashi Khandelwal, Kelvin Guu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has uncovered promising ways to extract well-calibrated
confidence estimates from language models (LMs), where the model's confidence
score reflects how likely it is to be correct. However, while LMs may appear
well-calibrated over broad distributions, this often hides significant
miscalibration within narrower slices (e.g., systemic over-confidence in math
can balance out systemic under-confidence in history, yielding perfect
calibration in aggregate). To attain well-calibrated confidence estimates for
any slice of a distribution, we propose a new framework for few-shot
slice-specific recalibration. Specifically, we train a recalibration model that
takes in a few unlabeled examples from any given slice and predicts a curve
that remaps confidence scores to be more accurate for that slice. Our trained
model can recalibrate for arbitrary new slices, without using any labeled data
from that slice. This enables us to identify domain-specific confidence
thresholds above which the LM's predictions can be trusted, and below which it
should abstain. Experiments show that our few-shot recalibrator consistently
outperforms existing calibration methods, for instance improving calibration
error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identification and Uses of Deep Learning Backbones via Pattern Mining <span class="chip">SDM24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Livanos, Ian Davidson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is extensively used in many areas of data mining as a black-box
method with impressive results. However, understanding the core mechanism of
how deep learning makes predictions is a relatively understudied problem. Here
we explore the notion of identifying a backbone of deep learning for a given
group of instances. A group here can be instances of the same class or even
misclassified instances of the same class. We view each instance for a given
group as activating a subset of neurons and attempt to find a subgraph of
neurons associated with a given concept/group. We formulate this problem as a
set cover style problem and show it is intractable and presents a highly
constrained integer linear programming (ILP) formulation. As an alternative, we
explore a coverage-based heuristic approach related to pattern mining, and show
it converges to a Pareto equilibrium point of the ILP formulation.
Experimentally we explore these backbones to identify mistakes and improve
performance, explanation, and visualization. We demonstrate application-based
results using several challenging data sets, including Bird Audio Detection
(BAD) Challenge and Labeled Faces in the Wild (LFW), as well as the classic
MNIST data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, published SIAM SDM24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSF-GAN: DownStream Feedback Generative Adversarial Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oriel Perets, Nadav Rappoport
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utility and privacy are two crucial measurements of the quality of synthetic
tabular data. While significant advancements have been made in privacy
measures, generating synthetic samples with high utility remains challenging.
To enhance the utility of synthetic samples, we propose a novel architecture
called the DownStream Feedback Generative Adversarial Network (DSF-GAN). This
approach incorporates feedback from a downstream prediction model during
training to augment the generator's loss function with valuable information.
Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of
synthetic samples. To evaluate our method, we tested it using two popular
datasets. Our experiments demonstrate improved model performance when training
on synthetic samples generated by DSF-GAN, compared to those generated by the
same GAN architecture without feedback. The evaluation was conducted on the
same validation set comprising real samples. All code and datasets used in this
research will be made openly available for ease of reproduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Generative Class Incremental Learning Performance with Model
  Forgetting Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taro Togo, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a novel approach to Generative Class Incremental Learning
(GCIL) by introducing the forgetting mechanism, aimed at dynamically managing
class information for better adaptation to streaming data. GCIL is one of the
hot topics in the field of computer vision, and this is considered one of the
crucial tasks in society, specifically the continual learning of generative
models. The ability to forget is a crucial brain function that facilitates
continual learning by selectively discarding less relevant information for
humans. However, in the field of machine learning models, the concept of
intentionally forgetting has not been extensively investigated. In this study
we aim to bridge this gap by incorporating the forgetting mechanisms into GCIL,
thereby examining their impact on the models' ability to learn in continual
learning. Through our experiments, we have found that integrating the
forgetting mechanisms significantly enhances the models' performance in
acquiring new knowledge, underscoring the positive role that strategic
forgetting plays in the process of continual learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manipulating Neural Path Planners via Slight Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikang Xiong, Suresh Jagannathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven neural path planners are attracting increasing interest in the
robotics community. However, their neural network components typically come as
black boxes, obscuring their underlying decision-making processes. Their
black-box nature exposes them to the risk of being compromised via the
insertion of hidden malicious behaviors. For example, an attacker may hide
behaviors that, when triggered, hijack a delivery robot by guiding it to a
specific (albeit wrong) destination, trapping it in a predefined region, or
inducing unnecessary energy expenditure by causing the robot to repeatedly
circle a region. In this paper, we propose a novel approach to specify and
inject a range of hidden malicious behaviors, known as backdoors, into neural
path planners. Our approach provides a concise but flexible way to define these
behaviors, and we show that hidden behaviors can be triggered by slight
perturbations (e.g., inserting a tiny unnoticeable object), that can
nonetheless significantly compromise their integrity. We also discuss potential
techniques to identify these backdoors aimed at alleviating such risks. We
demonstrate our approach on both sampling-based and search-based neural path
planners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual representation learning has been a cornerstone in computer vision,
evolving from supervised learning with human-annotated labels to aligning
image-text pairs from the Internet. Despite recent advancements in multi-modal
large language models (MLLMs), the visual representations they rely on, such as
CLIP embeddings, often lack access to external world knowledge critical for
real-world visual reasoning. In this work, we propose Visual Table, a novel
visual representation tailored for MLLMs. It provides hierarchical text
descriptions of holistic visual scenes, consisting of a scene description and
multiple object-centric descriptions that encompass categories, attributes, and
knowledge at instance level. We further develop a scalable generator for visual
table generation and train it on small-scale annotations from GPT4V. Extensive
evaluations demonstrate that, with generated visual tables as additional visual
representations, our model can consistently outperform the state-of-the-art
(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone
visual representations, our model can closely match or even beat the SOTA MLLMs
that are built on CLIP visual embeddings. Our code is available at
https://github.com/LaVi-Lab/Visual-Table.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/LaVi-Lab/Visual-Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Conversational Question Answering with Fine-Grained
  Retrieval-Augmentation and Self-Check 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linhao Ye, Zhikai Lei, Jianghao Yin, Qin Chen, Jie Zhou, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) aims to generate more reliable and
accurate responses, by augmenting large language models (LLMs) with the
external vast and dynamic knowledge. Most previous work focuses on using RAG
for single-round question answering, while how to adapt RAG to the complex
conversational setting wherein the question is interdependent on the preceding
context is not well studied. In this paper, we propose a conversation-level RAG
approach, which incorporates fine-grained retrieval augmentation and self-check
for conversational question answering (CQA). In particular, our approach
consists of three components, namely conversational question refiner,
fine-grained retriever and self-check based response generator, which work
collaboratively for question understanding and relevant information acquisition
in conversational settings. Extensive experiments demonstrate the great
advantages of our approach over the state-of-the-art baselines. Moreover, we
also release a Chinese CQA dataset with new features including reformulated
question, extracted keyword, retrieved paragraphs and their helpfulness, which
facilitates further researches in RAG enhanced CQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,
  Reconstruction, and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruikai Cui, Weizhe Liu, Weixuan Sun, Senbo Wang, Taizhang Shang, Yang Li, Xibin Song, Han Yan, Zhennan Wu, Shenzhou Chen, Hongdong Li, Pan Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D shape generation aims to produce innovative 3D content adhering to
specific conditions and constraints. Existing methods often decompose 3D shapes
into a sequence of localized components, treating each element in isolation
without considering spatial consistency. As a result, these approaches exhibit
limited versatility in 3D data representation and shape generation, hindering
their ability to generate highly diverse 3D shapes that comply with the
specified constraints. In this paper, we introduce a novel spatial-aware 3D
shape generation framework that leverages 2D plane representations for enhanced
3D shape modeling. To ensure spatial coherence and reduce memory usage, we
incorporate a hybrid shape representation technique that directly learns a
continuous signed distance field representation of the 3D shape using
orthogonal 2D planes. Additionally, we meticulously enforce spatial
correspondences across distinct planes using a transformer-based autoencoder
structure, promoting the preservation of spatial relationships in the generated
3D shapes. This yields an algorithm that consistently outperforms
state-of-the-art 3D shape generation methods on various tasks, including
unconditional shape generation, multi-modal shape completion, single-view
reconstruction, and text-to-shape synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Need Consultants for Reasoning: Becoming an Expert
  in a Complex Human System Through Behavior Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuwen Wang, Shirong Zeng, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), in conjunction with various reasoning
reinforcement methodologies, have demonstrated remarkable capabilities
comparable to humans in fields such as mathematics, law, coding, common sense,
and world knowledge. In this paper, we delve into the reasoning abilities of
LLMs within complex human systems. We propose a novel reasoning framework,
termed ``Mosaic Expert Observation Wall'' (MEOW) exploiting
generative-agents-based simulation technique. In the MEOW framework, simulated
data are utilized to train an expert model concentrating ``experience'' about a
specific task in each independent time of simulation. It is the accumulated
``experience'' through the simulation that makes for an expert on a task in a
complex human system. We conduct the experiments within a communication game
that mirrors real-world security scenarios. The results indicate that our
proposed methodology can cooperate with existing methodologies to enhance the
reasoning abilities of LLMs in complex human systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Transformer</span>-Based Framework for Payload Malware Detection and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As malicious cyber threats become more sophisticated in breaching computer
networks, the need for effective intrusion detection systems (IDSs) becomes
crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced
to allow IDSs analyze the content of network packets, providing more context
for identifying potential threats. IDSs traditionally rely on using
anomaly-based and signature-based detection techniques to detect unrecognized
and suspicious activity. Deep learning techniques have shown great potential in
DPI for IDSs due to their efficiency in learning intricate patterns from the
packet content being transmitted through the network. In this paper, we propose
a revolutionary DPI algorithm based on transformers adapted for the purpose of
detecting malicious traffic with a classifier head. Transformers learn the
complex content of sequence data and generalize them well to similar scenarios
thanks to their self-attention mechanism. Our proposed method uses the raw
payload bytes that represent the packet contents and is deployed as
man-in-the-middle. The payload bytes are used to detect malicious packets and
classify their types. Experimental results on the UNSW-NB15 and CIC-IOT23
datasets demonstrate that our transformer-based model is effective in
distinguishing malicious from benign traffic in the test dataset, attaining an
average accuracy of 79\% using binary classification and 72\% on the
multi-classification experiment, both using solely payload bytes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Two-Dimensional to Three-Dimensional Environment with Q-Learning:
  Modeling Autonomous Navigation with Reinforcement Learning and no Libraries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ergon Cugler de Moraes Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) algorithms have become indispensable tools in
artificial intelligence, empowering agents to acquire optimal decision-making
policies through interactions with their environment and feedback mechanisms.
This study explores the performance of RL agents in both two-dimensional (2D)
and three-dimensional (3D) environments, aiming to research the dynamics of
learning across different spatial dimensions. A key aspect of this
investigation is the absence of pre-made libraries for learning, with the
algorithm developed exclusively through computational mathematics. The
methodological framework centers on RL principles, employing a Q-learning agent
class and distinct environment classes tailored to each spatial dimension. The
research aims to address the question: How do reinforcement learning agents
adapt and perform in environments of varying spatial dimensions, particularly
in 2D and 3D settings? Through empirical analysis, the study evaluates agents'
learning trajectories and adaptation processes, revealing insights into the
efficacy of RL algorithms in navigating complex, multi-dimensional spaces.
Reflections on the findings prompt considerations for future research,
particularly in understanding the dynamics of learning in higher-dimensional
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Models for Fuzzy String Matching in Political
  Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fuzzy string matching remains a key issue when political scientists combine
data from different sources. Existing matching methods invariably rely on
string distances, such as Levenshtein distance and cosine similarity. As such,
they are inherently incapable of matching strings that refer to the same entity
with different names such as ''JP Morgan'' and ''Chase Bank'', ''DPRK'' and
''North Korea'', ''Chuck Fleischmann (R)'' and ''Charles Fleischmann (R)''. In
this letter, we propose to use large language models to entirely sidestep this
problem in an easy and intuitive manner. Extensive experiments show that our
proposed methods can improve the state of the art by as much as 39% in terms of
average precision while being substantially easier and more intuitive to use by
political scientists. Moreover, our results are robust against various
temperatures. We further note that enhanced prompting can lead to additional
performance improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, 1 table;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference-Based Planning in Stochastic Environments: From
  Partially-Ordered Temporal Goals to Most Preferred Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazhar Rahmani, Abhishek N. Kulkarni, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human preferences are not always represented via complete linear orders: It
is natural to employ partially-ordered preferences for expressing incomparable
outcomes. In this work, we consider decision-making and probabilistic planning
in stochastic systems modeled as Markov decision processes (MDPs), given a
partially ordered preference over a set of temporally extended goals.
Specifically, each temporally extended goal is expressed using a formula in
Linear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially
ordered preference, we introduce order theory to map a preference over temporal
goals to a preference over policies for the MDP. Accordingly, a most preferred
policy under a stochastic ordering induces a stochastic nondominated
probability distribution over the finite paths in the MDP. To synthesize a most
preferred policy, our technical approach includes two key steps. In the first
step, we develop a procedure to transform a partially ordered preference over
temporal goals into a computational model, called preference automaton, which
is a semi-automaton with a partial order over acceptance conditions. In the
second step, we prove that finding a most preferred policy is equivalent to
computing a Pareto-optimal policy in a multi-objective MDP that is constructed
from the original MDP, the preference automaton, and the chosen stochastic
ordering relation. Throughout the paper, we employ running examples to
illustrate the proposed preference specification and solution approaches. We
demonstrate the efficacy of our algorithm using these examples, providing
detailed analysis, and then discuss several potential future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2209.12267</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long and Short-Term Constraints Driven Safe Reinforcement Learning for
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuemin Hu, Pan Chen, Yijun Wen, Bo Tang, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has been widely used in decision-making tasks,
but it cannot guarantee the agent's safety in the training process due to the
requirements of interaction with the environment, which seriously limits its
industrial applications such as autonomous driving. Safe RL methods are
developed to handle this issue by constraining the expected safety violation
costs as a training objective, but they still permit unsafe state occurrence,
which is unacceptable in autonomous driving tasks. Moreover, these methods are
difficult to achieve a balance between the cost and return expectations, which
leads to learning performance degradation for the algorithms. In this paper, we
propose a novel algorithm based on the long and short-term constraints (LSTC)
for safe RL. The short-term constraint aims to guarantee the short-term state
safety that the vehicle explores, while the long-term constraint ensures the
overall safety of the vehicle throughout the decision-making process. In
addition, we develop a safe RL method with dual-constraint optimization based
on the Lagrange multiplier to optimize the training process for end-to-end
autonomous driving. Comprehensive experiments were conducted on the MetaDrive
simulator. Experimental results demonstrate that the proposed method achieves
higher safety in continuous state and action tasks, and exhibits higher
exploration performance in long-distance decision-making tasks compared with
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Evolutionary Network Architecture Search Framework with Adaptive
  Multimodal Fusion for Hand Gesture Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhang Xia, Shihao Song, Zhanglu Hou, Junwen Xu, Juan Zou, Yuan Liu, Shengxiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand gesture recognition (HGR) based on multimodal data has attracted
considerable attention owing to its great potential in applications. Various
manually designed multimodal deep networks have performed well in multimodal
HGR (MHGR), but most of existing algorithms require a lot of expert experience
and time-consuming manual trials. To address these issues, we propose an
evolutionary network architecture search framework with the adaptive multimodel
fusion (AMF-ENAS). Specifically, we design an encoding space that
simultaneously considers fusion positions and ratios of the multimodal data,
allowing for the automatic construction of multimodal networks with different
architectures through decoding. Additionally, we consider three input streams
corresponding to intra-modal surface electromyography (sEMG), intra-modal
accelerometer (ACC), and inter-modal sEMG-ACC. To automatically adapt to
various datasets, the ENAS framework is designed to automatically search a MHGR
network with appropriate fusion positions and ratios. To the best of our
knowledge, this is the first time that ENAS has been utilized in MHGR to tackle
issues related to the fusion position and ratio of multimodal data.
Experimental results demonstrate that AMF-ENAS achieves state-of-the-art
performance on the Ninapro DB2, DB3, and DB7 datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Privacy Protection Capabilities of Chinese Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Yang, Xiaowen Huang, Jitao Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), renowned for their impressive capabilities in
various tasks, have significantly advanced artificial intelligence. Yet, these
advancements have raised growing concerns about privacy and security
implications. To address these issues and explain the risks inherent in these
models, we have devised a three-tiered progressive framework tailored for
evaluating privacy in language systems. This framework consists of
progressively complex and in-depth privacy test tasks at each tier. Our primary
objective is to comprehensively evaluate the sensitivity of large language
models to private information, examining how effectively they discern, manage,
and safeguard sensitive data in diverse scenarios. This systematic evaluation
helps us understand the degree to which these models comply with privacy
protection guidelines and the effectiveness of their inherent safeguards
against privacy breaches. Our observations indicate that existing Chinese large
language models universally show privacy protection shortcomings. It seems that
at the moment this widespread issue is unavoidable and may pose corresponding
privacy risks in applications based on these models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EndToEndML: An Open-Source End-to-End Pipeline for Machine Learning
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nisha Pillai, Athish Ram Das, Moses Ayoola, Ganga Gireesan, Bindu Nanduri, Mahalingam Ramkumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) techniques are widely applied in the life
sciences. However, applying innovative AI techniques to understand and
deconvolute biological complexity is hindered by the learning curve for life
science scientists to understand and use computing languages. An open-source,
user-friendly interface for AI models, that does not require programming skills
to analyze complex biological data will be extremely valuable to the
bioinformatics community. With easy access to different sequencing technologies
and increased interest in different 'omics' studies, the number of biological
datasets being generated has increased and analyzing these high-throughput
datasets is computationally demanding. The majority of AI libraries today
require advanced programming skills as well as machine learning, data
preprocessing, and visualization skills. In this research, we propose a
web-based end-to-end pipeline that is capable of preprocessing, training,
evaluating, and visualizing machine learning (ML) models without manual
intervention or coding expertise. By integrating traditional machine learning
and deep neural network models with visualizations, our library assists in
recognizing, classifying, clustering, and predicting a wide range of
multi-modal, multi-sensor datasets, including images, languages, and
one-dimensional numerical data, for drug discovery, pathogen classification,
and medical diagnostics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 7th International Conference on Information and Computer
  Technologies (ICICT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Looking Beyond What You See: An Empirical Analysis on Subgroup
  Intersectional Fairness for Multi-label Chest X-ray Classification Using
  Social Determinants of Racial Health Inequities <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dana Moukheiber, Saurabh Mahindre, Lama Moukheiber, Mira Moukheiber, Mingchen Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been significant progress in implementing deep learning models in
disease diagnosis using chest X- rays. Despite these advancements, inherent
biases in these models can lead to disparities in prediction accuracy across
protected groups. In this study, we propose a framework to achieve accurate
diagnostic outcomes and ensure fairness across intersectional groups in
high-dimensional chest X- ray multi-label classification. Transcending
traditional protected attributes, we consider complex interactions within
social determinants, enabling a more granular benchmark and evaluation of
fairness. We present a simple and robust method that involves retraining the
last classification layer of pre-trained models using a balanced dataset across
groups. Additionally, we account for fairness constraints and integrate
class-balanced fine-tuning for multi-label settings. The evaluation of our
method on the MIMIC-CXR dataset demonstrates that our framework achieves an
optimal tradeoff between accuracy and fairness compared to baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV CVAMD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wan, Kaichen Zhou, jinhong Chen, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous assembly in robotics and 3D vision presents significant
challenges, particularly in ensuring assembly correctness. Presently,
predominant methods such as MEPNet focus on assembling components based on
manually provided images. However, these approaches often fall short in
achieving satisfactory results for tasks requiring long-term planning.
Concurrently, we observe that integrating a self-correction module can
partially alleviate such issues. Motivated by this concern, we introduce the
single-step assembly error correction task, which involves identifying and
rectifying misassembled components. To support research in this area, we
present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising
manual images for assembly steps and instances of assembly failures.
Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel
method to address this task. SCANet treats assembled components as queries,
determining their correctness in manual images and providing corrections when
necessary. Finally, we utilize SCANet to correct the assembly results of
MEPNet. Experimental results demonstrate that SCANet can identify and correct
MEPNet's misassembled results, significantly improving the correctness of
assembly. Our code and dataset are available at
https://github.com/Yaser-wyx/SCANet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can AI Models Appreciate Document Aesthetics? An Exploration of
  Legibility and Layout Quality in Relation to Prediction Confidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsiu-Wei Yang, Abhinav Agrawal, Pavlos Fragkogiannis, Shubham Nitin Mulay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A well-designed document communicates not only through its words but also
through its visual eloquence. Authors utilize aesthetic elements such as
colors, fonts, graphics, and layouts to shape the perception of information.
Thoughtful document design, informed by psychological insights, enhances both
the visual appeal and the comprehension of the content. While state-of-the-art
document AI models demonstrate the benefits of incorporating layout and image
data, it remains unclear whether the nuances of document aesthetics are
effectively captured. To bridge the gap between human cognition and AI
interpretation of aesthetic elements, we formulated hypotheses concerning AI
behavior in document understanding tasks, specifically anchored in document
design principles. With a focus on legibility and layout quality, we tested
four aspects of aesthetic effects: noise, font-size contrast, alignment, and
complexity, on model confidence using correlational analysis. The results and
observations highlight the value of model analysis rooted in document design
theories. Our work serves as a trailhead for further studies and we advocate
for continued research in this topic to deepen our understanding of how AI
interprets document aesthetics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanisms of non-factual hallucinations in language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yu, Meng Cao, Jackie Chi Kit Cheung, Yue Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art language models (LMs) sometimes generate non-factual
hallucinations that misalign with world knowledge. Despite extensive efforts to
detect and mitigate hallucinations, understanding their internal mechanisms
remains elusive. Our study investigates the mechanistic causes of
hallucination, specifically non-factual ones where the LM incorrectly predicts
object attributes in response to subject-relation queries. With causal
mediation analysis and embedding space projection, we identify two general
mechanistic causes of hallucinations shared across LMs of various scales and
designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and
2) failing to select the correct object attribute in upper layer attention
heads and MLPs. These two mechanisms exhibit varying degrees of subject-object
association, predictive uncertainty and perturbation robustness. Additionally,
we scrutinize LM pre-training checkpoints, revealing distinct learning dynamics
for the two mechanistic causes of hallucinations. We also highlight how
attribution features from our causal analysis can effectively construct
hallucination detectors. Our work proposes a mechanistic understanding of LM
factual errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Singh, Prajwal Singhania, Aditya K. Ranjan, Zack Sating, Abhinav Bhatele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large communication costs are a critical bottleneck in training
state-of-the-art neural networks on distributed systems. This paper introduces
AxoNN, a novel four-dimensional (4D) parallelization approach, inspired by
Agarwal's algorithm for matrix multiplication, for parallelizing tensor
computations in deep learning, AxoNN employs two key strategies to minimize
communication overhead. First, we optimize communication by overlapping
expensive collective operations (reduce-scatter, all-gather, all-reduce) with
computations. Our experiments with a 20-billion parameter transformer model
demonstrate that these optimizations deliver nearly 53\% improvement. Second,
we present an analytical model to assist users in identifying
communication-minimizing configurations within the vast search space defined by
our 4D algorithm. This model empowers practitioners by simplifying the tuning
process for their specific training workloads. When training an 80-billion
parameter model on 1024 GPUs of Perlmutter, AxoNN surpasses Megatron-LM, a
state-of-the-art framework, by a significant 26%. Additionally, it achieves 57%
of the theoretical peak FLOP/s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shifting to Machine Supervision: Annotation-Efficient Semi and
  <span class="highlight-title">Self-Supervised</span> Learning for Automatic Medical Image Segmentation and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10319v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10319v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Singh, Raviteja Chukkapalli, Shravan Chaudhari, Luoyao Chen, Mei Chen, Jinqian Pan, Craig Smuda, Jacopo Cirrone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in clinical treatment are increasingly constrained by the
limitations of supervised learning techniques, which depend heavily on large
volumes of annotated data. The annotation process is not only costly but also
demands substantial time from clinical specialists. Addressing this issue, we
introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)
pipeline, a novel approach that leverages advancements in self-supervised and
semi-supervised learning. These techniques engage in auxiliary tasks that do
not require labeling, thus simplifying the scaling of machine supervision
compared to fully-supervised methods. Our study benchmarks these techniques on
three distinct medical imaging datasets to evaluate their effectiveness in
classification and segmentation tasks. Notably, we observed that self
supervised learning significantly surpassed the performance of supervised
methods in the classification of all evaluated datasets. Remarkably, the
semi-supervised approach demonstrated superior outcomes in segmentation,
outperforming fully-supervised methods while using 50% fewer labels across all
datasets. In line with our commitment to contributing to the scientific
community, we have made the S4MI code openly accessible, allowing for broader
application and further development of these methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Seventeen pages (incl. references), five figures, and one table.
  (Under Review)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent-Pro: Learning to Evolve via Policy-Level Reflection and
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models exhibit robust problem-solving capabilities for diverse
tasks. However, most LLM-based agents are designed as specific task solvers
with sophisticated prompt engineering, rather than agents capable of learning
and evolving through interactions. These task solvers necessitate manually
crafted prompts to inform task rules and regulate LLM behaviors, inherently
incapacitating to address complex dynamic scenarios e.g., large interactive
games. In light of this, we propose Agent-Pro: an LLM-based Agent with
Policy-level Reflection and Optimization that can learn a wealth of expertise
from interactive experiences and progressively elevate its behavioral policy.
Specifically, it involves a dynamic belief generation and reflection process
for policy evolution. Rather than action-level reflection, Agent-Pro
iteratively reflects on past trajectories and beliefs, fine-tuning its
irrational beliefs for a better policy. Moreover, a depth-first search is
employed for policy optimization, ensuring continual enhancement in policy
payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,
outperforming vanilla LLM and specialized models. Our results show Agent-Pro
can learn and evolve in complex and dynamic scenes, which also benefits
numerous LLM-based applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LLM-based Agent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Contrast: Better Reflection Through Inconsistent Solving
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reflection capacity of Large Language Model (LLM) has garnered extensive
attention. A post-hoc prompting strategy, e.g., reflexion and self-refine,
refines LLM's response based on self-evaluated or external feedback. However,
recent research indicates without external feedback, LLM's intrinsic reflection
is unstable. Our investigation unveils that the key bottleneck is the quality
of the self-evaluated feedback. We find LLMs often exhibit overconfidence or
high randomness when self-evaluate, offering stubborn or inconsistent feedback,
which causes poor reflection. To remedy this, we advocate Self-Contrast: It
adaptively explores diverse solving perspectives tailored to the request,
contrasts the differences, and summarizes these discrepancies into a checklist
which could be used to re-examine and eliminate discrepancies. Our method
endows LLM with diverse perspectives to alleviate stubborn biases. Moreover,
their discrepancies indicate potential errors or inherent uncertainties that
LLM often overlooks. Reflecting upon these can catalyze more accurate and
stable reflection. Experiments conducted on a series of reasoning and
translation tasks with different LLMs serve to underscore the effectiveness and
generality of our strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization Bounds: Perspectives from Information Theory and
  PAC-Bayes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fredrik Hellström, Giuseppe Durisi, Benjamin Guedj, Maxim Raginsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental question in theoretical machine learning is generalization.
Over the past decades, the PAC-Bayesian approach has been established as a
flexible framework to address the generalization capabilities of machine
learning algorithms, and design new ones. Recently, it has garnered increased
interest due to its potential applicability for a variety of learning
algorithms, including deep neural networks. In parallel, an
information-theoretic view of generalization has developed, wherein the
relation between generalization and various information measures has been
established. This framework is intimately connected to the PAC-Bayesian
approach, and a number of results have been independently discovered in both
strands. In this monograph, we highlight this strong connection and present a
unified treatment of PAC-Bayesian and information-theoretic generalization
bounds. We present techniques and results that the two perspectives have in
common, and discuss the approaches and interpretations that differ. In
particular, we demonstrate how many proofs in the area share a modular
structure, through which the underlying ideas can be intuited. We pay special
attention to the conditional mutual information (CMI) framework; analytical
studies of the information complexity of learning algorithms; and the
application of the proposed methods to deep learning. This monograph is
intended to provide a comprehensive introduction to information-theoretic
generalization bounds and their connection to PAC-Bayes, serving as a
foundation from which the most recent developments are accessible. It is aimed
broadly towards researchers with an interest in generalization and theoretical
machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>228 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Data Consistency with Diffusion Purification for Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06054v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06054v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishankar, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently gained traction as a powerful class of deep
generative priors, excelling in a wide range of image restoration tasks due to
their exceptional ability to model data distributions. To solve image
restoration problems, many existing techniques achieve data consistency by
incorporating additional likelihood gradient steps into the reverse sampling
process of diffusion models. However, the additional gradient steps pose a
challenge for real-world practical applications as they incur a large
computational overhead, thereby increasing inference time. They also present
additional difficulties when using accelerated diffusion model samplers, as the
number of data consistency steps is limited by the number of reverse sampling
steps. In this work, we propose a novel diffusion-based image restoration
solver that addresses these issues by decoupling the reverse process from the
data consistency steps. Our method involves alternating between a
reconstruction phase to maintain data consistency and a refinement phase that
enforces the prior via diffusion purification. Our approach demonstrates
versatility, making it highly adaptable for efficient problem-solving in latent
space. Additionally, it reduces the necessity for numerous sampling steps
through the integration of consistency models. The efficacy of our approach is
validated through comprehensive experiments across various image restoration
tasks, including image denoising, deblurring, inpainting, and super-resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedSN: A Novel Federated Learning Framework over LEO Satellite Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01483v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01483v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Lin, Zhe Chen, Zihan Fang, Xianhao Chen, Xiong Wang, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a large number of Low Earth Orbit (LEO) satellites have been
launched and deployed successfully in space by commercial companies, such as
SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve
not only for communication but also for various machine learning applications,
such as space modulation recognition, remote sensing image classification, etc.
However, the ground station (GS) may be incapable of downloading such a large
volume of raw sensing data for centralized model training due to the limited
contact time with LEO satellites (e.g. 5 minutes). Therefore, federated
learning (FL) has emerged as the promising solution to address this problem via
on-device training. Unfortunately, to enable FL on LEO satellites, we still
face three critical challenges that are i) heterogeneous computing and memory
capabilities, ii) limited uplink rate, and iii) model staleness. To this end,
we propose FedSN as a general FL framework to tackle the above challenges, and
fully explore data diversity on LEO satellites. Specifically, we first present
a novel sub-structure scheme to enable heterogeneous local model training
considering different computing, memory, and communication constraints on LEO
satellites. Additionally, we propose a pseudo-synchronous model aggregation
strategy to dynamically schedule model aggregation for compensating model
staleness. To further demonstrate the effectiveness of the FedSN, we evaluate
it using space modulation recognition and remote sensing image classification
tasks by leveraging the data from real-world satellite networks. Extensive
experimental results demonstrate that FedSN framework achieves higher accuracy,
lower computing, and communication overhead than the state-of-the-art
benchmarks and the effectiveness of each components in FedSN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonlinear Control Allocation: A Learning Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.06180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.06180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hafiz Zeeshan Iqbal Khan, Surrayya Mobeen, Jahanzeb Rajput, Jamshed Riaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern aircraft are designed with redundant control effectors to cater for
fault tolerance and maneuverability requirements. This leads to aircraft being
over-actuated and requires control allocation schemes to distribute the control
commands among control effectors. Traditionally, optimization-based control
allocation schemes are used; however, for nonlinear allocation problems, these
methods require large computational resources. In this work, an artificial
neural network (ANN) based nonlinear control allocation scheme is proposed. The
proposed scheme is composed of learning the inverse of the control
effectiveness map through ANN, and then implementing it as an allocator instead
of solving an online optimization problem. Stability conditions are presented
for closed-loop systems incorporating the allocator, and computational
challenges are explored with piece-wise linear effectiveness functions and
ANN-based allocators. To demonstrate the efficacy of the proposed scheme, it is
compared with a standard quadratic programming-based method for control
allocation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE Conference on Decision and Control (CDC), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, Sheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent large-scale text-to-speech (TTS) models have achieved
significant progress, they still fall short in speech quality, similarity, and
prosody. Considering speech intricately encompasses various attributes (e.g.,
content, prosody, timbre, and acoustic details) that pose significant
challenges for generation, a natural idea is to factorize speech into
individual subspaces representing different attributes and generate them
individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with
novel factorized diffusion models to generate natural speech in a zero-shot
way. Specifically, 1) we design a neural codec with factorized vector
quantization (FVQ) to disentangle speech waveform into subspaces of content,
prosody, timbre, and acoustic details; 2) we propose a factorized diffusion
model to generate attributes in each subspace following its corresponding
prompt. With this factorization design, NaturalSpeech 3 can effectively and
efficiently model intricate speech with disentangled subspaces in a
divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the
state-of-the-art TTS systems on quality, similarity, prosody, and
intelligibility, and achieves on-par quality with human recordings.
Furthermore, we achieve better performance by scaling to 1B parameters and 200K
hours of training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Achieving human-level quality and naturalness on multi-speaker
  datasets (e.g., LibriSpeech) in a zero-shot way</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat<span class="highlight-title">GPT</span> Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for EU AI policy act concerning ethics, digital
divide, and sustainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incorporating simulated spatial context information improves the
  effectiveness of contrastive learning models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15120v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15120v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhen Zhu, James Z. Wang, Wonseuk Lee, Brad Wyble
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual learning often occurs in a specific context, where an agent acquires
skills through exploration and tracking of its location in a consistent
environment. The historical spatial context of the agent provides a similarity
signal for self-supervised contrastive learning. We present a unique approach,
termed Environmental Spatial Similarity (ESS), that complements existing
contrastive learning methods. Using images from simulated, photorealistic
environments as an experimental setting, we demonstrate that ESS outperforms
traditional instance discrimination approaches. Moreover, sampling additional
data from the same environment substantially improves accuracy and provides new
augmentations. ESS allows remarkable proficiency in room classification and
spatial prediction tasks, especially in unfamiliar environments. This learning
paradigm has the potential to enable rapid visual learning in agents operating
in new environments with unique visual characteristics. Potentially
transformative applications span from robotics to space exploration. Our proof
of concept demonstrates improved efficiency over methods that rely on
extensive, disconnected datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Approximation with Delayed Updates: Finite-Time Rates under
  Markovian Sampling <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Adibi, Nicolo Dal Fabbro, Luca Schenato, Sanjeev Kulkarni, H. Vincent Poor, George J. Pappas, Hamed Hassani, Aritra Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by applications in large-scale and multi-agent reinforcement
learning, we study the non-asymptotic performance of stochastic approximation
(SA) schemes with delayed updates under Markovian sampling. While the effect of
delays has been extensively studied for optimization, the manner in which they
interact with the underlying Markov process to shape the finite-time
performance of SA remains poorly understood. In this context, our first main
contribution is to show that under time-varying bounded delays, the delayed SA
update rule guarantees exponentially fast convergence of the \emph{last
iterate} to a ball around the SA operator's fixed point. Notably, our bound is
\emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the
mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel
inductive proof technique that, unlike various existing delayed-optimization
analyses, relies on establishing uniform boundedness of the iterates. As such,
our proof may be of independent interest. Next, to mitigate the impact of the
maximum delay on the convergence rate, we provide the first finite-time
analysis of a delay-adaptive SA scheme under Markovian sampling. In particular,
we show that the exponent of convergence of this scheme gets scaled down by
$\tau_{avg}$, as opposed to $\tau_{max}$ for the vanilla delayed SA rule; here,
$\tau_{avg}$ denotes the average delay across all iterations. Moreover, the
adaptive scheme requires no prior knowledge of the delay sequence for step-size
tuning. Our theoretical findings shed light on the finite-time effects of
delays for a broad class of algorithms, including TD learning, Q-learning, and
stochastic gradient descent under Markovian sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised
  Learning <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12091v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12091v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yu, Danruo Deng, Furui Liu, Yueming Jin, Qi Dou, Guangyong Chen, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled
data and test data are from the same distribution. Open-set semi-supervised
learning (Open-set SSL) considers a more practical scenario, where unlabeled
data and test data contain new categories (outliers) not observed in labeled
data (inliers). Most previous works focused on outlier detection via binary
classifiers, which suffer from insufficient scalability and inability to
distinguish different types of uncertainty. In this paper, we propose a novel
framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these
limitations. Concretely, we first introduce evidential deep learning (EDL) as
an outlier detector to quantify different types of uncertainty, and design
different uncertainty metrics for self-training and inference. Furthermore, we
propose a novel adaptive negative optimization strategy, making EDL more
tailored to the unlabeled dataset containing both inliers and outliers. As
demonstrated empirically, our proposed method outperforms existing
state-of-the-art methods across four datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental
  Health Sensing Studies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshat Choube, Vedant Das Swain, Varun Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in mobile and wearable technologies have enabled the potential to
passively monitor a person's mental, behavioral, and affective health. These
approaches typically rely on longitudinal collection of self-reported outcomes,
e.g., depression, stress, and anxiety, to train machine learning (ML) models.
However, the need to continuously self-report adds a significant burden on the
participants, often resulting in attrition, missing labels, or insincere
responses. In this work, we introduce the Scale Scores Simulation using Mental
Models (SeSaMe) framework to alleviate participants' burden in digital mental
health studies. By leveraging pre-trained large language models (LLMs), SeSaMe
enables the simulation of participants' responses on psychological scales. In
SeSaMe, researchers can prompt LLMs with information on participants' internal
behavioral dispositions, enabling LLMs to construct mental models of
participants to simulate their responses on psychological scales. We
demonstrate an application of SeSaMe, where we use GPT-4 to simulate responses
on one scale using responses from another as behavioral information. We also
evaluate the alignment between human and SeSaMe-simulated responses to
psychological scales. Then, we present experiments to inspect the utility of
SeSaMe-simulated responses as ground truth in training ML models by replicating
established depression and anxiety screening tasks from a previous study. Our
results indicate SeSaMe to be a promising approach, but its alignment may vary
across scales and specific prediction objectives. We also observed that model
performance with simulated data was on par with using the real data for
training in most evaluation scenarios. We conclude by discussing the potential
implications of SeSaMe in addressing some challenges researchers face with
ground-truth collection in passive sensing studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Byzantine-resilient Federated Learning With Adaptivity to Data
  Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13374v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13374v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyuan Zuo, Xingrun Yan, Rongfei Fan, Han Hu, Hangguan Shan, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deals with federated learning (FL) in the presence of malicious
Byzantine attacks and data heterogeneity. A novel Robust Average Gradient
Algorithm (RAGA) is proposed, which leverages the geometric median for
aggregation and can freely select the round number for local updating.
Different from most existing resilient approaches, which perform convergence
analysis based on strongly-convex loss function or homogeneously distributed
dataset, we conduct convergence analysis for not only strongly-convex but also
non-convex loss function over heterogeneous dataset. According to our
theoretical analysis, as long as the fraction of dataset from malicious users
is less than half, RAGA can achieve convergence at rate
$\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and
$\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for
strongly-convex loss function. Moreover, stationary point or global optimal
solution is proved to obtainable as data heterogeneity vanishes. Experimental
results corroborate the robustness of RAGA to Byzantine attacks and verifies
the advantage of RAGA over baselines on convergence performance under various
intensity of Byzantine attacks, for heterogeneous dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Misconceptions in Social Bots Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Cresci, Kai-Cheng Yang, Angelo Spognardi, Roberto Di Pietro, Filippo Menczer, Marinella Petrocchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on social bots aims at advancing knowledge and providing solutions
to one of the most debated forms of online manipulation. Yet, social bot
research is plagued by widespread biases, hyped results, and misconceptions
that set the stage for ambiguities, unrealistic expectations, and seemingly
irreconcilable findings. Overcoming such issues is instrumental towards
ensuring reliable solutions and reaffirming the validity of the scientific
method. In this contribution, we review some recent results in social bots
research, highlighting and revising factual errors as well as methodological
and conceptual biases. More importantly, we demystify common misconceptions,
addressing fundamental points on how social bots research is discussed. Our
analysis surfaces the need to discuss research about online disinformation and
manipulation in a rigorous, unbiased, and responsible way. This article
bolsters such effort by identifying and refuting common fallacious arguments
used by both proponents and opponents of social bots research, as well as
providing directions toward sound methodologies for future research in the
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepMachining: Online Prediction of Machining Errors of Lathe Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16451v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16451v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, Chen-Hsin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe DeepMachining, a deep learning-based AI system for online
prediction of machining errors of lathe machine operations. We have built and
evaluated DeepMachining based on manufacturing data from factories.
Specifically, we first pretrain a deep learning model for a given lathe
machine's operations to learn the salient features of machining states. Then,
we fine-tune the pretrained model to adapt to specific machining tasks. We
demonstrate that DeepMachining achieves high prediction accuracy for multiple
tasks that involve different workpieces and cutting tools. To the best of our
knowledge, this work is one of the first factory experiments using pre-trained
deep-learning models to predict machining errors of lathe machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure Guided Large Language Model for SQL Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinggang Zhang, Junnan Dong, Hao Chen, Wentao Li, Feiran Huang, Xiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating accurate Structured Querying Language (SQL) is a long-standing
problem, especially in matching users' semantic queries with structured
databases and then generating structured SQL. Existing models typically input
queries and database schemas into the LLM and rely on the LLM to perform
semantic-structure matching and generate structured SQL. However, such
solutions overlook the structural information within user queries and
databases, which can be utilized to enhance the generation of structured SQL.
This oversight can lead to inaccurate or unexecutable SQL generation. To fully
exploit the structure, we propose a structure-to-SQL framework, which leverages
the inherent structure information to improve the SQL generation of LLMs.
Specifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.
SGU-SQL first links user queries and databases in a structure-enhanced manner.
It then decomposes complicated linked structures with grammar trees to guide
the LLM to generate the SQL step by step. Extensive experiments on two
benchmark datasets illustrate that SGU-SQL can outperform sixteen SQL
generation baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrent Action <span class="highlight-title">Transformer</span> with Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09459v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09459v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Staroverov, Egor Cherepanov, Dmitry Yudin, Alexey K. Kovalev, Aleksandr I. Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the use of transformers in offline reinforcement learning has
become a rapidly developing area. This is due to their ability to treat the
agent's trajectory in the environment as a sequence, thereby reducing the
policy learning problem to sequence modeling. In environments where the agent's
decisions depend on past events, it is essential to capture both the event
itself and the decision point in the context of the model. However, the
quadratic complexity of the attention mechanism limits the potential for
context expansion. One solution to this problem is to enhance transformers with
memory mechanisms. In this paper, we propose the Recurrent Action Transformer
with Memory (RATE) - a model that incorporates recurrent memory. To evaluate
our model, we conducted extensive experiments on both memory-intensive
environments (VizDoom-Two-Color, T-Maze) and classic Atari games and MuJoCo
control environments. The results show that the use of memory can significantly
improve performance in memory-intensive environments while maintaining or
improving results in classic environments. We hope that our findings will
stimulate research on memory mechanisms for transformers applicable to offline
reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative <span class="highlight-title">Pre-Train</span>ing of Time-Series Data for Unsupervised Fault
  Detection in Semiconductor Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sewoong Lee, JinKyou Choi, Min Su Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces TRACE-GPT, which stands for Time-seRies
Anomaly-detection with Convolutional Embedding and Generative Pre-trained
Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor
data and detect faults on unlabeled datasets in semiconductor manufacturing. In
semiconductor industry, classifying abnormal time-series sensor data from
normal data is important because it is directly related to wafer defect.
However, small, unlabeled, and even mixed training data without enough
anomalies make classification tasks difficult. In this research, we capture
features of time-series data with temporal convolutional embedding and
Generative Pre-trained Transformer (GPT) to classify abnormal sequences from
normal sequences using cross entropy loss. We prove that our model shows better
performance than previous unsupervised models with both an open dataset, the
University of California Riverside (UCR) time-series classification archive,
and the process log of our Chemical Vapor Deposition (CVD) equipment. Our model
has the highest F1 score at Equal Error Rate (EER) across all datasets and is
only 0.026 below the supervised state-of-the-art baseline on the open dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attacks, Defenses and Evaluations for LLM Conversation Safety: A <span class="highlight-title">Survey</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09283v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09283v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shapley Values-Powered Framework for Fair Reward Split in Content
  Produced by GenAI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Glinsky, Alexey Sokolsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is evident that, currently, generative models are surpassed in quality by
human professionals. However, with the advancements in Artificial Intelligence,
this gap will narrow, leading to scenarios where individuals who have dedicated
years of their lives to mastering a skill become obsolete due to their high
costs, which are inherently linked to the time they require to complete a task
-- a task that AI could accomplish in minutes or seconds. To avoid future
social upheavals, we must, even now, contemplate how to fairly assess the
contributions of such individuals in training generative models and how to
compensate them for the reduction or complete loss of their incomes. In this
work, we propose a method to structure collaboration between model developers
and data providers. To achieve this, we employ Shapley Values to quantify the
contribution of artist(s) in an image generated by the Stable Diffusion-v1.5
model and to equitably allocate the reward among them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 32 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ABScribe: Rapid Exploration & Organization of Multiple Writing
  Variations in Human-AI Co-Writing Tasks using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00117v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00117v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohi Reza, Nathan Laundry, Ilya Musabirov, Peter Dushniku, Zhi Yuan "Michael" Yu, Kashish Mittal, Tovi Grossman, Michael Liut, Anastasia Kuzminykh, Joseph Jay Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring alternative ideas by rewriting text is integral to the writing
process. State-of-the-art Large Language Models (LLMs) can simplify writing
variation generation. However, current interfaces pose challenges for
simultaneous consideration of multiple variations: creating new variations
without overwriting text can be difficult, and pasting them sequentially can
clutter documents, increasing workload and disrupting writers' flow. To tackle
this, we present ABScribe, an interface that supports rapid, yet visually
structured, exploration and organization of writing variations in human-AI
co-writing tasks. With ABScribe, users can swiftly modify variations using LLM
prompts, which are auto-converted into reusable buttons. Variations are stored
adjacently within text fields for rapid in-place comparisons using mouse-over
interactions on a popup toolbar. Our user study with 12 writers shows that
ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances
user perceptions of the revision process (d = 2.41, p < 0.001) compared to a
popular baseline workflow, and provides insights into how writers explore
variations using LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label
  Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Tian, Hongxin Wei, Yiqun Wang, Lei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial-label learning (PLL) is an important weakly supervised learning
problem, which allows each training example to have a candidate label set
instead of a single ground-truth label. Identification-based methods have been
widely explored to tackle label ambiguity issues in PLL, which regard the true
label as a latent variable to be identified. However, identifying the true
labels accurately and completely remains challenging, causing noise in pseudo
labels during model training. In this paper, we propose a new method called
CroSel, which leverages historical predictions from the model to identify true
labels for most training examples. First, we introduce a cross selection
strategy, which enables two deep models to select true labels of partially
labeled data for each other. Besides, we propose a novel consistency
regularization term called co-mix to avoid sample waste and tiny noise caused
by false selection. In this way, CroSel can pick out the true labels of most
examples with high precision. Extensive experiments demonstrate the superiority
of CroSel, which consistently outperforms previous state-of-the-art methods on
benchmark datasets. Additionally, our method achieves over 90\% accuracy and
quantity for selecting true labels on CIFAR-type datasets under various
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenging Common Paradigms in Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04698v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04698v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cathrin Elich, Lukas Kirchdorfer, Jan M. Köhler, Lukas Schott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While multi-task learning (MTL) has gained significant attention in recent
years, its underlying mechanisms remain poorly understood. Recent methods did
not yield consistent performance improvements over single task learning (STL)
baselines, underscoring the importance of gaining more profound insights about
challenges specific to MTL. In our study, we challenge paradigms in MTL in the
context of STL: First, the impact of the choice of optimizer has only been
mildly investigated in MTL. We show the pivotal role of common STL tools such
as the Adam optimizer in MTL empirically in various experiments. To further
investigate Adam's effectiveness, we theoretical derive a partial loss-scale
invariance under mild assumptions. Second, the notion of gradient conflicts has
often been phrased as a specific problem in MTL. We delve into the role of
gradient conflicts in MTL and compare it to STL. For angular gradient alignment
we find no evidence that this is a unique problem in MTL. We emphasize
differences in gradient magnitude as the main distinguishing factor. Lastly, we
compare the transferability of features learned through MTL and STL on common
image corruptions, and find light evidence that MTL can lead to superior
transferability. Overall, we find surprising similarities between STL and MTL
suggesting to consider methods from both fields in a broader context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>-</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving a Real-World Package Delivery Routing Problem Using Quantum
  Annealers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eneko Osaba, Esther Villar-Rodriguez, Antón Asla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research focused on the conjunction between quantum computing and routing
problems has been very prolific in recent years. Most of the works revolve
around classical problems such as the Traveling Salesman Problem or the Vehicle
Routing Problem. Even though working on these problems is valuable, it is also
undeniable that their academic-oriented nature falls short of real-world
requirements. The main objective of this research is to present a solving
method for realistic instances, avoiding problem relaxations or technical
shortcuts. Instead, a quantum-classical hybrid solver has been developed,
coined Q4RPD, that considers a set of real constraints such as a heterogeneous
fleet of vehicles, priority deliveries, and capacities characterized by two
values: weight and dimensions of the packages. Q4RPD resorts to the Leap
Constrained Quadratic Model Hybrid Solver of D-Wave. To demonstrate the
application of Q4RPD, an experimentation composed of six different instances
has been conducted, aiming to serve as illustrative examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures and 4 tables. Paper submitted for review in
  Scientific Reports</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hourglass Tokenizer for Efficient <span class="highlight-title">Transformer</span>-Based 3D Human Pose
  Estimation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a plug-and-play pruning-and-recovering framework,
called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose
estimation from videos. Our HoT begins with pruning pose tokens of redundant
frames and ends with recovering full-length tokens, resulting in a few pose
tokens in the intermediate transformer blocks and thus improving the model
efficiency. To effectively achieve this, we propose a token pruning cluster
(TPC) that dynamically selects a few representative tokens with high semantic
diversity while eliminating the redundancy of video frames. In addition, we
develop a token recovering attention (TRA) to restore the detailed
spatio-temporal information based on the selected tokens, thereby expanding the
network output to the original full-length temporal resolution for fast
inference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and
MPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and
estimation accuracy compared to the original VPT models. For instance, applying
to MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs
without sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,
respectively. Code and models are available at
https://github.com/NationalGAILab/HoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024, Open Sourced</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\textit{Link<span class="highlight-title">Prompt</span>}$: Natural and Universal Adversarial Attacks on
  <span class="highlight-title">Prompt</span>-based Language Models <span class="chip">NAACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Xu, Wenjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based learning is a new language model training paradigm that adapts
the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes
the performance benchmarks across various natural language processing (NLP)
tasks. Instead of using a fixed prompt template to fine-tune the model, some
research demonstrates the effectiveness of searching for the prompt via
optimization. Such prompt optimization process of prompt-based learning on PLMs
also gives insight into generating adversarial prompts to mislead the model,
raising concerns about the adversarial vulnerability of this paradigm. Recent
studies have shown that universal adversarial triggers (UATs) can be generated
to alter not only the predictions of the target PLMs but also the prediction of
corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based
learning paradigm. However, UATs found in previous works are often unreadable
tokens or characters and can be easily distinguished from natural texts with
adaptive defenses. In this work, we consider the naturalness of the UATs and
develop $\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs
by a gradient-based beam search algorithm that not only effectively attacks the
target PLMs and PFMs but also maintains the naturalness among the trigger
tokens. Extensive results demonstrate the effectiveness of
$\textit{LinkPrompt}$, as well as the transferability of UATs generated by
$\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and
API-accessed LLM GPT-3.5-turbo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of NAACL2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLatrieval: LLM-Verified Retrieval for Verifiable Generation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07838v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07838v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang Sun, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verifiable generation aims to let the large language model (LLM) generate
text with supporting documents, which enables the user to flexibly verify the
answer and makes the LLM's output more reliable. Retrieval plays a crucial role
in verifiable generation. Specifically, the retrieved documents not only
supplement knowledge to help the LLM generate correct answers, but also serve
as supporting evidence for the user to verify the LLM's output. However, the
widely used retrievers become the bottleneck of the entire pipeline and limit
the overall performance. Their capabilities are usually inferior to LLMs since
they often have much fewer parameters than the large language model and have
not been demonstrated to scale well to the size of LLMs. If the retriever does
not correctly find the supporting documents, the LLM can not generate the
correct and verifiable answer, which overshadows the LLM's remarkable
abilities. To address these limitations, we propose \LLatrieval (Large Language
Model Verified Retrieval), where the LLM updates the retrieval result until it
verifies that the retrieved documents can sufficiently support answering the
question. Thus, the LLM can iteratively provide feedback to retrieval and
facilitate the retrieval result to fully support verifiable generation.
Experiments show that LLatrieval significantly outperforms extensive baselines
and achieves state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Object Coherence in Layout-to-Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10522v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10522v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Wang, Weizhong Zhang, Jianwei Zheng, Cheng Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Layout-to-image synthesis is an emerging technique in conditional image
generation. It aims to generate complex scenes, where users require fine
control over the layout of the objects in a scene. However, it remains
challenging to control the object coherence, including semantic coherence
(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the
hand and the racket should not be misaligned). In this paper, we propose a
novel diffusion model with effective global semantic fusion (GSF) and
self-similarity feature enhancement modules to guide the object coherence for
this task. For semantic coherence, we argue that the image caption contains
rich information for defining the semantic relationship within the objects in
the images. Instead of simply employing cross-attention between captions and
generated images, which addresses the highly relevant layout restriction and
semantic coherence separately and thus leads to unsatisfying results shown in
our experiments, we develop GSF to fuse the supervision from the layout
restriction and semantic coherence requirement and exploit it to guide the
image synthesis process. Moreover, to improve the physical coherence, we
develop a Self-similarity Coherence Attention (SCA) module to explicitly
integrate local contextual physical coherence into each pixel's generation
process. Specifically, we adopt a self-similarity map to encode the coherence
restrictions and employ it to extract coherent features from text embedding.
Through visualization of our self-similarity map, we explore the essence of
SCA, revealing that its effectiveness is not only in capturing reliable
physical coherence patterns but also in enhancing complex texture generation.
Extensive experiments demonstrate the superiority of our proposed method in
both image generation quality and controllability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To help the open-source community have a better understanding of
Mixture-of-Experts (MoE) based large language models (LLMs), we train and
release OpenMoE, a series of fully open-sourced and reproducible decoder-only
MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T
tokens. Our investigation confirms that MoE-based LLMs can offer a more
favorable cost-effectiveness trade-off than dense LLMs, highlighting the
potential effectiveness for future LLM development.
  One more important contribution of this study is an in-depth analysis of the
routing mechanisms within our OpenMoE models, leading to three significant
findings: Context-Independent Specialization, Early Routing Learning, and
Drop-towards-the-End. We discovered that routing decisions in MoE models are
predominantly based on token IDs, with minimal context relevance. The
token-to-expert assignments are determined early in the pre-training phase and
remain largely unchanged. This imperfect routing can result in performance
degradation, particularly in sequential tasks like multi-turn conversations,
where tokens appearing later in a sequence are more likely to be dropped.
Finally, we rethink our design based on the above-mentioned observations and
analysis. To facilitate future MoE LLM development, we propose potential
strategies for mitigating the issues we found and further improving
off-the-shelf MoE LLM designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIGraph: Generative <span class="highlight-title">Self-supervised</span> Learning for Class-Imbalanced Node
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulan Hu, Sheng Ouyang, Zhirui Yang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class imbalance in graph data presents significant challenges for node
classification. While existing methods, such as SMOTE-based approaches,
partially mitigate this issue, they still exhibit limitations in constructing
imbalanced graphs. Generative self-supervised learning (SSL) methods,
exemplified by graph autoencoders (GAEs), offer a promising solution by
directly generating minority nodes from the data itself, yet their potential
remains underexplored. In this paper, we delve into the shortcomings of
SMOTE-based approaches in the construction of imbalanced graphs. Furthermore,
we introduce VIGraph, a simple yet effective generative SSL approach that
relies on the Variational GAE as the fundamental model. VIGraph strictly
adheres to the concept of imbalance when constructing imbalanced graphs and
innovatively leverages the variational inference (VI) ability of Variational
GAE to generate nodes for minority classes. VIGraph introduces comprehensive
training strategies, including cross-view contrastive learning at the decoding
phase to capture semantic knowledge, adjacency matrix reconstruction to
preserve graph structure, and alignment strategy to ensure stable training.
VIGraph can generate high-quality nodes directly usable for classification,
eliminating the need to integrate the generated nodes back to the graph as well
as additional retraining found in SMOTE-based methods. We conduct extensive
experiments, results from which demonstrate the superiority and generality of
our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Quadruped Locomotion Using Differentiable Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Song, Sangbae Kim, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While most recent advancements in legged robot control have been driven by
model-free reinforcement learning, we explore the potential of differentiable
simulation. Differentiable simulation promises faster convergence and more
stable training by computing low-variant first-order gradients using the robot
model, but so far, its use for legged robot control has remained limited to
simulation. The main challenge with differentiable simulation lies in the
complex optimization landscape of robotic tasks due to discontinuities in
contact-rich environments, e.g., quadruped locomotion. This work proposes a
new, differentiable simulation framework to overcome these challenges. The key
idea involves decoupling the complex whole-body simulation, which may exhibit
discontinuities due to contact, into two separate continuous domains.
Subsequently, we align the robot state resulting from the simplified model with
a more precise, non-differentiable simulator to maintain sufficient simulation
accuracy. Our framework enables learning quadruped walking in minutes using a
single simulated robot without any parallelization. When augmented with GPU
parallelization, our approach allows the quadruped robot to master diverse
locomotion skills, including trot, pace, bound, and gallop, on challenging
terrains in minutes. Additionally, our policy achieves robust locomotion
performance in the real world zero-shot. To the best of our knowledge, this
work represents the first demonstration of using differentiable simulation for
controlling a real quadruped robot. This work provides several important
insights into using differentiable simulations for legged locomotion in the
real world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Generation for Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10997v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10997v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) showcase impressive capabilities but encounter
challenges like hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the generation,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval, the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces up-to-date evaluation framework and
benchmark. At the end, this article delineates the challenges currently faced
and points out prospective avenues for research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing Work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ World Models via Policy-Guided Trajectory Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08533v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08533v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Rigter, Jun Yamada, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models are a powerful tool for developing intelligent agents. By
predicting the outcome of a sequence of actions, world models enable policies
to be optimised via on-policy reinforcement learning (RL) using synthetic data,
i.e. in "in imagination". Existing world models are autoregressive in that they
interleave predicting the next state with sampling the next action from the
policy. Prediction error inevitably compounds as the trajectory length grows.
In this work, we propose a novel world modelling approach that is not
autoregressive and generates entire on-policy trajectories in a single pass
through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion
(PolyGRAD), leverages a denoising model in addition to the gradient of the
action distribution of the policy to diffuse a trajectory of initially random
states and actions into an on-policy synthetic trajectory. We analyse the
connections between PolyGRAD, score-based generative models, and
classifier-guided diffusion models. Our results demonstrate that PolyGRAD
outperforms state-of-the-art baselines in terms of trajectory prediction error
for short trajectories, with the exception of autoregressive diffusion. For
short trajectories, PolyGRAD obtains similar errors to autoregressive
diffusion, but with lower computational requirements. For long trajectories,
PolyGRAD obtains comparable performance to baselines. Our experiments
demonstrate that PolyGRAD enables performant policies to be trained via
on-policy RL in imagination for MuJoCo continuous control domains. Thus,
PolyGRAD introduces a new paradigm for accurate on-policy world modelling
without autoregressive sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in TMLR, March 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Functional Graph Convolutional Networks: A unified multi-task and
  multi-modal learning framework to facilitate health and social-care insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobia Boschi, Francesca Bonin, Rodrigo Ordonez-Hurtado, Cécile Rousseau, Alessandra Pascale, John Dinsmore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel Functional Graph Convolutional Network (funGCN)
framework that combines Functional Data Analysis and Graph Convolutional
Networks to address the complexities of multi-task and multi-modal learning in
digital health and longitudinal studies. With the growing importance of health
solutions to improve health care and social support, ensure healthy lives, and
promote well-being at all ages, funGCN offers a unified approach to handle
multivariate longitudinal data for multiple entities and ensures
interpretability even with small sample sizes. Key innovations include
task-specific embedding components that manage different data types, the
ability to perform classification, regression, and forecasting, and the
creation of a knowledge graph for insightful data interpretation. The efficacy
of funGCN is validated through simulation experiments and a real-data
application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Concept-Based Causal Transition and Symbolic Reasoning for
  Visual Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilue Qian, Peiyu Yu, Ying Nian Wu, Yao Su, Wei Wang, Lifeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual planning simulates how humans make decisions to achieve desired goals
in the form of searching for visual causal transitions between an initial
visual state and a final visual goal state. It has become increasingly
important in egocentric vision with its advantages in guiding agents to perform
daily tasks in complex environments. In this paper, we propose an interpretable
and generalizable visual planning framework consisting of i) a novel
Substitution-based Concept Learner (SCL) that abstracts visual inputs into
disentangled concept representations, ii) symbol abstraction and reasoning that
performs task planning via the self-learned symbols, and iii) a Visual Causal
Transition model (ViCT) that grounds visual causal transitions to semantically
similar real-world actions. Given an initial state, we perform goal-conditioned
visual planning with a symbolic reasoning method fueled by the learned
representations and causal transitions to reach the goal state. To verify the
effectiveness of the proposed model, we collect a large-scale visual planning
dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this
challenging dataset demonstrate the superior performance of our method in
visual task planning. Empirically, we show that our framework can generalize to
unseen task trajectories, unseen object categories, and real-world data.
Further details of this work are provided at
https://fqyqc.github.io/ConTranPlan/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying the Correlation Between Language Distance and Cross-Lingual
  Transfer in a Multilingual Representation Space <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fred Philippy, Siwen Guo, Shohreh Haddadan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research has investigated the impact of various linguistic features on
cross-lingual transfer performance. In this study, we investigate the manner in
which this effect can be mapped onto the representation space. While past
studies have focused on the impact on cross-lingual alignment in multilingual
language models during fine-tuning, this study examines the absolute evolution
of the respective language representation spaces produced by MLLMs. We place a
specific emphasis on the role of linguistic characteristics and investigate
their inter-correlation with the impact on representation spaces and
cross-lingual transfer performance. Additionally, this paper provides
preliminary evidence of how these findings can be leveraged to enhance transfer
to linguistically distant languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGTYP Workshop 2023 (co-located with EACL 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Effects of Mixed Sample Data Augmentation are Class Dependent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeil Lee, Hansang Lee, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed Sample Data Augmentation (MSDA) techniques, such as Mixup, CutMix, and
PuzzleMix, have been widely acknowledged for enhancing performance in a variety
of tasks. A previous study reported the class dependency of traditional data
augmentation (DA), where certain classes benefit disproportionately compared to
others. This paper reveals a class dependent effect of MSDA, where some classes
experience improved performance while others experience degraded performance.
This research addresses the issue of class dependency in MSDA and proposes an
algorithm to mitigate it. The approach involves training on a mixture of MSDA
and non-MSDA data, which not only mitigates the negative impact on the affected
classes, but also improves overall accuracy. Furthermore, we provide in-depth
analysis and discussion of why MSDA introduced class dependencies and which
classes are most likely to have them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 18 figures, Overall Revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18920v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18920v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers, Florian Bernard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although 3D shape matching and interpolation are highly interrelated, they
are often studied separately and applied sequentially to relate different 3D
shapes, thus resulting in sub-optimal performance. In this work we present a
unified framework to predict both point-wise correspondences and shape
interpolation between 3D shapes. To this end, we combine the deep functional
map framework with classical surface deformation models to map shapes in both
spectral and spatial domains. On the one hand, by incorporating spatial maps,
our method obtains more accurate and smooth point-wise correspondences compared
to previous functional map methods for shape matching. On the other hand, by
introducing spectral maps, our method gets rid of commonly used but
computationally expensive geodesic distance constraints that are only valid for
near-isometric shape deformations. Furthermore, we propose a novel test-time
adaptation scheme to capture both pose-dominant and shape-dominant
deformations. Using different challenging datasets, we demonstrate that our
method outperforms previous state-of-the-art methods for both shape matching
and interpolation, even compared to supervised approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Regulatable AI Systems: Technical Gaps and Policy Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12609v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12609v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Shen, Hannah Brown, Jiashu Tao, Martin Strobel, Yao Tong, Akshay Narayan, Harold Soh, Finale Doshi-Velez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is increasing attention being given to how to regulate AI systems. As
governing bodies grapple with what values to encapsulate into regulation, we
consider the technical half of the question: To what extent can AI experts vet
an AI system for adherence to regulatory requirements? We investigate this
question through the lens of two public sector procurement checklists,
identifying what we can do now, what should be possible with technical
innovation, and what requirements need a more interdisciplinary approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>scheduled for publication in the Communications of the ACM, titled
  "Directions of Technical Innovation for Regulatable AI Systems"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMP++: Motion Manifold Primitives with Parametric Curve Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17072v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17072v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghyeon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion Manifold Primitives (MMP), a manifold-based approach for encoding
basic motion skills, can produce diverse trajectories, enabling the system to
adapt to unseen constraints. Nonetheless, we argue that current MMP models lack
crucial functionalities of movement primitives, such as temporal and via-points
modulation, found in traditional approaches. This shortfall primarily stems
from MMP's reliance on discrete-time trajectories. To overcome these
limitations, we introduce Motion Manifold Primitives++ (MMP++), a new model
that integrates the strengths of both MMP and traditional methods by
incorporating parametric curve representations into the MMP framework.
Furthermore, we identify a significant challenge with MMP++: performance
degradation due to geometric distortions in the latent space, meaning that
similar motions are not closely positioned. To address this, Isometric Motion
Manifold Primitives++ (IMMP++) is proposed to ensure the latent space
accurately preserves the manifold's geometry. Our experimental results across
various applications, including 2-DoF planar motions, 7-DoF robot arm motions,
and SE(3) trajectory planning, show that MMP++ and IMMP++ outperform existing
methods in trajectory generation tasks, achieving substantial improvements in
some cases. Moreover, they enable the modulation of latent coordinates and
via-points, thereby allowing efficient online adaptation to dynamic
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages. This work has been submitted to the IEEE for possible
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regret-Based Defense in Adversarial Reinforcement Learning <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06912v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06912v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Belaire, Pradeep Varakantham, Thanh Nguyen, David Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable
to small adversarial noise in observations. Such adversarial noise can have
disastrous consequences in safety-critical environments. For instance, a
self-driving car receiving adversarially perturbed sensory observations about
nearby signs (e.g., a stop sign physically altered to be perceived as a speed
limit sign) or objects (e.g., cars altered to be recognized as trees) can be
fatal. Existing approaches for making RL algorithms robust to an
observation-perturbing adversary have focused on reactive approaches that
iteratively improve against adversarial examples generated at each iteration.
While such approaches have been shown to provide improvements over regular RL
methods, they are reactive and can fare significantly worse if certain
categories of adversarial examples are not generated during training. To that
end, we pursue a more proactive approach that relies on directly optimizing a
well-studied robustness measure, regret instead of expected value. We provide a
principled approach that minimizes maximum regret over a "neighborhood" of
observations to the received "observation". Our regret criterion can be used to
modify existing value- and policy-based Deep RL methods. We demonstrate that
our approaches provide a significant improvement in performance across a wide
variety of benchmarks against leading approaches for robust Deep RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAMAS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MA4DIV: Multi-Agent Reinforcement Learning for Search Result
  Diversification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqun Chen, Jiaxin Mao, Yi Zhang, Dehong Ma, Long Xia, Jun Fan, Daiting Shi, Zhicong Cheng, Simiu Gu, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of search result diversification (SRD) is to ensure that
selected documents cover as many different subtopics as possible. Existing
methods primarily utilize a paradigm of "greedy selection", i.e., selecting one
document with the highest diversity score at a time. These approaches tend to
be inefficient and are easily trapped in a suboptimal state. In addition, some
other methods aim to approximately optimize the diversity metric, such as
$\alpha$-NDCG, but the results still remain suboptimal. To address these
challenges, we introduce Multi-Agent reinforcement learning (MARL) for search
result DIVersity, which called MA4DIV. In this approach, each document is an
agent and the search result diversification is modeled as a cooperative task
among multiple agents. This approach allows for directly optimizing the
diversity metrics, such as $\alpha$-NDCG, while achieving high training
efficiency. We conducted preliminary experiments on public TREC datasets to
demonstrate the effectiveness and potential of MA4DIV. Considering the limited
number of queries in public TREC datasets, we construct a large-scale dataset
from industry sources and show that MA4DIV achieves substantial improvements in
both effectiveness and efficiency than existing baselines on a industrial scale
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs Are Few-Shot In-Context Low-Resource Language Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Cahyawijaya, Holy Lovenia, Pascale Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) empowers large language models (LLMs) to perform
diverse tasks in underrepresented languages using only short in-context
information, offering a crucial avenue for narrowing the gap between
high-resource and low-resource languages. Nonetheless, there is only a handful
of works explored ICL for low-resource languages with most of them focusing on
relatively high-resource languages, such as French and Spanish. In this work,
we extensively study ICL and its cross-lingual variation (X-ICL) on 25
low-resource and 7 relatively higher-resource languages. Our study not only
assesses the effectiveness of ICL with LLMs in low-resource languages but also
identifies the shortcomings of in-context label alignment, and introduces a
more effective alternative: query alignment. Moreover, we provide valuable
insights into various facets of ICL for low-resource languages. Our study
concludes the significance of few-shot in-context information on enhancing the
low-resource understanding quality of LLMs through semantically relevant
information by closing the language gap in the target language and aligning the
semantics between the targeted low-resource and the high-resource language that
the model is proficient in. Our work highlights the importance of advancing ICL
research, particularly for low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Programming Education with Chat<span class="highlight-title">GPT</span>: A Case Study on Student
  Perceptions and Interactions in a Python Course 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxaun Ma, Li Chen, Shin'ichi Konomi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of ChatGPT as a supportive tool in education, notably in
programming courses, addresses the unique challenges of programming education
by providing assistance with debugging, code generation, and explanations.
Despite existing research validating ChatGPT's effectiveness, its application
in university-level programming education and a detailed understanding of
student interactions and perspectives remain limited. This paper explores
ChatGPT's impact on learning in a Python programming course tailored for
first-year students over eight weeks. By analyzing responses from surveys,
open-ended questions, and student-ChatGPT dialog data, we aim to provide a
comprehensive view of ChatGPT's utility and identify both its advantages and
limitations as perceived by students. Our study uncovers a generally positive
reception toward ChatGPT and offers insights into its role in enhancing the
programming education experience. These findings contribute to the broader
discourse on AI's potential in education, suggesting paths for future research
and application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SSM Meets Video Diffusion Models: Efficient Video Generation with
  Structured State Spaces <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, Yutaka Matsuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the remarkable achievements in image generation through diffusion
models, the research community has shown increasing interest in extending these
models to video generation. Recent diffusion models for video generation have
predominantly utilized attention layers to extract temporal features. However,
attention layers are limited by their memory consumption, which increases
quadratically with the length of the sequence. This limitation presents
significant challenges when attempting to generate longer video sequences using
diffusion models. To overcome this challenge, we propose leveraging state-space
models (SSMs). SSMs have recently gained attention as viable alternatives due
to their linear memory consumption relative to sequence length. In the
experiments, we first evaluate our SSM-based model with UCF101, a standard
benchmark of video generation. In addition, to investigate the potential of
SSMs for longer video generation, we perform an experiment using the MineRL
Navigate dataset, varying the number of frames to 64, 200, and 400. In these
settings, our SSM-based model can considerably save memory consumption for
longer sequences, while maintaining competitive FVD scores to the
attention-based models. Our codes are available at
https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as workshop paper at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly Supervised AUC Optimization: A Unified Partial AUC Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Xie, Yu Liu, Hao-Yuan He, Ming Li, Zhi-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since acquiring perfect supervision is usually difficult, real-world machine
learning tasks often confront inaccurate, incomplete, or inexact supervision,
collectively referred to as weak supervision. In this work, we present WSAUC, a
unified framework for weakly supervised AUC optimization problems, which covers
noisy label learning, positive-unlabeled learning, multi-instance learning, and
semi-supervised learning scenarios. Within the WSAUC framework, we first frame
the AUC optimization problems in various weakly supervised scenarios as a
common formulation of minimizing the AUC risk on contaminated sets, and
demonstrate that the empirical risk minimization problems are consistent with
the true AUC. Then, we introduce a new type of partial AUC, specifically, the
reversed partial AUC (rpAUC), which serves as a robust training objective for
AUC maximization in the presence of contaminated labels. WSAUC offers a
universal solution for AUC optimization in various weakly supervised scenarios
by maximizing the empirical rpAUC. Theoretical and experimental results under
multiple settings support the effectiveness of WSAUC on a range of weakly
supervised AUC optimization tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate
  Professional and Non-Professional Styled Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zong, Yuyan Chen, Weiming Lu, Jian Shao, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated efficacy in various linguistic
applications, including text summarization and controlled text generation.
However, studies into their capacity of switching between styles via
fine-tuning remain underexplored. This study concentrates on textual
professionalism and introduces a novel methodology, named ProSwitch, which
equips a language model with the ability to produce both professional and
non-professional responses through knowledge-guided instruction tuning.
ProSwitch unfolds across three phases: data preparation for gathering domain
knowledge and training corpus; instruction tuning for optimizing language
models with multiple levels of instruction formats; and comprehensive
evaluation for assessing the professionalism discrimination and reference-based
quality of generated text. Comparative analysis of ProSwitch against both
general and specialized language models reveals that our approach outperforms
baselines in switching between professional and non-professional text
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Re2LLM: Reflective Reinforcement Large Language Model for Session-based
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16427v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16427v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong Feng, Wenya Wang, Jie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are emerging as promising approaches to enhance
session-based recommendation (SBR), where both prompt-based and
fine-tuning-based methods have been widely investigated to align LLMs with SBR.
However, the former methods struggle with optimal prompts to elicit the correct
reasoning of LLMs due to the lack of task-specific feedback, leading to
unsatisfactory recommendations. Although the latter methods attempt to
fine-tune LLMs with domain-specific knowledge, they face limitations such as
high computational costs and reliance on open-source backbones. To address such
issues, we propose a Reflective Reinforcement Large Language Model (Re2LLM) for
SBR, guiding LLMs to focus on specialized knowledge essential for more accurate
recommendations effectively and efficiently. In particular, we first design the
Reflective Exploration Module to effectively extract knowledge that is readily
understandable and digestible by LLMs. To be specific, we direct LLMs to
examine recommendation errors through self-reflection and construct a knowledge
base (KB) comprising hints capable of rectifying these errors. To efficiently
elicit the correct reasoning of LLMs, we further devise the Reinforcement
Utilization Module to train a lightweight retrieval agent. It learns to select
hints from the constructed KB based on the task-specific feedback, where the
hints can serve as guidance to help correct LLMs reasoning for better
recommendations. Extensive experiments on multiple real-world datasets
demonstrate that our method consistently outperforms state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue
  Systems <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04357v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04357v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenpeng Su, Xing Wu, Wei Zhou, Guangyuan Ma, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue response selection aims to select an appropriate response from
several candidates based on a given user and system utterance history. Most
existing works primarily focus on post-training and fine-tuning tailored for
cross-encoders. However, there are no post-training methods tailored for dense
encoders in dialogue response selection. We argue that when the current
language model, based on dense dialogue systems (such as BERT), is employed as
a dense encoder, it separately encodes dialogue context and response, leading
to a struggle to achieve the alignment of both representations. Thus, we
propose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward
yet effective post-training technique tailored for dense encoders in dialogue
response selection. Dial-MAE uses an asymmetric encoder-decoder architecture to
compress the dialogue semantics into dense vectors, which achieves better
alignment between the features of the dialogue context and response. Our
experiments have demonstrated that Dial-MAE is highly effective, achieving
state-of-the-art performance on two commonly evaluated benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoftTiger: A Clinical Foundation Model for Healthcare Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Chen, Igor Couto, Wei Cai, Cong Fu, Bruno Dorneles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SoftTiger, a clinical large language model (CLaM) designed as a
foundation model for healthcare workflows. The narrative and unstructured
nature of clinical notes is a major obstacle for healthcare intelligentization.
We address a critical problem of structuring clinical notes into clinical data,
according to international interoperability standards. We collect and annotate
data for three subtasks, namely, international patient summary, clinical
impression and medical encounter. We then supervised fine-tuned a
state-of-the-art LLM using public and credentialed clinical data. The training
is orchestrated in a way that the target model can first support basic clinical
tasks such as abbreviation expansion and temporal information extraction, and
then learn to perform more complex downstream clinical tasks. Moreover, we
address several modeling challenges in the healthcare context, e.g., extra long
context window. Our blind pairwise evaluation shows that SoftTiger outperforms
other popular open-source models and GPT-3.5, comparable to Gemini-pro, with a
mild gap from GPT-4. We believe that LLMs may become a step-stone towards
healthcare digitalization and democratization. Therefore, we publicly release
SoftTiger models at scales of 13 billion and 70 billion parameters, as well as
datasets and code for our innovative scalable evaluation, hopefully, making a
significant contribution to the healthcare industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probing Multimodal Large Language Models for Global and Local Semantic
  Representations <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxu Tao, Quzhe Huang, Kun Xu, Liwei Chen, Yansong Feng, Dongyan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of Multimodal Large Language Models (MLLMs) has greatly
accelerated the development of applications in understanding integrated texts
and images. Recent works leverage image-caption datasets to train MLLMs,
achieving state-of-the-art performance on image-to-text tasks. However, there
are few studies exploring which layers of MLLMs make the most effort to the
global image information, which plays vital roles in multimodal comprehension
and generation. In this study, we find that the intermediate layers of models
can encode more global semantic information, whose representation vectors
perform better on visual-language entailment tasks, rather than the topmost
layers. We further probe models regarding local semantic representations
through object recognition tasks. We find that the topmost layers may
excessively focus on local information, leading to a diminished ability to
encode global information. Our code and data are released via
https://github.com/kobayashikanna01/probing_MLLM_rep.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024 as a short paper (Camera Ready)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Distribution and Out-of-Distribution <span class="highlight-title">Self-supervised</span> ECG
  Representation Learning for Arrhythmia Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Soltanieh, Javad Hashemi, Ali Etemad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a systematic investigation into the effectiveness of
Self-Supervised Learning (SSL) methods for Electrocardiogram (ECG) arrhythmia
detection. We begin by conducting a novel analysis of the data distributions on
three popular ECG-based arrhythmia datasets: PTB-XL, Chapman, and Ribeiro. To
the best of our knowledge, our study is the first to quantitatively explore and
characterize these distributions in the area. We then perform a comprehensive
set of experiments using different augmentations and parameters to evaluate the
effectiveness of various SSL methods, namely SimCRL, BYOL, and SwAV, for ECG
representation learning, where we observe the best performance achieved by
SwAV. Furthermore, our analysis shows that SSL methods achieve highly
competitive results to those achieved by supervised state-of-the-art methods.
To further assess the performance of these methods on both In-Distribution (ID)
and Out-of-Distribution (OOD) ECG data, we conduct cross-dataset training and
testing experiments. Our comprehensive experiments show almost identical
results when comparing ID and OOD schemes, indicating that SSL techniques can
learn highly effective representations that generalize well across different
OOD datasets. This finding can have major implications for ECG-based arrhythmia
detection. Lastly, to further analyze our results, we perform detailed
per-disease studies on the performance of the SSL methods on the three
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been published in the IEEE Journal of Biomedical and
  Health Informatics (JBHI). Copyright IEEE. Please cite as: S. Soltanieh, J.
  Hashemi and A. Etemad, "In-Distribution and Out-of-Distribution
  Self-Supervised ECG Representation Learning for Arrhythmia Detection," in
  IEEE Journal of Biomedical and Health Informatics, vol. 28, no. 2, pp.
  789-800, Feb. 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imitating Cost-Constrained Behaviors in Reinforcement Learning <span class="chip">ICAPS-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17456v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17456v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Shao, Pradeep Varakantham, Shih-Fen Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex planning and scheduling problems have long been solved using various
optimization or heuristic approaches. In recent years, imitation learning that
aims to learn from expert demonstrations has been proposed as a viable
alternative to solving these problems. Generally speaking, imitation learning
is designed to learn either the reward (or preference) model or directly the
behavioral policy by observing the behavior of an expert. Existing work in
imitation learning and inverse reinforcement learning has focused on imitation
primarily in unconstrained settings (e.g., no limit on fuel consumed by the
vehicle). However, in many real-world domains, the behavior of an expert is
governed not only by reward (or preference) but also by constraints. For
instance, decisions on self-driving delivery vehicles are dependent not only on
the route preferences/rewards (depending on past demand data) but also on the
fuel in the vehicle and the time available. In such problems, imitation
learning is challenging as decisions are not only dictated by the reward model
but are also dependent on a cost-constrained model. In this paper, we provide
multiple methods that match expert distributions in the presence of trajectory
cost constraints through (a) Lagrangian-based method; (b) Meta-gradients to
find a good trade-off between expected return and minimizing constraint
violation; and (c) Cost-violation-based alternating gradient. We empirically
show that leading imitation learning approaches imitate cost-constrained
behaviors poorly and our meta-gradient-based approach achieves the best
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 34th International Conference on Automated Planning
  and Scheduling (ICAPS-24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Innovation Paradox: Concept Space Expansion with Diminishing
  Originality and the Promise of Creative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13300v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13300v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serhad Sarica, Jianxi Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Innovation, typically spurred by reusing, recombining, and synthesizing
existing concepts, is expected to result in an exponential growth of the
concept space over time. However, our statistical analysis of TechNet, which is
a comprehensive technology semantic network encompassing over four million
concepts derived from patent texts, reveals a linear rather than exponential
expansion of the overall technological concept space. Moreover, there is a
notable decline in the originality of newly created concepts. These trends can
be attributed to the constraints of human cognitive abilities to innovate
beyond an ever-growing space of prior art, among other factors. Integrating
creative artificial intelligence (CAI) into the innovation process holds the
potential to overcome these limitations and alter the observed trends in the
future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Forthcoming on the Design Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs in Political Science: Heralding a New Era of Visual Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00154v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00154v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interest is increasing among political scientists in leveraging the extensive
information available in images. However, the challenge of interpreting these
images lies in the need for specialized knowledge in computer vision and access
to specialized hardware. As a result, image analysis has been limited to a
relatively small group within the political science community. This landscape
could potentially change thanks to the rise of large language models (LLMs).
This paper aims to raise awareness of the feasibility of using Gemini for image
content analysis. A retrospective analysis was conducted on a corpus of 688
images. Content reports were elicited from Gemini for each image and then
manually evaluated by the authors. We find that Gemini is highly accurate in
performing object detection, which is arguably the most common and fundamental
task in image analysis for political scientists. Equally important, we show
that it is easy to implement as the entire command consists of a single prompt
in natural language; it is fast to run and should meet the time budget of most
researchers; and it is free to use and does not require any specialized
hardware. In addition, we illustrate how political scientists can leverage
Gemini for other image understanding tasks, including face identification,
sentiment analysis, and caption generation. Our findings suggest that Gemini
and other similar LLMs have the potential to drastically stimulate and
accelerate image research in political science and social sciences more
broadly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-Tuning for Ad-hoc Document Retrieval Using <span class="highlight-title">Pre-train</span>ed Language
  Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Keyaki, Ribeka Keyaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning in information retrieval systems using pre-trained language
models (PLM-based IR) requires learning query representations and
query-document relations, in addition to downstream task-specific learning.
This study introduces coarse-tuning as an intermediate learning stage that
bridges pre-training and fine-tuning. By learning query representations and
query-document relations in coarse-tuning, we aim to reduce the load of
fine-tuning and improve the learning effect of downstream IR tasks. We propose
Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the
appropriateness of query-document pairs. Evaluation experiments show that the
proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc
document retrieval datasets. Furthermore, the results of the query prediction
task suggested that coarse-tuning facilitated learning of query representation
and query-document relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Look Before You Leap: Problem Elaboration <span class="highlight-title">Prompt</span>ing Improves
  Mathematical Reasoning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Liao, Jidong Tian, Shaohua Hu, Hao He, Yaohui Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) still grapple with complex tasks like
mathematical reasoning. Despite significant efforts invested in improving
prefix prompts or reasoning process, the crucial role of problem context might
have been neglected. Accurate recognition of inputs is fundamental for solving
mathematical tasks, as ill-formed problems could potentially mislead LLM's
reasoning. In this study, we propose a new approach named Problem Elaboration
Prompting (PEP) to enhance the mathematical capacities of LLMs. Specifically,
PEP decomposes and elucidates the problem context before reasoning, therefore
enhancing the context modeling and parsing efficiency. Experiments across
datasets and models demonstrate promising performances: (1) PEP demonstrates an
overall enhancement in various mathematical tasks. For instance, with the
GPT-3.5 model, PEP exhibits improvements of 9.93% and 8.80% on GSM8k through
greedy decoding and self-consistency, respectively. (2) PEP can be easily
implemented and integrated with other prompting methods. (3) PEP shows
particular strength in handling distraction problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Follower Agnostic Methods for Stackelberg Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01421v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01421v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Maheshwari, James Cheng, S. Shankar Sasty, Lillian Ratliff, Eric Mazumdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present an efficient algorithm to solve online Stackelberg
games, featuring multiple followers, in a follower-agnostic manner. Unlike
previous works, our approach works even when leader has no knowledge about the
followers' utility functions or strategy space. Our algorithm introduces a
unique gradient estimator, leveraging specially designed strategies to probe
followers. In a departure from traditional assumptions of optimal play, we
model followers' responses using a convergent adaptation rule, allowing for
realistic and dynamic interactions. The leader constructs the gradient
estimator solely based on observations of followers' actions. We provide both
non-asymptotic convergence rates to stationary points of the leader's objective
and demonstrate asymptotic convergence to a \emph{local Stackelberg
equilibrium}. To validate the effectiveness of our algorithm, we use this
algorithm to solve the problem of incentive design on a large-scale
transportation network, showcasing its robustness even when the leader lacks
access to followers' demand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Act without Actions <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Schmidt, Minqi Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training large models on vast amounts of web data has proven to be an
effective approach for obtaining powerful, general models in domains such as
language and vision. However, this paradigm has not yet taken hold in
reinforcement learning. This is because videos, the most abundant form of
embodied behavioral data on the web, lack the action labels required by
existing methods for imitating behavior from demonstrations. We introduce
Latent Action Policies (LAPO), a method for recovering latent action
information, and thereby latent-action policies, world models, and inverse
dynamics models, purely from videos. LAPO is the first method able to recover
the structure of the true action space just from observed dynamics, even in
challenging procedurally-generated environments. LAPO enables training
latent-action policies that can be rapidly fine-tuned into expert-level
policies, either offline using a small action-labeled dataset, or online with
rewards. LAPO takes a first step towards pre-training powerful, generalist
policies and world models on the vast amounts of videos readily available on
the web.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2024 (spotlight). The code can be found at
  http://github.com/schmidtdominik/LAPO</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Signal Processing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Characterization of Spatial-Temporal Channel Statistics from Indoor
  Measurement Data at D Band 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chathuri Weragama, Joonas Kokkoniemi, Mar Francis De Guzman, Katsuyuki Haneda, Pekka Kyosti, Markku Juntti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Millimeter-wave (mmWave) and D Band (110--170~GHz) frequencies are poised to
play a pivotal role in the advancement of sixth-generation (6G) systems and
beyond, owing to their ability to enhance performance metrics such as capacity,
ultra-low latency, and spectral efficiency. This paper concentrates on deriving
statistical insights into power, delay, and the number of paths based on
measurements conducted across four distinct locations at a center frequency of
143.1 GHz. The findings underscore the suitability of various distributions in
characterizing power behavior in line-of-sight (LOS) scenarios, including
lognormal, Nakagami, gamma, and beta distributions, whereas the loglogistic
distribution gives the optimal fit for power distribution in non-line-of-sight
(NLOS) scenarios. Moreover, the exponential distribution shows to be the most
appropriate model for the delay distribution in both LOS and NLOS scenarios. In
terms of the number of paths, observations indicate a tendency for the highest
concentration within the 10 m to 30 m distance range between the transmitter
(Tx) and receiver (Rx). These insights shed light on the statistical nature of
D band propagation characteristics, which are vital for informing the design
and optimization of future 6G communication systems
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic
  Learning over Low-power Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Fu, Gaolei Li, Jun Wu, Jianhua Li, Xi Lin, Kai Zhou, Yuchen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated neuromorphic learning (FedNL) leverages event-driven spiking neural
networks and federated learning frameworks to effectively execute intelligent
analysis tasks over amounts of distributed low-power devices but also perform
vulnerability to poisoning attacks. The threat of backdoor attacks on
traditional deep neural networks typically comes from time-invariant data.
However, in FedNL, unknown threats may be hidden in time-varying spike signals.
In this paper, we start to explore a novel vulnerability of FedNL-based systems
with the concept of time division multiplexing, termed Spikewhisper, which
allows attackers to evade detection as much as possible, as multiple malicious
clients can imperceptibly poison with different triggers at different
timeslices. In particular, the stealthiness of Spikewhisper is derived from the
time-domain divisibility of global triggers, in which each malicious client
pastes only one local trigger to a certain timeslice in the neuromorphic
sample, and also the polarity and motion of each local trigger can be
configured by attackers. Extensive experiments based on two different
neuromorphic datasets demonstrate that the attack success rate of Spikewispher
is higher than the temporally centralized attacks. Besides, it is validated
that the effect of Spikewispher is sensitive to the trigger duration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Maximum Consensus over Noisy Links 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Lari, Reza Arablouei, Naveen K. D. Venkategowda, Stefan Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a distributed algorithm, termed noise-robust distributed maximum
consensus (RD-MC), for estimating the maximum value within a multi-agent
network in the presence of noisy communication links. Our approach entails
redefining the maximum consensus problem as a distributed optimization problem,
allowing a solution using the alternating direction method of multipliers.
Unlike existing algorithms that rely on multiple sets of noise-corrupted
estimates, RD-MC employs a single set, enhancing both robustness and
efficiency. To further mitigate the effects of link noise and improve
robustness, we apply moving averaging to the local estimates. Through extensive
simulations, we demonstrate that RD-MC is significantly more robust to
communication link noise compared to existing maximum-consensus algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures, submitted to EUSIPCO 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing EEG Signals from Event-Related Potential Paradigms with
  Conditional Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guido Klein, Pierre Guetschel, Gianluigi Silvestri, Michael Tangermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity in the brain-computer interface field can be alleviated through
the use of generative models, specifically diffusion models. While diffusion
models have previously been successfully applied to electroencephalogram (EEG)
data, existing models lack flexibility w.r.t.~sampling or require alternative
representations of the EEG data. To overcome these limitations, we introduce a
novel approach to conditional diffusion models that utilizes classifier-free
guidance to directly generate subject-, session-, and class-specific EEG data.
In addition to commonly used metrics, domain-specific metrics are employed to
evaluate the specificity of the generated samples. The results indicate that
the proposed model can generate EEG data that resembles real data for each
subject, session, and class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to 9th Graz BCI conference, 6 pages, 3 figures, first
  figure is split into two subfigures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asymptotic Analysis of Synchronous Signal Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Vilà-Insa, Jaume Riba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper extends various theoretical results from stationary data
processing to cyclostationary (CS) processes under a unified framework. We
first derive their asymptotic eigenbasis, which provides a link between their
Fourier and Karhunen-Lo\`eve (KL) expansions, through a unitary transformation
dictated by the cyclic spectrum. By exploiting this connection and the
optimalities offered by the KL representation, we study the asymptotic
performance of smoothing, filtering and prediction of CS processes, without the
need for deriving explicit implementations. We obtain minimum mean squared
error expressions that depend on the cyclic spectrum and include classical
limits based on the power spectral density as particular cases. We conclude
this work by applying the results to a practical scenario, in order to quantify
the achievable gains of synchronous signal processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures, submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stragglers-Aware Low-Latency Synchronous Federated Learning via
  Layer-Wise Model Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natalie Lang, Alejandro Cohen, Nir Shlezinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synchronous federated learning (FL) is a popular paradigm for collaborative
edge learning. It typically involves a set of heterogeneous devices locally
training neural network (NN) models in parallel with periodic centralized
aggregations. As some of the devices may have limited computational resources
and varying availability, FL latency is highly sensitive to stragglers.
Conventional approaches discard incomplete intra-model updates done by
stragglers, alter the amount of local workload and architecture, or resort to
asynchronous settings; which all affect the trained model performance under
tight training latency constraints. In this work, we propose straggler-aware
layer-wise federated learning (SALF) that leverages the optimization procedure
of NNs via backpropagation to update the global model in a layer-wise fashion.
SALF allows stragglers to synchronously convey partial gradients, having each
layer of the global model be updated independently with a different
contributing set of users. We provide a theoretical analysis, establishing
convergence guarantees for the global model under mild assumptions on the
distribution of the participating devices, revealing that SALF converges at the
same asymptotic rate as FL with no timing limitations. This insight is matched
with empirical observations, demonstrating the performance gains of SALF
compared to alternative mechanisms mitigating the device heterogeneity gap in
FL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving Distributed Nonnegative Matrix Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Lari, Reza Arablouei, Stefan Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonnegative matrix factorization (NMF) is an effective data representation
tool with numerous applications in signal processing and machine learning.
However, deploying NMF in a decentralized manner over ad-hoc networks
introduces privacy concerns due to the conventional approach of sharing raw
data among network agents. To address this, we propose a privacy-preserving
algorithm for fully-distributed NMF that decomposes a distributed large data
matrix into left and right matrix factors while safeguarding each agent's local
data privacy. It facilitates collaborative estimation of the left matrix factor
among agents and enables them to estimate their respective right factors
without exposing raw data. To ensure data privacy, we secure information
exchanges between neighboring agents utilizing the Paillier cryptosystem, a
probabilistic asymmetric algorithm for public-key cryptography that allows
computations on encrypted data without decryption. Simulation results conducted
on synthetic and real-world datasets demonstrate the effectiveness of the
proposed algorithm in achieving privacy-preserving distributed NMF over ad-hoc
networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, submitted to EUSIPCO 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UAV Corridor Coverage Analysis with Base Station Antenna Uptilt and
  Strongest Signal Association 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sung Joon Maeng, İsmail Güvenç
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned aerial vehicle (UAV) corridors are sky lanes where UAVs fly through
safely between their origin and destination. To ensure the successful operation
of UAV corridors, beyond visual line of sight (BVLOS) wireless connectivity
within the corridor is crucial. One promising solution to support this is the
use of cellular-connected UAV (C-UAV) networks, which offer long-range and
seamless wireless coverage. However, conventional terrestrial base stations
(BSs) that typically employ down-tilted sector antennas to serve ground users
are not ideally suited to serve the aerial vehicles positioned above the BSs.
In our previous work, we focused on studying the optimal uptilt angle of BS
antennas to maximize the wireless coverage probability in UAV corridors.
However, the association of BSs with UAVs was restricted to the nearest BS
association, which limits the potential coverage benefits. In this paper, we
address this limitation by considering the strongest BS signal association in
UAV corridors, which enables enhanced coverage within the corridor compared to
the nearest BS association. The strongest BS association allows UAVs to connect
with the second nearest BSs while also accounting for interference from the
third nearest BSs. Closed-form expression analysis and simulation results show
that the strongest BSs association in UAV corridors yields a superior coverage
probability when compared to the nearest BS association.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mutual Information Optimization for SIM-Based Holographic MIMO Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nemanja Stefan Perović, Le-Nam Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of emerging stacked intelligent metasurface (SIM)-based
holographic MIMO (HMIMO) systems, a fundamental problem is to study the mutual
information (MI) between transmitted and received signals to establish their
capacity. However, direct optimization or analytical evaluation of the MI,
particularly for discrete signaling, is often intractable. To address this
challenge, we adopt the channel cutoff rate (CR) as an alternative optimization
metric for the MI maximization. In this regard, we propose an alternating
projected gradient method (APGM), which optimizes the CR of a SIM-based HMIMO
system by adjusting signal precoding and the phase shifts across the transmit
and receive SIMs in a layer-by-layer basis. Simulation results indicate that
the proposed algorithm significantly enhances the CR, achieving substantial
gains proportional to those observed for the corresponding MI. This justifies
the effectiveness of using the channel CR for the MI optimization. Moreover, we
demonstrate that the integration of digital precoding, even on a modest scale,
has a significant impact on the ultimate performance of SIM-aided systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic
  Communication Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhang Zheng, Kechao Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to semantic communication tasks rely on the knowledge
of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these
methods necessitate training under specific SNR conditions, entailing
considerable time and computational resources. In this paper, we propose GeNet,
a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at
combating noise, thereby facilitating Task-Oriented Communication (TOC). We
propose a novel approach where we first transform the input data image into
graph structures. Then we leverage a GNN-based encoder to extract semantic
information from the source data. This extracted semantic information is then
transmitted through the channel. At the receiver's end, a GNN-based decoder is
utilized to reconstruct the relevant semantic information from the source data
for TOC. Through experimental evaluation, we show GeNet's effectiveness in
anti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet's
performance by varying the number of nodes, revealing its versatility as a new
paradigm for semantic communication. Additionally, we show GeNet's robustness
to geometric transformations by testing it with different rotation angles,
without resorting to data augmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Data Consistency with Diffusion Purification for Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06054v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06054v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishankar, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently gained traction as a powerful class of deep
generative priors, excelling in a wide range of image restoration tasks due to
their exceptional ability to model data distributions. To solve image
restoration problems, many existing techniques achieve data consistency by
incorporating additional likelihood gradient steps into the reverse sampling
process of diffusion models. However, the additional gradient steps pose a
challenge for real-world practical applications as they incur a large
computational overhead, thereby increasing inference time. They also present
additional difficulties when using accelerated diffusion model samplers, as the
number of data consistency steps is limited by the number of reverse sampling
steps. In this work, we propose a novel diffusion-based image restoration
solver that addresses these issues by decoupling the reverse process from the
data consistency steps. Our method involves alternating between a
reconstruction phase to maintain data consistency and a refinement phase that
enforces the prior via diffusion purification. Our approach demonstrates
versatility, making it highly adaptable for efficient problem-solving in latent
space. Additionally, it reduces the necessity for numerous sampling steps
through the integration of consistency models. The efficacy of our approach is
validated through comprehensive experiments across various image restoration
tasks, including image denoising, deblurring, inpainting, and super-resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Static Target Detection and Power Allocation for Integrated
  Sensing and Communication in Cell-Free Massive MIMO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zinat Behdad, Özlem Tuğfe Demir, Ki Won Sung, Emil Björnson, Cicek Cavdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies an integrated sensing and communication (ISAC) system
within a centralized cell-free massive MIMO (multiple-input multiple-output)
network for target detection. ISAC transmit access points serve the user
equipments in the downlink and optionally steer a beam toward the target in a
multi-static sensing framework. A maximum a posteriori ratio test detector is
developed for target detection in the presence of clutter, so-called
target-free signals. Additionally, sensing spectral efficiency (SE) is
introduced as a key metric, capturing the impact of resource utilization in
ISAC. A power allocation algorithm is proposed to maximize the sensing
signal-to-interference-plus-noise ratio while ensuring minimum communication
requirements. Two ISAC configurations are studied: utilizing existing
communication beams for sensing and using additional sensing beams. The
proposed algorithm's efficiency is investigated in realistic and idealistic
scenarios, corresponding to the presence and absence of the target-free
channels, respectively. Despite performance degradation in the presence of
target-free channels, the proposed algorithm outperforms the
interference-unaware benchmark, leveraging clutter statistics. Comparisons with
a fully communication-centric algorithm reveal superior performance in both
cluttered and clutter-free environments. The incorporation of an extra sensing
beam enhances detection performance for lower radar cross-section variances.
Moreover, the results demonstrate the effectiveness of the integrated operation
of sensing and communication compared to an orthogonal resource-sharing
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Near-Field EM-Based Multistatic Radar Range Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        François De Saint Moulin, Guillaume Thiran, Christophe Craeye, Luc Vandendorpe, Claude Oestges
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radar targets are traditionally modelled as point target reflectors, even in
the near-field region. Yet, for radar systems operating at high carrier
frequencies and small distances, traditional radar propagation models do not
accurately model the scatterer responses. In this paper, a novel
electromagnetic-based model is thus developed for the multistatic radar
detection of a rectangular plate reflector in the near-field region. This model
is applied to an automotive scenario, in which a linear antenna array is spread
out at the front of a vehicle, and performs a radar measurement of the distance
to the back of the vehicle ahead. Based on the developed received signal model,
the maximum likelihood estimator of the range is designed. By exploiting the
near-field target model, this estimator is shown to provide a significant gain
with respect to traditional range estimators. The impact of the system and
scenario parameters, i.e. the carrier frequency, bandwidth and distance to the
target, is furthermore evaluated. This analysis shows that the radar resolution
in the near-field regime is improved at high carrier frequencies, while
saturating to the traditional bandwidth-dependent resolution in the far-field
region.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Performance, Calibration Time and Efficiency in Brain-Machine
  Interfaces through Transfer Learning and Wearable EEG Technology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07798v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07798v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaying Wang, Lan Mei, Victor Kartsch, Andrea Cossettini, Luca Benini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain-machine interfaces (BMIs) have emerged as a transformative force in
assistive technologies, empowering individuals with motor impairments by
enabling device control and facilitating functional recovery. However, the
persistent challenge of inter-session variability poses a significant hurdle,
requiring time-consuming calibration at every new use. Compounding this issue,
the low comfort level of current devices further restricts their usage. To
address these challenges, we propose a comprehensive solution that combines a
tiny CNN-based Transfer Learning (TL) approach with a comfortable, wearable EEG
headband. The novel wearable EEG device features soft dry electrodes placed on
the headband and is capable of on-board processing. We acquire multiple
sessions of motor-movement EEG data and achieve up to 96% inter-session
accuracy using TL, greatly reducing the calibration time and improving
usability. By executing the inference on the edge every 100ms, the system is
estimated to achieve 30h of battery life. The comfortable BMI setup with tiny
CNN and TL paves the way to future on-device continual learning, essential for
tackling inter-session variability and improving usability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Tutorial on 5G Positioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Italiano, Bernardo Camajori Tedeschini, Mattia Brambilla, Huiping Huang, Monica Nicoli, Henk Wymeersch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of the fifth generation (5G) of cellular networks has
brought new opportunities for the development of localization-based services.
High-accuracy positioning use cases and functionalities defined by the
standards are drawing the interest of vertical industries. In the transition
towards the deployment, this paper aims to provide an in-depth tutorial on 5G
positioning, summarizing the evolutionary path that led to the standardization
of cellular-based positioning, describing the localization elements in current
and forthcoming releases of the Third Generation Partnership Project (3GPP)
standard, and the major research trends. By providing fundamental notions on
wireless localization, comprehensive definitions of measurements and
architectures, examples of algorithms, and details on simulation approaches,
this paper is intended to represent an exhaustive guide for researchers and
practitioners. Our approach aims to merge practical aspects of enabled use
cases and related requirements with theoretical methodologies and fundamental
bounds, allowing to understand the trade-off between system complexity and
achievable, i.e., tangible, benefits of 5G positioning services. We analyze the
performance of 3GPP Rel-16 positioning by standard-compliant simulations in
realistic outdoor and indoor propagation environments, investigating the impact
of the system configuration and the limitations to be resolved for delivering
accurate positioning solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE Communications Surveys &
  Tutorials for possible publication. Copyright may be transferred without
  notice, after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Chen, Chao Tang, Amir Aghabiglou, Chung San Chu, Yves Wiaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for non-Cartesian magnetic resonance image
reconstruction. While unrolled architectures provide robustness via
data-consistency layers, embedding measurement operators in Deep Neural Network
(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)
approaches, where the denoising DNNs are blind to the measurement setting, are
not affected by this limitation and have also proven effective, but their
highly iterative nature also affects scalability. To address this scalability
challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic
range imaging (R2D2)" approach recently introduced in astronomical imaging.
R2D2's reconstruction is formed as a series of residual images, iteratively
estimated as outputs of DNNs taking the previous iteration's image estimate and
associated data residual as inputs. The method can be interpreted as a learned
version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,
considering radial k-space sampling acquisition sequences. Our preliminary
results suggest that R2D2 achieves: (i) suboptimal performance compared to its
unrolled incarnation R2D2-Net, which is however non-scalable due to the
necessary embedding of NUFFT-based data-consistency layers; (ii) superior
reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based
approximation for data consistency; (iii) superior reconstruction quality to
PnP, while only requiring few iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards 6G Digital Twin Channel Using Radio Environment Knowledge Pool 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10287v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10287v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialin Wang, Jianhua Zhang, Yuxiang Zhang, Yutong Sun,  Gaofeng,  Nie, Lianzheng Shi, Ping Zhang, Guangyi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The digital twin channel (DTC) is crucial for 6G wireless autonomous networks
as it replicates the wireless channel fading states in 6G air interface
transmissions. It is well known that the physical environment influences
channels. A key task for accurately twinning channels in complex 6G scenarios
is establishing precise relationships between the environment and the channels.
In this article, the radio environment knowledge pool (REKP) is proposed, with
its core function being to construct and store as much knowledge between the
environment and channels as possible. Firstly, the research progress related to
DTC is summarized, and a comparative analysis of these achievements on key
indicators in digital twin is conducted, proposing the challenges faced in
knowledge construction. Secondly, instructions on how to construct and update
REKP are given. Then, a typical case is presented to demonstrate the great
potential of REKP in enabling DTC. Finally, how to utilize REKP to address open
issues in the 6G wireless communication system is discussed, including
enhancing performance, reducing costs, and keeping a trustworthy DTC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Distribution and Out-of-Distribution <span class="highlight-title">Self-supervised</span> ECG
  Representation Learning for Arrhythmia Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Soltanieh, Javad Hashemi, Ali Etemad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a systematic investigation into the effectiveness of
Self-Supervised Learning (SSL) methods for Electrocardiogram (ECG) arrhythmia
detection. We begin by conducting a novel analysis of the data distributions on
three popular ECG-based arrhythmia datasets: PTB-XL, Chapman, and Ribeiro. To
the best of our knowledge, our study is the first to quantitatively explore and
characterize these distributions in the area. We then perform a comprehensive
set of experiments using different augmentations and parameters to evaluate the
effectiveness of various SSL methods, namely SimCRL, BYOL, and SwAV, for ECG
representation learning, where we observe the best performance achieved by
SwAV. Furthermore, our analysis shows that SSL methods achieve highly
competitive results to those achieved by supervised state-of-the-art methods.
To further assess the performance of these methods on both In-Distribution (ID)
and Out-of-Distribution (OOD) ECG data, we conduct cross-dataset training and
testing experiments. Our comprehensive experiments show almost identical
results when comparing ID and OOD schemes, indicating that SSL techniques can
learn highly effective representations that generalize well across different
OOD datasets. This finding can have major implications for ECG-based arrhythmia
detection. Lastly, to further analyze our results, we perform detailed
per-disease studies on the performance of the SSL methods on the three
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been published in the IEEE Journal of Biomedical and
  Health Informatics (JBHI). Copyright IEEE. Please cite as: S. Soltanieh, J.
  Hashemi and A. Etemad, "In-Distribution and Out-of-Distribution
  Self-Supervised ECG Representation Learning for Arrhythmia Detection," in
  IEEE Journal of Biomedical and Health Informatics, vol. 28, no. 2, pp.
  789-800, Feb. 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Systems and Control
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Usage-Specific Survival Modeling Based on Operational Data and Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olov Holmer, Mattias Krysander, Erik Frisk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate predictions of when a component will fail are crucial when planning
maintenance, and by modeling the distribution of these failure times, survival
models have shown to be particularly useful in this context. The presented
methodology is based on conventional neural network-based survival models that
are trained using data that is continuously gathered and stored at specific
times, called snapshots. An important property of this type of training data is
that it can contain more than one snapshot from a specific individual which
results in that standard maximum likelihood training can not be directly
applied since the data is not independent. However, the papers show that if the
data is in a specific format where all snapshot times are the same for all
individuals, called homogeneously sampled, maximum likelihood training can be
applied and produce desirable results. In many cases, the data is not
homogeneously sampled and in this case, it is proposed to resample the data to
make it homogeneously sampled. How densely the dataset is sampled turns out to
be an important parameter; it should be chosen large enough to produce good
results, but this also increases the size of the dataset which makes training
slow. To reduce the number of samples needed during training, the paper also
proposes a technique to, instead of resampling the dataset once before the
training starts, randomly resample the dataset at the start of each epoch
during the training. The proposed methodology is evaluated on both a simulated
dataset and an experimental dataset of starter battery failures. The results
show that if the data is homogeneously sampled the methodology works as
intended and produces accurate survival models. The results also show that
randomly resampling the dataset on each epoch is an effective way to reduce the
size of the training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Connections between Reachability and Time Optimality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juho Bae, Ji Hoon Bai, Byung-Yoon Lee, Jun-Yong Lee, Chang-Hun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the concept of an equivalence relation between the set of
optimal control problems. By leveraging this concept, we show that the boundary
of the reachability set can be constructed by the solutions of time optimal
problems. Alongside, a more generalized equivalence theorem is presented
together. The findings facilitate the use of solution structures from a certain
class of optimal control problems to address problems in corresponding
equivalent classes. As a byproduct, we state and prove the construction methods
of the reachability sets of three-dimensional curves with prescribed curvature
bound. The findings are twofold: Firstly, we prove that any boundary point of
the reachability set, with the terminal direction taken into account, can be
accessed via curves of H, CSC, CCC, or their respective subsegments, where H
denotes a helicoidal arc, C a circular arc with maximum curvature, and S a
straight segment. Secondly, we show that any boundary point of the reachability
set, without considering the terminal direction, can be accessed by curves of
CC, CS, or their respective subsegments. These findings extend the developments
presented in literature regarding planar curves, or Dubins car dynamics, into
spatial curves in $\mathbb{R}^3$. For higher dimensions, we confirm that the
problem of identifying the reachability set of curvature bounded paths subsumes
the well-known Markov-Dubins problem. These advancements in understanding the
reachability of curvature bounded paths in $\mathbb{R}^3$ hold significant
practical implications, particularly in the contexts of mission planning
problems and time optimal guidance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Automatica</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fpga-Based Neural Thrust Controller for UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharif Azem, David Scheunert, Mengguang Li, Jonas Gehrunger, Kai Cui, Christian Hochberger, Heinz Koepp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of unmanned aerial vehicles (UAVs) has improved a variety of
fields by providing a versatile, cost-effective and accessible platform for
implementing state-of-the-art algorithms. To accomplish a broader range of
tasks, there is a growing need for enhanced on-board computing to cope with
increasing complexity and dynamic environmental conditions. Recent advances
have seen the application of Deep Neural Networks (DNNs), particularly in
combination with Reinforcement Learning (RL), to improve the adaptability and
performance of UAVs, especially in unknown environments. However, the
computational requirements of DNNs pose a challenge to the limited computing
resources available on many UAVs. This work explores the use of Field
Programmable Gate Arrays (FPGAs) as a viable solution to this challenge,
offering flexibility, high performance, energy and time efficiency. We propose
a novel hardware board equipped with an Artix-7 FPGA for a popular open-source
micro-UAV platform. We successfully validate its functionality by implementing
an RL-based low-level controller using real-world experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient Risk-aware Branch MPC for Automated Driving that is Robust
  to Uncertain Vehicle Behaviors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyao Zhang, George Pantazis, Shaohang Han, Sergio Grammatico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the critical challenges in automated driving is ensuring safety of
automated vehicles despite the unknown behavior of the other vehicles. Although
motion prediction modules are able to generate a probability distribution
associated with various behavior modes, their probabilistic estimates are often
inaccurate, thus leading to a possibly unsafe trajectory. To overcome this
challenge, we propose a risk-aware motion planning framework that appropriately
accounts for the ambiguity in the estimated probability distribution. We
formulate the risk-aware motion planning problem as a min-max optimization
problem and develop an efficient iterative method by incorporating a
regularization term in the probability update step. Via extensive numerical
studies, we validate the convergence of our method and demonstrate its
advantages compared to the state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Network-Based Piecewise Survival Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olov Holmer, Erik Frisk, Mattias Krysander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a family of neural network-based survival models is presented.
The models are specified based on piecewise definitions of the hazard function
and the density function on a partitioning of the time; both constant and
linear piecewise definitions are presented, resulting in a family of four
models. The models can be seen as an extension of the commonly used
discrete-time and piecewise exponential models and thereby add flexibility to
this set of standard models. Using a simulated dataset the models are shown to
perform well compared to the highly expressive, state-of-the-art energy-based
model, while only requiring a fraction of the computation time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Data Annotation Challenges in Multiple Sensors: A Solution
  for Scania Collected <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajinkya Khoche, Aron Asefaw, Alejandro Gonzalez, Bogdan Timus, Sina Sharif Mansouri, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data annotation in autonomous vehicles is a critical step in the development
of Deep Neural Network (DNN) based models or the performance evaluation of the
perception system. This often takes the form of adding 3D bounding boxes on
time-sequential and registered series of point-sets captured from active
sensors like Light Detection and Ranging (LiDAR) and Radio Detection and
Ranging (RADAR). When annotating multiple active sensors, there is a need to
motion compensate and translate the points to a consistent coordinate frame and
timestamp respectively. However, highly dynamic objects pose a unique
challenge, as they can appear at different timestamps in each sensor's data.
Without knowing the speed of the objects, their position appears to be
different in different sensor outputs. Thus, even after motion compensation,
highly dynamic objects are not matched from multiple sensors in the same frame,
and human annotators struggle to add unique bounding boxes that capture all
objects. This article focuses on addressing this challenge, primarily within
the context of Scania collected datasets. The proposed solution takes a track
of an annotated object as input and uses the Moving Horizon Estimation (MHE) to
robustly estimate its speed. The estimated speed profile is utilized to correct
the position of the annotated box and add boxes to object clusters missed by
the original annotation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to European Control Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Control Synthesis of Markov Decision Processes for Efficiency
  with Surveillance Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Chen, Xuanyuan Yin, Shaoyuan Li, Xiang Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the problem of optimal control synthesis for Markov Decision
Processes (MDPs), addressing both qualitative and quantitative objectives.
Specifically, we require the system to fulfill a qualitative surveillance task
in the sense that a specific region of interest can be visited infinitely often
with probability one. Furthermore, to quantify the performance of the system,
we consider the concept of efficiency, which is defined as the ratio between
rewards and costs. This measure is more general than the standard long-run
average reward metric as it aims to maximize the reward obtained per unit cost.
Our objective is to synthesize a control policy that ensures the surveillance
task while maximizes the efficiency. We provide an effective approach to
synthesize a stationary control policy achieving $\epsilon$-optimality by
integrating state classifications of MDPs and perturbation analysis in a novel
manner. Our results generalize existing works on efficiency-optimal control
synthesis for MDP by incorporating qualitative surveillance tasks. A robot
motion planning case study is provided to illustrate the proposed algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Virtual Reality to the Emerging Discipline of Perception
  Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven M. LaValle, Evan G. Center, Timo Ojala, Matti Pouke, Nicoletta Prencipe, Basak Sakcak, Markku Suomalainen, Kalle G. Timperi, Vadim K. Weinstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper makes the case that a powerful new discipline, which we term
perception engineering, is steadily emerging. It follows from a progression of
ideas that involve creating illusions, from historical paintings and film, to
video games and virtual reality in modern times. Rather than creating physical
artifacts such as bridges, airplanes, or computers, perception engineers create
illusory perceptual experiences. The scope is defined over any agent that
interacts with the physical world, including both biological organisms (humans,
animals) and engineered systems (robots, autonomous systems). The key idea is
that an agent, called a producer, alters the environment with the intent to
alter the perceptual experience of another agent, called a receiver. Most
importantly, the paper introduces a precise mathematical formulation of this
process, based on the von Neumann-Morgenstern notion of information, to help
scope and define the discipline. It is then applied to the cases of engineered
and biological agents with discussion of its implications on existing fields
such as virtual reality, robotics, and even social media. Finally, open
challenges and opportunities for involvement are identified.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bootstrapping Guarantees: Stability and Performance Analysis for Dynamic
  Encrypted Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Schlor, Frank Allgöwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Encrypted dynamic controllers that operate for an unlimited time have been a
challenging subject of research. The fundamental difficulty is the accumulation
of errors and scaling factors in the internal state during operation.
Bootstrapping, a technique commonly employed in fully homomorphic
cryptosystems, can be used to avoid overflows in the controller state but can
potentially introduce significant numerical errors. In this paper, we analyze
dynamic encrypted control with explicit consideration of bootstrapping. By
recognizing the bootstrapping errors occurring in the controller's state as an
uncertainty in the robust control framework, we can provide stability and
performance guarantees for the whole encrypted control system. Further, the
conservatism of the stability and performance test is reduced by using a lifted
version of the control system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formal Verification with Constrained Polynomial Logical Zonotope 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Hafez, Frank J. Jiang, Karl H. Johansson, Amr Alanwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose using constrained polynomial logical zonotopes for
formal verification of logical systems. We perform reachability analysis to
compute the set of states that could be reached. To do this, we utilize a
recently introduced set representation called polynomial logical zonotopes for
performing computationally efficient and exact reachability analysis on logical
systems. Notably, polynomial logical zonotopes address the "curse of
dimensionality" when analyzing the reachability of logical systems since the
set representation can represent 2^n binary vectors using n generators. After
finishing the reachability analysis, the formal verification involves verifying
whether the intersection of the calculated reachable set and the unsafe set is
empty or not. However, polynomial logical zonotopes are not closed under
intersections. To address this, we formulate constrained polynomial logical
zonotopes, which maintain the computational efficiency and exactness of
polynomial logical zonotopes for reachability analysis while supporting exact
intersections. Furthermore, we present an extensive empirical study
illustrating and verifying the benefits of using constrained polynomial logical
zonotopes for the formal verification of logical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dynamic Programming Approach for Road Traffic Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Laurini, Irene Saccani, Stefano Ardizzoni, Luca Consolini, Marco Locatelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a road network represented by a directed graph. We assume to
collect many measurements of traffic flows on all the network arcs, or on a
subset of them. We assume that the users are divided into different groups.
Each group follows a different path. The flows of all user groups are modeled
as a set of independent Poisson processes. Our focus is estimating the paths
followed by each user group, and the means of the associated Poisson processes.
We present a possible solution based on a Dynamic Programming algorithm. The
method relies on the knowledge of high order cumulants. We discuss the
theoretical properties of the introduced method. Finally, we present some
numerical tests on well-known benchmark networks, using synthetic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stability Properties of the Impulsive Goodwin's Oscillator in 1-cycle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton V. Proskurnikov, Alexander Medvedev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Impulsive Goodwin's Oscillator (IGO) is a mathematical model of a hybrid
closed-loop system. It arises by closing a special kind of continuous linear
positive time-invariant system with impulsive feedback, which employs both
amplitude and frequency pulse modulation. The structure of IGO precludes the
existence of equilibria, and all its solutions are oscillatory. With its origin
in mathematical biology, the IGO also presents a control paradigm useful in a
wide range of applications, in particular dosing of chemicals and medicines.
Since the pulse modulation feedback mechanism introduces significant
nonlinearity and non-smoothness in the closedloop dynamics, conventional
controller design methods fail to apply. However, the hybrid dynamics of IGO
reduce to a nonlinear, time-invariant discrete-time system, exhibiting a
one-to-one correspondence between periodic solutions of the original IGO and
those of the discrete-time system. The paper proposes a design approach that
leverages the linearization of the equivalent discrete-time dynamics in the
vicinity of a fixed point. A simple and efficient local stability condition of
the 1-cycle in terms of the characteristics of the amplitude and frequency
modulation functions is obtained.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE CDC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe and Robust Reinforcement-Learning: Principles and Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taku Yamagata, Raul Santos-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has shown remarkable success in solving
relatively complex tasks, yet the deployment of RL systems in real-world
scenarios poses significant challenges related to safety and robustness. This
paper aims to identify and further understand those challenges thorough the
exploration of the main dimensions of the safe and robust RL landscape,
encompassing algorithmic, ethical, and practical considerations. We conduct a
comprehensive review of methodologies and open problems that summarizes the
efforts in recent years to address the inherent risks associated with RL
applications.
  After discussing and proposing definitions for both safe and robust RL, the
paper categorizes existing research works into different algorithmic approaches
that enhance the safety and robustness of RL agents. We examine techniques such
as uncertainty estimation, optimisation methodologies, exploration-exploitation
trade-offs, and adversarial training. Environmental factors, including
sim-to-real transfer and domain adaptation, are also scrutinized to understand
how RL systems can adapt to diverse and dynamic surroundings. Moreover, human
involvement is an integral ingredient of the analysis, acknowledging the broad
set of roles that humans can take in this context.
  Importantly, to aid practitioners in navigating the complexities of safe and
robust RL implementation, this paper introduces a practical checklist derived
from the synthesized literature. The checklist encompasses critical aspects of
algorithm design, training environment considerations, and ethical guidelines.
It will serve as a resource for developers and policymakers alike to ensure the
responsible deployment of RL systems in many application domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feedback Linearizable Discretizations of Second Order Mechanical Systems
  using Retraction Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyas N. B., David Martin Diego, Ravi Banavar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mechanical systems, in nature, are often described by a set of
continuous-time, nonlinear, second-order differential equations (SODEs). This
has motivated designs of various control laws implemented on digital
controllers, consequently requiring numerical discretization schemes. Feedback
linearizability of such sampled systems depends on the discretization scheme or
map choice. In this article, we utilize retraction maps and their lifts to
construct feedback linearizable discretizations for SODEs, which can be applied
to various mechanical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyRRT-Connect: A Bidirectional Rapidly-Exploring Random Trees Motion
  Planning Algorithm for Hybrid Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Wang, Ricardo G. Sanfelice
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a bidirectional rapidly-exploring random trees (RRT)
algorithm to solve the motion planning problem for hybrid systems. The proposed
algorithm, called HyRRT-Connect, propagates in both forward and backward
directions in hybrid time until an overlap between the forward and backward
propagation results is detected. Then, HyRRT-Connect constructs a motion plan
through the reversal and concatenation of functions defined on hybrid time
domains, ensuring the motion plan thoroughly satisfies the given hybrid
dynamics. To address the potential discontinuity along the flow caused by
tolerating some distance between the forward and backward partial motion plans,
we reconstruct the backward partial motion plan by a forward-in-hybrid-time
simulation from the final state of the forward partial motion plan. By applying
the reversed input of the backward partial motion plan, the reconstruction
process effectively eliminates the discontinuity and ensures that as the
tolerance distance decreases to zero, the distance between the endpoint of the
reconstructed motion plan and the final state set approaches zero. The proposed
algorithm is applied to an actuated bouncing ball example and a walking robot
example so as to highlight its generality and computational improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 8th IFAC International Conference on Analysis and
  Design of Hybrid Systems (ADHS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Economic Model Predictive Control for linear systems with
  performance guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Degner, Raffaele Soloperto, Melanie N. Zeilinger, John Lygeros, Johannes Köhler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a model predictive control (MPC) formulation to directly optimize
economic criteria for linear constrained systems subject to disturbances and
uncertain model parameters. The proposed formulation combines a certainty
equivalent economic MPC with a simple least-squares parameter adaptation. For
the resulting adaptive economic MPC scheme, we derive strong asymptotic and
transient performance guarantees. We provide a numerical example involving
building temperature control and demonstrate performance benefits of online
parameter adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, submitted to IEEE CDC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multivariable control of modular multilevel converters with convergence
  and safety guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Daniel Reyes Dreke, Ygor Pereira Marca, Maurice Roes, Mircea Lazar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Well-designed current control is a key factor in ensuring the efficient and
safe operation of modular multilevel converters (MMCs). Even though this
control problem involves multiple control objectives, conventional current
control schemes are comprised of independently designed decoupled controllers,
e.g., proportional-integral (PI) or proportional-resonant (PR). Due to the
bilinearity of the MMC dynamics, tuning PI and PR controllers so that good
performance and constraint satisfaction are guaranteed is quite challenging.
This challenge becomes more relevant in an AC/AC MMC configuration due to the
complexity of tracking the single-phase sinusoidal components of the MMC
output. In this paper, we propose a method to design a multivariable
controller, i.e., a static feedback gain, to regulate the MMC currents. We use
a physics-informed transformation to model the MMC dynamics linearly and
synthesise the proposed controller. We use this linear model to formulate a
linear matrix inequality that computes a feedback gain that guarantees safe and
effective operation, including (i) limited tracking error, (ii) stability, and
(iii) meeting all constraints. To test the efficacy of our method, we examine
its performance in a direct AC/AC MMC simulated in Simulink/PLECS and in a
scaled-down AC/AC MMC prototype to investigate the ultra-fast charging of
electric vehicles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Open Journal of the Industrial Electronics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Dual Gradient Tracking for Distributed Resource
  Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Huo, Xiaomeng Chen, Lingying Huang, Karl Henrik Johansson, Ling Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates privacy issues in distributed resource allocation
over directed networks, where each agent holds a private cost function and
optimizes its decision subject to a global coupling constraint through local
interaction with other agents. Conventional methods for resource allocation
over directed networks require all agents to transmit their original data to
neighbors, which poses the risk of disclosing sensitive and private
information. To address this issue, we propose an algorithm called
differentially private dual gradient tracking (DP-DGT) for distributed resource
allocation, which obfuscates the exchanged messages using independent Laplacian
noise. Our algorithm ensures that the agents' decisions converge to a
neighborhood of the optimal solution almost surely. Furthermore, without the
assumption of bounded gradients, we prove that the cumulative differential
privacy loss under the proposed algorithm is finite even when the number of
iterations goes to infinity. To the best of our knowledge, we are the first to
simultaneously achieve these two goals in distributed resource allocation
problems over directed networks. Finally, numerical simulations on economic
dispatch problems within the IEEE 14-bus system illustrate the effectiveness of
our proposed algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Distributed Nonconvex Stochastic Optimization
  with Quantized Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialong Chen, Jimin Wang, Ji-Feng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a new distributed nonconvex stochastic optimization
algorithm that can achieve privacy protection, communication efficiency and
convergence simultaneously. Specifically, each node adds time-varying privacy
noises to its local state to avoid information leakage, and then quantizes its
noise-perturbed state before transmitting to improve communication efficiency.
By employing the subsampling method controlled through the sample-size
parameter, the proposed algorithm reduces the impact of privacy noises, and
enhances the differential privacy level. When the global cost function
satisfies the Polyak-Lojasiewicz condition, the mean and high-probability
convergence rate and the oracle complexity of the proposed algorithm are given.
Importantly, the proposed algorithm achieves both the mean convergence and a
finite cumulative differential privacy budget over infinite iterations as the
sample-size goes to infinity. A numerical example of the distributed training
on the "MNIST" dataset is given to show the effectiveness of the algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linear Hybrid Asymmetrical Load-Modulated Balanced Amplifier with
  Multi-Band Reconfigurability and Antenna-VSWR Resilience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachen Guo, Yuchen Cao, Kenle Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first-ever highly linear and load-insensitive
three-way load-modulation power amplifier (PA) based on reconfigurable hybrid
asymmetrical load modulated balanced amplifier (H-ALMBA). Through proper
amplitude and phase controls, the carrier, control amplifier (CA), and two
peaking balanced amplifiers (BA1 and BA2) can form a linear high-order load
modulation over wide bandwidth. Moreover, it is theoretically unveiled that the
load modulation behavior of H-ALMBA can be insensitive to load mismatch by
leveraging bias reconfiguration and the intrinsic load-insensitivity of
balanced topology. Specifically, the PA's linearity and efficiency profiles can
be maintained against arbitrary load mismatch through $Z_\mathrm{L}$-dependent
reconfiguration of CA supply voltage ($V_\mathrm{DD,CA}$) and turning-on
sequence of BA1 and BA2. Based on the proposed theory, an RF-input linear
H-ALMBA is developed with GaN transistors and wideband quadrature hybrids. Over
the design bandwidth from $1.7$-$2.9$ GHz, an efficiency of $56.8\%$$-$$72.9\%$
at peak power and $49.8\%$$-$$61.2\%$ at $10$-dB PBO are measured together with
linear AMAM and AMPM responses. In modulated evaluation with 4G LTE signal, an
EVM of $3.1\%$, ACPR of $-39$ dB, and average efficiency of up to $52\%$ are
measured. Moreover, the reconfigurable H-ALMBA experimentally maintains an
excellent average efficiency and linearity against arbitrary load mismatch at
$2:1$ VSWR, and this mismatch-resilient operation can be achieved at any
in-band frequencies. The overall measured performance favorably outperforms the
state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Execution-time-certified QP Algorithm for $\ell_1$ penalty-based
  Soft-constrained MPC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Wu, Richard D. Braatz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Providing an execution time certificate and handling possible infeasibility
in closed-loop are two pressing requirements of Model Predictive Control (MPC).
To simultaneously meet these two requirements, this paper uses $\ell_1$
penalty-based soft-constrained MPC formulation and innovatively transforms the
resulting non-smooth QP into a box-constrained QP, which is solved by our
previously proposed direct and execution-time certified algorithm with only
dimension-dependent (data-independent) and exact number of iterations [1]. This
approach not only overcomes the limitation of our previously proposed algorithm
[1], only applicable to input-constrained MPC, but also enjoys exact recovery
feature (exactly recover the same solution when the original problem is
feasible) of $\ell_1$ penalty-based soft-constrained MPC formulation without
suffering numerical difficulty of the resulting non-smoothness. Other various
real-time QP applications, not limited to MPC, will also benefit from our QP
algorithm with execution-time certificate and global feasibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fault-tolerant properties of scale-free linear protocols for
  synchronization of homogeneous multi-agent systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton A. Stoorvogel, Ali Saberi, Zhenwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Originally, protocols were designed for multi-agent systems (MAS) using
information about the network. However, in many cases there is no or only
limited information available about the network. Recently, there has been a
focus on scale-free synchronization of multi-agent systems (MAS). In this case,
the protocol is designed without any prior information about the network. As
long as the network contains a directed spanning tree, the scale-free protocol
guarantees that the network achieves synchronization.
  If there is no directed spanning tree for the network then synchronization
cannot be achieved. But what happens when these scale-free protocols are
applied to such a network where the directed spanning tree no longer exists?
The latter might arise if, for instance, a fault occurs in one of more crucial
links. This paper establishes that the network decomposes into a number of
basic bicomponents which achieves synchronization among all nodes in this basic
bicomponent. On the other hand, nodes which are not part of any basic
bicomponent converge to a weighted average of the synchronized trajectories of
the basic bicomponents. The weights are independent of the initial conditions
and are independent of the designed protocol.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article was submitted to IEEE Transactions on Automatic Control
  for review at March 27th, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incentive-Compatible Vertiport Reservation in Advanced Air Mobility: An
  Auction-Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pan-Yang Su, Chinmay Maheshwari, Victoria Tuck, Shankar Sastry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of advanced air mobility (AAM) is expected to become a
multibillion-dollar industry in the near future. Market-based mechanisms are
touted to be an integral part of AAM operations, which comprise heterogeneous
operators with private valuations. In this work, we study the problem of
designing a mechanism to coordinate the movement of electric vertical take-off
and landing (eVTOL) aircraft, operated by multiple operators each having
heterogeneous valuations associated with their fleet, between vertiports, while
enforcing the arrival, departure, and parking constraints at vertiports.
Particularly, we propose an incentive-compatible and individually rational
vertiport reservation mechanism that maximizes a social welfare metric, which
encapsulates the objective of maximizing the overall valuations of all
operators while minimizing the congestion at vertiports. Additionally, we
improve the computational tractability of designing the reservation mechanism
by proposing a mixed binary linear programming approach that is based on
constructing network flow graph corresponding to the underlying problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incentive Designs for Learning Agents to Stabilize Coupled Exogenous
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jair Certório, Nuno C. Martins, Richard J. La, Murat Arcak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a large population of learning agents noncooperatively selecting
strategies from a common set, influencing the dynamics of an exogenous system
(ES) we seek to stabilize at a desired equilibrium. Our approach is to design a
dynamic payoff mechanism capable of shaping the population's strategy profile,
thus affecting the ES's state, by offering incentives for specific strategies
within budget limits. Employing system-theoretic passivity concepts, we
establish conditions under which a payoff mechanism can be systematically
constructed to ensure the global asymptotic stabilization of the ES's
equilibrium. In comparison to previous approaches originally studied in the
context of the so-called epidemic population games, the method proposed here
allows for more realistic epidemic models and other types of ES, such as
predator-prey dynamics. Stabilization is established with the support of a
Lyapunov function, which provides useful bounds on the transients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study of Three Influencer Archetypes for the Control of Opinion Spread
  in Time-Varying Social Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael DeBuse, Sean Warnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we consider the impact of information spread in time-varying
social networks, where agents request to follow other agents with aligned
opinions while dropping ties to neighbors whose posts are too dissimilar to
their own views. Opinion control and rhetorical influence has a very long
history, employing various methods including education, persuasion, propaganda,
marketing, and manipulation through mis-, dis-, and mal-information. The
automation of opinion controllers, however, has only recently become easily
deployable at a wide scale, with the advent of large language models (LLMs) and
generative AI that can translate the quantified commands from opinion
controllers into actual content with the appropriate nuance. Automated agents
in social networks can be deployed for various purposes, such as breaking up
echo chambers, bridging valuable new connections between agents, or shaping the
opinions of a target population -- and all of these raise important ethical
concerns that deserve serious attention and thoughtful discussion and debate.
This paper attempts to contribute to this discussion by considering three
archetypal influencing styles observed by human drivers in these settings,
comparing and contrasting the impact of these different control methods on the
opinions of agents in the network. We will demonstrate the efficacy of current
generative AI for generating nuanced content consistent with the command signal
from automatic opinion controllers like these, and we will report on frameworks
for approaching the relevant ethical considerations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submission to IEEE 2024 Conference on Decision and Control. 8 pages,
  7 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13483v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13483v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagar Patel, Sangeetha Abdu Jyothi, Nina Narodytska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CrystalBox, a novel, model-agnostic, posthoc explainability
framework for Deep Reinforcement Learning (DRL) controllers in the large family
of input-driven environments which includes computer systems. We combine the
natural decomposability of reward functions in input-driven environments with
the explanatory power of decomposed returns. We propose an efficient algorithm
to generate future-based explanations across both discrete and continuous
control environments. Using applications such as adaptive bitrate streaming and
congestion control, we demonstrate CrystalBox's capability to generate
high-fidelity explanations. We further illustrate its higher utility across
three practical use cases: contrastive explanations, network observability, and
guided reward design, as opposed to prior explainability techniques that
identify salient features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unconstrained learning of networked nonlinear systems via free
  parametrization of stable interconnected operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13967v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13967v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Massai, Danilo Saccani, Luca Furieri, Giancarlo Ferrari-Trecate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper characterizes a new parametrization of nonlinear networked
incrementally $L_2$-bounded operators in discrete time. The distinctive novelty
is that our parametrization is \emph{free} -- that is, a sparse large-scale
operator with bounded incremental $L_2$ gain is obtained for any choice of the
real values of our parameters. This property allows one to freely search over
optimal parameters via unconstrained gradient descent, enabling direct
applications in large-scale optimal control and system identification. Further,
we can embed prior knowledge about the interconnection topology and stability
properties of the system directly into the large-scale distributed operator we
design. Our approach is extremely general in that it can seamlessly encapsulate
and interconnect state-of-the-art Neural Network (NN) parametrizations of
stable dynamical systems. To demonstrate the effectiveness of this approach, we
provide a simulation example showcasing the identification of a networked
nonlinear system. The results underscore the superiority of our free
parametrizations over standard NN-based identification methods where a prior
over the system topology and local stability properties are not enforced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full version of the paper to appear at ECC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonlinear Control Allocation: A Learning Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.06180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.06180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hafiz Zeeshan Iqbal Khan, Surrayya Mobeen, Jahanzeb Rajput, Jamshed Riaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern aircraft are designed with redundant control effectors to cater for
fault tolerance and maneuverability requirements. This leads to aircraft being
over-actuated and requires control allocation schemes to distribute the control
commands among control effectors. Traditionally, optimization-based control
allocation schemes are used; however, for nonlinear allocation problems, these
methods require large computational resources. In this work, an artificial
neural network (ANN) based nonlinear control allocation scheme is proposed. The
proposed scheme is composed of learning the inverse of the control
effectiveness map through ANN, and then implementing it as an allocator instead
of solving an online optimization problem. Stability conditions are presented
for closed-loop systems incorporating the allocator, and computational
challenges are explored with piece-wise linear effectiveness functions and
ANN-based allocators. To demonstrate the efficacy of the proposed scheme, it is
compared with a standard quadratic programming-based method for control
allocation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE Conference on Decision and Control (CDC), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07494v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07494v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Wang, P S Pravin, Zhe Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational efficiency and non-adversarial robustness are critical factors
in real-world engineering applications. Yet, conventional neural networks often
fall short in addressing both simultaneously, or even separately. Drawing
insights from natural physical systems and existing literature, it is known
that an input convex architecture enhances computational efficiency, while a
Lipschitz-constrained architecture bolsters non-adversarial robustness. By
leveraging the strengths of convexity and Lipschitz continuity, we develop a
novel network architecture, termed Input Convex Lipschitz Recurrent Neural
Networks. This model is explicitly designed for fast and robust
optimization-based tasks and outperforms existing recurrent units across a
spectrum of engineering tasks in terms of computational efficiency and
non-adversarial robustness, including real-world solar irradiance prediction
for Solar PV system planning at LHT Holdings in Singapore and real-time Model
Predictive Control optimization for a nonlinear chemical reactor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Approximation with Delayed Updates: Finite-Time Rates under
  Markovian Sampling <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Adibi, Nicolo Dal Fabbro, Luca Schenato, Sanjeev Kulkarni, H. Vincent Poor, George J. Pappas, Hamed Hassani, Aritra Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by applications in large-scale and multi-agent reinforcement
learning, we study the non-asymptotic performance of stochastic approximation
(SA) schemes with delayed updates under Markovian sampling. While the effect of
delays has been extensively studied for optimization, the manner in which they
interact with the underlying Markov process to shape the finite-time
performance of SA remains poorly understood. In this context, our first main
contribution is to show that under time-varying bounded delays, the delayed SA
update rule guarantees exponentially fast convergence of the \emph{last
iterate} to a ball around the SA operator's fixed point. Notably, our bound is
\emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the
mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel
inductive proof technique that, unlike various existing delayed-optimization
analyses, relies on establishing uniform boundedness of the iterates. As such,
our proof may be of independent interest. Next, to mitigate the impact of the
maximum delay on the convergence rate, we provide the first finite-time
analysis of a delay-adaptive SA scheme under Markovian sampling. In particular,
we show that the exponent of convergence of this scheme gets scaled down by
$\tau_{avg}$, as opposed to $\tau_{max}$ for the vanilla delayed SA rule; here,
$\tau_{avg}$ denotes the average delay across all iterations. Moreover, the
adaptive scheme requires no prior knowledge of the delay sequence for step-size
tuning. Our theoretical findings shed light on the finite-time effects of
delays for a broad class of algorithms, including TD learning, Q-learning, and
stochastic gradient descent under Markovian sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Redesigning Large-Scale Multimodal Transit Networks with Shared
  Autonomous Mobility Services 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.16075v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.16075v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max T. M. Ng, Hani S. Mahmassani, Ömer Verbas, Taner Cokyasar, Roman Engelhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses a large-scale multimodal transit network design problem,
with Shared Autonomous Mobility Services (SAMS) as both transit feeders and an
origin-to-destination mode. The framework captures spatial demand and modal
characteristics, considers intermodal transfers and express services,
determines transit infrastructure investment and path flows, and generates
transit routes. A system-optimal multimodal transit network is designed with
minimum total door-to-door generalized costs of users and operators, satisfying
transit origin-destination demand within a pre-set infrastructure budget.
Firstly, the geography, demand, and modes in each zone are characterized with
continuous approximation. The decisions of network link investment and
multimodal path flows in zonal connection optimization are formulated as a
minimum-cost multi-commodity network flow (MCNF) problem and solved efficiently
with a mixed-integer linear programming (MILP) solver. Subsequently, the route
generation problem is solved by expanding the MCNF formulation to minimize
intramodal transfers. The model is illustrated through a set of experiments
with the Chicago network comprised of 50 zones and seven modes, under three
scenarios. The computational results present savings in traveler journey time
and operator cost demonstrating the potential benefits of collaboration between
multimodal transit systems and SAMS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 18 figures, accepted for publication in Transportation
  Research Part C: Emerging Technologies, and presentation in the 25th
  International Symposium on Transportation and Traffic Theory (ISTTT25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aggregate Model of District Heating Network for Integrated Energy
  Dispatch: A Physically Informed Data-Driven Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10483v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10483v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Lu, Zihang Gao, Yong Sun, Suhan Zhang, Baoju Li, Chengliang Hao, Yijun Xu, Wei Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The district heating network (DHN) is essential in enhancing the operational
flexibility of integrated energy systems (IES). Yet, it is hard to obtain an
accurate and concise DHN model for the operation owing to complicated network
features and imperfect measurements. Considering this, this paper proposes a
physical-ly informed data-driven aggregate model (AGM) for the DHN, providing a
concise description of the source-load relationship of DHN without exposing
network details. First, we derive the analytical relationship between the state
variables of the source and load nodes of the DHN, offering a physical
fundament for the AGM. Second, we propose a physics-informed estimator for the
AGM that is robust to low-quality measurements, in which the physical
constraints associated with the parameter normalization and sparsity are
embedded to improve the accuracy and robustness. Finally, we propose a
physics-enhanced algorithm to solve the nonlinear estimator with non-closed
constraints efficiently. Simulation results verify the effectiveness of the
proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural-artificial hybrid swarm: Cyborg-insect group navigation in
  unknown obstructed soft terrain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Bai, Phuoc Thanh Tran Ngoc, Huu Duoc Nguyen, Duc Long Le, Quang Huy Ha, Kazuki Kai, Yu Xiang See To, Yaosheng Deng, Jie Song, Naoki Wakamiya, Hirotaka Sato, Masaki Ogura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating multi-robot systems in complex terrains has always been a
challenging task. This is due to the inherent limitations of traditional robots
in collision avoidance, adaptation to unknown environments, and sustained
energy efficiency. In order to overcome these limitations, this research
proposes a solution by integrating living insects with miniature electronic
controllers to enable robotic-like programmable control, and proposing a novel
control algorithm for swarming. Although these creatures, called cyborg
insects, have the ability to instinctively avoid collisions with neighbors and
obstacles while adapting to complex terrains, there is a lack of literature on
the control of multi-cyborg systems. This research gap is due to the difficulty
in coordinating the movements of a cyborg system under the presence of insects'
inherent individual variability in their reactions to control input. In
response to this issue, we propose a novel swarm navigation algorithm
addressing these challenges. The effectiveness of the algorithm is demonstrated
through an experimental validation in which a cyborg swarm was successfully
navigated through an unknown sandy field with obstacles and hills. This
research contributes to the domain of swarm robotics and showcases the
potential of integrating biological organisms with robotics and control theory
to create more intelligent autonomous systems with real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Polygonal Cone Control Barrier Functions (PolyC2BF) for safe navigation
  in cluttered environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manan Tayal, Shishir Kolathaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In fields such as mining, search and rescue, and archaeological exploration,
ensuring real-time, collision-free navigation of robots in confined, cluttered
environments is imperative. Despite the value of established path planning
algorithms, they often face challenges in convergence rates and handling
dynamic infeasibilities. Alternative techniques like collision cones struggle
to accurately represent complex obstacle geometries. This paper introduces a
novel category of control barrier functions, known as Polygonal Cone Control
Barrier Function (PolyC2BF), which addresses overestimation and computational
complexity issues. The proposed PolyC2BF, formulated as a Quadratic Programming
(QP) problem, proves effective in facilitating collision-free movement of
multiple robots in complex environments. The efficacy of this approach is
further demonstrated through PyBullet simulations on quadruped (unicycle
model), and crazyflie 2.1 (quadrotor model) in cluttered environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 Pages, 6 Figures. Accepted at European Control Conference (ECC)
  2024. arXiv admin note: text overlap with arXiv:2303.15871</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Commutation Design: Applied to Switched Reluctance Motors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max van Meer, Gert Witvoet, Tom Oomen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Switched Reluctance Motors (SRMs) are cost-effective electric actuators that
utilize magnetic reluctance to generate torque, with torque ripple arising from
unaccounted manufacturing defects in the rotor tooth geometry. This paper aims
to design a versatile, resource-efficient commutation function for accurate
control of a range of SRMs, mitigating torque ripple despite manufacturing
variations across SRMs and individual rotor teeth. The developed commutation
function optimally distributes current between coils by leveraging the variance
in the torque-current-angle model and is designed with few parameters for easy
integration on affordable hardware. Monte Carlo simulations and experimental
results show a tracking error reduction of up to 31% and 11%, respectively. The
developed approach is beneficial for applications using a single driver for
multiple systems and those constrained by memory or modeling effort, providing
an economical solution for improved tracking performance and reduced acoustic
noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures. Final version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Performance, Calibration Time and Efficiency in Brain-Machine
  Interfaces through Transfer Learning and Wearable EEG Technology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07798v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07798v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaying Wang, Lan Mei, Victor Kartsch, Andrea Cossettini, Luca Benini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain-machine interfaces (BMIs) have emerged as a transformative force in
assistive technologies, empowering individuals with motor impairments by
enabling device control and facilitating functional recovery. However, the
persistent challenge of inter-session variability poses a significant hurdle,
requiring time-consuming calibration at every new use. Compounding this issue,
the low comfort level of current devices further restricts their usage. To
address these challenges, we propose a comprehensive solution that combines a
tiny CNN-based Transfer Learning (TL) approach with a comfortable, wearable EEG
headband. The novel wearable EEG device features soft dry electrodes placed on
the headband and is capable of on-board processing. We acquire multiple
sessions of motor-movement EEG data and achieve up to 96% inter-session
accuracy using TL, greatly reducing the calibration time and improving
usability. By executing the inference on the edge every 100ms, the system is
estimated to achieve 30h of battery life. The comfortable BMI setup with tiny
CNN and TL paves the way to future on-device continual learning, essential for
tackling inter-session variability and improving usability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nigel -- Mechatronic Design and Robust Sim2Real Control of an
  Over-Actuated Autonomous Vehicle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11542v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11542v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Vilas Samak, Tanmay Vilas Samak, Javad Mohammadpour Velni, Venkat Narayan Krovi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation to reality (sim2real) transfer from a dynamics and controls
perspective usually involves re-tuning or adapting the designed algorithms to
suit real-world operating conditions, which often violates the performance
guarantees established originally. This work presents a generalizable framework
for achieving reliable sim2real transfer of autonomy-oriented control systems
using multi-model multi-objective robust optimal control synthesis, which lends
well to uncertainty handling and disturbance rejection with theoretical
guarantees. Particularly, this work is centered around a novel
actuation-redundant scaled autonomous vehicle called Nigel, with independent
all-wheel drive and independent all-wheel steering architecture, whose enhanced
configuration space bodes well for robust control applications. To this end, we
present the mechatronic design, dynamics modeling, parameter identification,
and robust stabilizing as well as tracking control of Nigel using the proposed
framework, with exhaustive experimentation and benchmarking in simulation as
well as real-world settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensuring Disturbance Rejection Performance by Synthesizing
  Grid-Following and Grid-Forming Inverters in Power Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16488v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16488v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuyilong Ma, Huanhai Xin, Zhiyi Li, Linbin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To satisfy dynamic requirements of power systems, it is imperative for
grid-tied inverters to ensure good disturbance rejection performance (DRP)
under variable grid conditions. This letter discovers and theoretically proves
that for general networks, synthesizing grid-following (GFL) inverters and
grid-forming (GFM) inverters can always more effectively ensure the DRP of
multiple inverters, as compared to homogeneous inverter-based systems that
solely utilize either GFL or GFM inverters. The synthesis of GFL inverters and
GFM inverters can concurrently increase the smallest eigenvalue and decrease
the largest eigenvalue of the network grounded Laplacian matrix. This can be
equivalent to rematching the proper short-circuit ratio (SCR) for GFL and GFM
inverters, thereby ensuring the DRP of inverters both in weak and strong grids.
The results reveal the necessity of synthesizing diverse inverter control
schemes from the network-based perspective. Sensitivity function-based tests
and real-time simulations validate our results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Logic in Computer Science
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Logic Formalisation of ISO 34502 Critical Scenarios: Modular
  Construction with the RSS Safety Distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesse Reimann, Nico Mansion, James Haydon, Benjamin Bray, Agnishom Chattopadhyay, Sota Sato, Masaki Waga, Étienne André, Ichiro Hasuo, Naoki Ueda, Yosuke Yokoyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the development of autonomous vehicles progresses, efficient safety
assurance methods become increasingly necessary. Safety assurance methods such
as monitoring and scenario-based testing call for formalisation of driving
scenarios. In this paper, we develop a temporal-logic formalisation of an
important class of critical scenarios in the ISO standard 34502. We use signal
temporal logic (STL) as a logical formalism. Our formalisation has two main
features: 1) modular composition of logical formulas for systematic and
comprehensive formalisation (following the compositional methodology of ISO
34502); 2) use of the RSS distance for defining danger. We find our
formalisation comes with few parameters to tune thanks to the RSS distance. We
experimentally evaluated our formalisation; using its results, we discuss the
validity of our formalisation and its stability with respect to the choice of
some parameter values.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures, 5 tables. Accepted to SAC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Four Formal Models of IEEE 1394 Link Layer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hubert Garavel, Bas Luttik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit the IEEE 1394 high-performance serial bus ("FireWire"), which
became a success story in formal methods after three PhD students, by using
process algebra and model checking, detected a deadlock error in this IEEE
standard. We present four formal models for the asynchronous mode of the Link
Layer of IEEE 1394: the original model in muCRL, a simplified model in mCRL2, a
revised model in LOTOS, and a novel model in LNT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings MARS 2024, arXiv:2403.17862</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formally Modelling the Rijkswaterstaat Tunnel Control Systems in a
  Constrained Industrial Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin H. J. Jilissen, Peter Dieleman, Jan Friso Groote
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rijkswaterstaat, the National Dutch body responsible for infrastructure,
recognised the importance of formal modelling and set up a program to model the
control of road tunnels. This is done to improve the standardisation of tunnel
control and make communication with suppliers smoother. A subset of SysML is
used to formulate the models, which are substantial. In an earlier paper we
have shown that these models can be used to prove behavioural properties by
manually translating the models to mCRL2. In this paper we report on an
automatic translation to mCRL2. As the results of the translation became
unwieldy, we also investigated modelling tunnel control in the specification
language Dezyne which has built-in verification capabilities and compared the
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings MARS 2024, arXiv:2403.17862</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synergistic Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Cachin, David Lehnherr, Thomas Studer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In formal epistemology, group knowledge is often modelled as the knowledge
that the group would have, if the agents shared all their individual knowledge.
However, this interpretation does not account for relations between agents. In
this work, we propose the notion of synergistic knowledge which makes it
possible to model those relationships.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safety Verification of Wait-Only Non-Blocking Broadcast Protocols 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucie Guillou, Arnaud Sangnier, Nathalie Sznajder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study networks of processes that all execute the same finite protocol and
communicate synchronously in two different ways: a process can broadcast one
message to all other processes or send it to at most one other process. In both
cases, if no process can receive the message, it will still be sent. We
establish a precise complexity class for two coverability problems with a
parameterised number of processes: the state coverability problem and the
configuration coverability problem. It is already known that these problems are
Ackermann-hard (but decidable) in the general case. We show that when the
protocol is Wait-Only, i.e., it has no state from which a process can send and
receive messages, the complexity drops to P and PSPACE, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Long version of a paper accepted to PetriNets 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formal Verification with Constrained Polynomial Logical Zonotope 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Hafez, Frank J. Jiang, Karl H. Johansson, Amr Alanwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose using constrained polynomial logical zonotopes for
formal verification of logical systems. We perform reachability analysis to
compute the set of states that could be reached. To do this, we utilize a
recently introduced set representation called polynomial logical zonotopes for
performing computationally efficient and exact reachability analysis on logical
systems. Notably, polynomial logical zonotopes address the "curse of
dimensionality" when analyzing the reachability of logical systems since the
set representation can represent 2^n binary vectors using n generators. After
finishing the reachability analysis, the formal verification involves verifying
whether the intersection of the calculated reachable set and the unsafe set is
empty or not. However, polynomial logical zonotopes are not closed under
intersections. To address this, we formulate constrained polynomial logical
zonotopes, which maintain the computational efficiency and exactness of
polynomial logical zonotopes for reachability analysis while supporting exact
intersections. Furthermore, we present an extensive empirical study
illustrating and verifying the benefits of using constrained polynomial logical
zonotopes for the formal verification of logical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Path Towards Legal Autonomy: An interoperable and explainable approach
  to extracting, transforming, loading and computing legal information using
  large language models, expert systems and Bayesian networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Constant, Hannes Westermann, Bryan Wilson, Alex Kiefer, Ines Hipolito, Sylvain Pronovost, Steven Swanson, Mahault Albarracin, Maxwell J. D. Ramstead
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal autonomy - the lawful activity of artificial intelligence agents - can
be achieved in one of two ways. It can be achieved either by imposing
constraints on AI actors such as developers, deployers and users, and on AI
resources such as data, or by imposing constraints on the range and scope of
the impact that AI agents can have on the environment. The latter approach
involves encoding extant rules concerning AI driven devices into the software
of AI agents controlling those devices (e.g., encoding rules about limitations
on zones of operations into the agent software of an autonomous drone device).
This is a challenge since the effectivity of such an approach requires a method
of extracting, loading, transforming and computing legal information that would
be both explainable and legally interoperable, and that would enable AI agents
to reason about the law. In this paper, we sketch a proof of principle for such
a method using large language models (LLMs), expert legal systems known as
legal decision paths, and Bayesian networks. We then show how the proposed
method could be applied to extant regulation in matters of autonomous cars,
such as the California Vehicle Code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Propositional Dynamic Logic and Concurrency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Acclavio, Fabrizio Montesi, Marco Peressotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic logic in the setting of concurrency has proved problematic because of
the challenge of capturing interleaving. This challenge stems from the fact
that the operational semantics for programs considered in these logics is
tailored on trace reasoning for sequential programs. In this work, we
generalise propositional dynamic logic (PDL) to a logic framework we call
operational propositional dynamic logic (OPDL) in which we are able to reason
on sets of programs provided with arbitrary operational semantics. We prove
cut-elimination and adequacy of a sequent calculus for PDL and we extend these
results to OPDL. We conclude by discussing OPDL for Milner's CCS and
Choreographic Programming.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Overview</span> of the Lebesgue Differentiation Theorem in Coq 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reynald Affeldt, Zachary Stone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Formalization of real analysis offers a chance to rebuild traditional proofs
of important theorems as unambiguous theories that can be interactively
explored. This paper provides a comprehensive overview of the Lebesgue
Differentiation Theorem formalized in the Coq proof assistant, from which the
first Fundamental Theorem of Calculus (FTC) for the Lebesgue integral is
obtained as a corollary. Proving the first FTC in this way has the advantage of
decomposing into loosely-coupled theories of moderate size and of independent
interest that lend themselves well to incremental and collaborative
development. We explain how we formalize all the topological constructs and all
the standard lemmas needed to eventually relate the definitions of derivability
and of Lebesgue integration of MathComp-Analysis, a formalization of analysis
developed on top of the Mathematical Components library. In the course of this
experiment, we substantially enrich MathComp-Analysis and even devise a new
proof for Urysohn's lemma.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference-Based Planning in Stochastic Environments: From
  Partially-Ordered Temporal Goals to Most Preferred Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazhar Rahmani, Abhishek N. Kulkarni, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human preferences are not always represented via complete linear orders: It
is natural to employ partially-ordered preferences for expressing incomparable
outcomes. In this work, we consider decision-making and probabilistic planning
in stochastic systems modeled as Markov decision processes (MDPs), given a
partially ordered preference over a set of temporally extended goals.
Specifically, each temporally extended goal is expressed using a formula in
Linear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially
ordered preference, we introduce order theory to map a preference over temporal
goals to a preference over policies for the MDP. Accordingly, a most preferred
policy under a stochastic ordering induces a stochastic nondominated
probability distribution over the finite paths in the MDP. To synthesize a most
preferred policy, our technical approach includes two key steps. In the first
step, we develop a procedure to transform a partially ordered preference over
temporal goals into a computational model, called preference automaton, which
is a semi-automaton with a partial order over acceptance conditions. In the
second step, we prove that finding a most preferred policy is equivalent to
computing a Pareto-optimal policy in a multi-objective MDP that is constructed
from the original MDP, the preference automaton, and the chosen stochastic
ordering relation. Throughout the paper, we employ running examples to
illustrate the proposed preference specification and solution approaches. We
demonstrate the efficacy of our algorithm using these examples, providing
detailed analysis, and then discuss several potential future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2209.12267</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flip-Breakability: A Combinatorial Dichotomy for Monadically Dependent
  Graph Classes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15201v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15201v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Dreier, Nikolas Mählmann, Szymon Toruńczyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A conjecture in algorithmic model theory predicts that the model-checking
problem for first-order logic is fixed-parameter tractable on a hereditary
graph class if and only if the class is monadically dependent. Originating in
model theory, this notion is defined in terms of logic, and encompasses nowhere
dense classes, monadically stable classes, and classes of bounded twin-width.
Working towards this conjecture, we provide the first two combinatorial
characterizations of monadically dependent graph classes. This yields the
following dichotomy.
  On the structure side, we characterize monadic dependence by a
Ramsey-theoretic property called flip-breakability. This notion generalizes the
notions of uniform quasi-wideness, flip-flatness, and bounded grid rank, which
characterize nowhere denseness, monadic stability, and bounded twin-width,
respectively, and played a key role in their respective model checking
algorithms. Natural restrictions of flip-breakability additionally characterize
bounded treewidth and cliquewidth and bounded treedepth and shrubdepth.
  On the non-structure side, we characterize monadic dependence by explicitly
listing few families of forbidden induced subgraphs. This result is analogous
to the characterization of nowhere denseness via forbidden subdivided cliques,
and allows us to resolve one half of the motivating conjecture: First-order
model checking is AW[$*$]-hard on every hereditary graph class that is
monadically independent. The result moreover implies that hereditary graph
classes which are small, have almost bounded twin-width, or have almost bounded
flip-width, are monadically dependent.
  Lastly, we lift our result to also obtain a combinatorial dichotomy in the
more general setting of monadically dependent classes of binary structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: added section "Conclusions and Future Work"</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-26T00:00:00Z">2024-03-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal
  Propagation Analysis for Large Language Models <span class="chip">ICLR
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Kyunggeun Lee, Jun Ma, Harris Teague
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large generative models, such as large language models (LLMs) and diffusion
models have as revolutionized the fields of NLP and computer vision
respectively. However, their slow inference, high computation and memory
requirement makes it challenging to deploy them on edge devices. In this study,
we propose a light-weight quantization aware fine tuning technique using
knowledge distillation (KD-QAT) to improve the performance of 4-bit weight
quantized LLMs using commonly available datasets to realize a popular language
use case, on device chat applications. To improve this paradigm of finetuning,
as main contributions, we provide insights into stability of KD-QAT by
empirically studying the gradient propagation during training to better
understand the vulnerabilities of KD-QAT based approaches to low-bit
quantization errors. Based on our insights, we propose ov-freeze, a simple
technique to stabilize the KD-QAT process. Finally, we experiment with the
popular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that
ov-freeze results in near float-point precision performance, i.e., less than
0.7% loss of accuracy on Commonsense Reasoning benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Practical ML for Low Resource Settings Workshop at ICLR
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Produce Responses Perceived to be Empathic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoon Kyung Lee, Jina Suh, Hongli Zhan, Junyi Jessy Li, Desmond C. Ong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated surprising performance on many
tasks, including writing supportive messages that display empathy. Here, we had
these models generate empathic messages in response to posts describing common
life experiences, such as workplace situations, parenting, relationships, and
other anxiety- and anger-eliciting situations. Across two studies (N=192, 202),
we showed human raters a variety of responses written by several models (GPT4
Turbo, Llama2, and Mistral), and had people rate these responses on how
empathic they seemed to be. We found that LLM-generated responses were
consistently rated as more empathic than human-written responses. Linguistic
analyses also show that these models write in distinct, predictable ``styles",
in terms of their use of punctuation, emojis, and certain words. These results
highlight the potential of using LLMs to enhance human peer support in contexts
where empathy is important.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution <span class="chip">ICAPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Feng, Adittyo Paul, Zhe Chen, Jiaoyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One area of research in multi-agent path finding is to determine how
replanning can be efficiently achieved in the case of agents being delayed
during execution. One option is to reschedule the passing order of agents,
i.e., the sequence in which agents visit the same location. In response, we
propose Switchable-Edge Search (SES), an A*-style algorithm designed to find
optimal passing orders. We prove the optimality of SES and evaluate its
efficiency via simulations. The best variant of SES takes less than 1 second
for small- and medium-sized problems and runs up to 4 times faster than
baselines for large-sized problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICAPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Juru: Legal Brazilian Large Language Model from Reputable Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roseval Malaquias Junior, Ramon Pires, Roseli Romero, Rodrigo Nogueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high computational cost associated with pretraining large language models
limits their research. Two strategies have emerged to address this issue:
domain specialization and pretraining with high-quality data. To explore these
strategies, we specialized the Sabi\'a-2 Small model with 1.9 billion unique
tokens from reputable Brazilian legal sources and conducted few-shot
evaluations on legal and general knowledge exams. Our model, Juru, demonstrates
the benefits of domain specialization with a reduced amount of pretraining
data. However, this specialization comes at the expense of degrading
performance in other knowledge areas within the same language. This study
contributes to the growing body of scientific evidence showing that pretraining
data selection may enhance the performance of large language models, enabling
the exploration of these models at a lower cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Securing GNNs: Explanation-Based Identification of Backdoored Training
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jane Downer, Ren Wang, Binghui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet
they are vulnerable to backdoor attacks that can compromise their performance
and ethical application. The detection of these attacks is crucial for
maintaining the reliability and security of GNN classification tasks, but
effective detection techniques are lacking. Following an initial investigation,
we observed that while graph-level explanations can offer limited insights,
their effectiveness in detecting backdoor triggers is inconsistent and
incomplete. To bridge this gap, we extract and transform secondary outputs of
GNN explanation mechanisms, designing seven novel metrics that more effectively
detect backdoor attacks. Additionally, we develop an adaptive attack to
rigorously evaluate our approach. We test our method on multiple benchmark
datasets and examine its efficacy against various attack models. Our results
show that our method can achieve high detection performance, marking a
significant advancement in safeguarding GNNs against backdoor attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AE SemRL: Learning Semantic Association Rules with Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erkan Karabulut, Victoria Degeler, Paul Groth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Association Rule Mining (ARM) is the task of learning associations among data
features in the form of logical rules. Mining association rules from
high-dimensional numerical data, for example, time series data from a large
number of sensors in a smart environment, is a computationally intensive task.
In this study, we propose an Autoencoder-based approach to learn and extract
association rules from time series data (AE SemRL). Moreover, we argue that in
the presence of semantic information related to time series data sources,
semantics can facilitate learning generalizable and explainable association
rules. Despite enriching time series data with additional semantic features, AE
SemRL makes learning association rules from high-dimensional data feasible. Our
experiments show that semantic association rules can be extracted from a latent
representation created by an Autoencoder and this method has in the order of
hundreds of times faster execution time than state-of-the-art ARM approaches in
many scenarios. We believe that this study advances a new way of extracting
associations from representations and has the potential to inspire more
research in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recommendation of data-free class-incremental learning algorithms by
  simulating future data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eva Feillet, Adrian Popescu, Céline Hudelot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning deals with sequential data streams composed of
batches of classes. Various algorithms have been proposed to address the
challenging case where samples from past classes cannot be stored. However,
selecting an appropriate algorithm for a user-defined setting is an open
problem, as the relative performance of these algorithms depends on the
incremental settings. To solve this problem, we introduce an algorithm
recommendation method that simulates the future data stream. Given an initial
set of classes, it leverages generative models to simulate future classes from
the same visual domain. We evaluate recent algorithms on the simulated stream
and recommend the one which performs best in the user-defined incremental
setting. We illustrate the effectiveness of our method on three large datasets
using six algorithms and six incremental settings. Our method outperforms
competitive baselines, and performance is close to that of an oracle choosing
the best algorithm in each setting. This work contributes to facilitate the
practical deployment of incremental learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with
  Autoformalization <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, Yuhuai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM), such as Google's Minerva and OpenAI's GPT
families, are becoming increasingly capable of solving mathematical
quantitative reasoning problems. However, they still make unjustified logical
and computational errors in their reasoning steps and answers. In this paper,
we leverage the fact that if the training corpus of LLMs contained sufficiently
many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving
environment), they can be prompted to translate i.e. autoformalize informal
mathematical statements into formal Isabelle code -- which can be verified
automatically for internal consistency. This provides a mechanism to
automatically reject solutions whose formalized versions are inconsistent
within themselves or with the formalized problem statement. We evaluate our
method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach
provides a consistently better heuristic than vanilla majority voting -- the
previously best method to identify correct answers, by more than 12% on GSM8K.
In our experiments it improves results consistently across all datasets and LLM
model sizes. The code can be found at https://github.com/jinpz/dtv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sabiá-2: A New Generation of Portuguese Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thales Sales Almeida, Hugo Abonizio, Rodrigo Nogueira, Ramon Pires
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Sabi\'a-2, a family of large language models trained on
Portuguese texts. The models are evaluated on a diverse range of exams,
including entry-level tests for Brazilian universities, professional
certification exams, and graduate-level exams for various disciplines such as
accounting, economics, engineering, law and medicine. Our results reveal that
our best model so far, Sabi\'a-2 Medium, matches or surpasses GPT-4's
performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64
exams. Notably, specialization has a significant impact on a model's
performance without the need to increase its size, allowing us to offer
Sabi\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.
Finally, we identified that math and coding are key abilities that need
improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HIVE: Harnessing Human Feedback for Instructional Visual Editing <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating human feedback has been shown to be crucial to align text
generated by large language models to human preferences. We hypothesize that
state-of-the-art instructional image editing models, where outputs are
generated based on an input image and an editing instruction, could similarly
benefit from human feedback, as their outputs may not adhere to the correct
instructions and preferences of users. In this paper, we present a novel
framework to harness human feedback for instructional visual editing (HIVE).
Specifically, we collect human feedback on the edited images and learn a reward
function to capture the underlying user preferences. We then introduce scalable
diffusion model fine-tuning methods that can incorporate human preferences
based on the estimated reward. Besides, to mitigate the bias brought by the
limitation of data, we contribute a new 1M training dataset, a 3.6K reward
dataset for rewards learning, and a 1K evaluation dataset to boost the
performance of instructional image editing. We conduct extensive empirical
experiments quantitatively and qualitatively, showing that HIVE is favored over
previous state-of-the-art instructional image editing approaches by a large
margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In CVPR, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Batched Low-Rank Adaptation of Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeming Wen, Swarat Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning
foundation models by incorporating trainable low-rank matrices, thereby
reducing the number of trainable parameters. While LoRA offers numerous
advantages, its applicability for real-time serving to a diverse and global
user base is constrained by its incapability to handle multiple task-specific
adapters efficiently. This imposes a performance bottleneck in scenarios
requiring personalized, task-specific adaptations for each incoming request. To
mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which
each input example in a minibatch can be associated with its unique low-rank
adaptation weights, allowing for efficient batching of heterogeneous requests.
We empirically demonstrate that FLoRA retains the performance merits of LoRA,
showcasing competitive results on the MultiPL-E code generation benchmark
spanning over 8 languages and a multilingual speech recognition task across 6
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Signal Processing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive TTD Configurations for Near-Field Communications: An
  Unsupervised <span class="highlight-title">Transformer</span> Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsienchih Ting, Zhaolin Wang, Yuanwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  True-time delayers (TTDs) are popular analog devices for facilitating
near-field wideband beamforming subject to the spatial-wideband effect. In this
paper, an adaptive TTD configuration is proposed for short-range TTDs. Compared
to the existing TTD configurations, the proposed one can effectively combat the
spatial-widebandd effect for arbitrary user locations and array shapes with the
aid of a switch network. A novel end-to-end deep neural network is proposed to
optimize the hybrid beamforming with adaptive TTDs for maximizing spectral
efficiency. 1) First, based on the U-Net architecture, a near-field channel
learning module (NFC-LM) is proposed for adaptive beamformer design through
extracting the latent channel response features of various users across
different frequencies. In the NFC-LM, an improved cross attention (CA) is
introduced to further optimize beamformer design by enhancing the latent
feature connection between near-field channel and different beamformers. 2)
Second, a switch multi-user transformer (S-MT) is proposed to adaptively
control the connection between TTDs and phase shifters (PSs). In the S-MT, an
improved multi-head attention, namely multi-user attention (MSA), is introduced
to optimize the switch network through exploring the latent channel relations
among various users. 3) Third, a multi feature cross attention (MCA) is
introduced to simultaneously optimize the NFC-LM and S-MT by enhancing the
latent feature correlation between beamformers and switch network. Numerical
simulation results show that 1) the proposed adaptive TTD configuration
effectively eliminates the spatial-wideband effect under uniform linear array
(ULA) and uniform circular array (UCA) architectures, and 2) the proposed deep
neural network can provide near optimal spectral efficiency, and solve the
multi-user bemformer design and dynamical connection problem in real-time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Statistical Analysis of the Multipath Propagation Model
  Parameters for Power Line Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Pittolo, Irene Povedano, José A. Cortés, Francisco J. Cañete, Andrea M. Tonello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a fitting procedure that aims to identify the statistical
properties of the parameters that describe the most widely known multipath
propagation model (MPM) used in power line communication (PLC). Firstly, the
MPM parameters are computed by fitting the theoretical model to a large
database of single-input-single-output (SISO) experimental measurements,
carried out in typical home premises. Secondly, the determined parameters are
substituted back into the MPM formulation with the aim to prove their
faithfulness, thus validating the proposed computation procedure. Then, the MPM
parameters properties have been evaluated. In particular, the statistical
behavior is established identifying the best fitting distribution by comparing
the most common distributions through the use of the likelihood function.
Moreover, the relationship among the different paths is highlighted in terms of
statistical correlation. The identified statistical behavior for the MPM
parameters confirms the assumptions of the previous works that, however, were
mostly established in an heuristic way.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Channel Estimation and Beamforming for Beyond Diagonal Reconfigurable
  Intelligent Surfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Li, Shanpu Shen, Yumeng Zhang, Bruno Clerckx
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beyond diagonal reconfigurable intelligent surface (BD-RIS) is a new advance
and generalization of the RIS technique. BD-RIS breaks through the isolation
between RIS elements by creatively introducing inter-element connections,
thereby enabling smarter wave manipulation and enlarging coverage. However,
exploring proper channel estimation schemes suitable for BD-RIS aided
communication systems still remains an open problem. In this paper, we study
channel estimation and beamforming design for BD-RIS aided multi-antenna
systems. We first describe the channel estimation strategy based on the least
square (LS) method, derive the mean square error (MSE) of the LS estimation,
and formulate the joint pilot sequence and BD-RIS design problem with unique
constraints induced by BD-RIS architectures. Specifically, we propose an
efficient pilot sequence and BD-RIS design which theoretically guarantees to
achieve the minimum MSE. With the estimated channel, we then consider two
BD-RIS scenarios and propose beamforming design algorithms. Finally, we provide
simulation results to verify the effectiveness of the proposed channel
estimation scheme and beamforming design algorithms. We also show that more
interelement connections in BD-RIS improves the performance while increasing
the training overhead for channel estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures, submitted to IEEE journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R2D2 image reconstruction with model uncertainty quantification in radio
  astronomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Aghabiglou, Chung San Chu, Arwa Dabbech, Yves Wiaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ``Residual-to-Residual DNN series for high-Dynamic range imaging'' (R2D2)
approach was recently introduced for Radio-Interferometric (RI) imaging in
astronomy. R2D2's reconstruction is formed as a series of residual images,
iteratively estimated as outputs of Deep Neural Networks (DNNs) taking the
previous iteration's image estimate and associated data residual as inputs. In
this work, we investigate the robustness of the R2D2 image estimation process,
by studying the uncertainty associated with its series of learned models.
Adopting an ensemble averaging approach, multiple series can be trained,
arising from different random DNN initializations of the training process at
each iteration. The resulting multiple R2D2 instances can also be leveraged to
generate ``R2D2 samples'', from which empirical mean and standard deviation
endow the algorithm with a joint estimation and uncertainty quantification
functionality. Focusing on RI imaging, and adopting a telescope-specific
approach, multiple R2D2 instances were trained to encompass the most general
observation setting of the Very Large Array (VLA). Simulations and real-data
experiments confirm that: (i) R2D2's image estimation capability is superior to
that of the state-of-the-art algorithms; (ii) its ultra-fast reconstruction
capability (arising from series with only few DNNs) makes the computation of
multiple reconstruction samples and of uncertainty maps practical even at large
image dimension; (iii) it is characterized by a very low model uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Indoor and Outdoor THz Communications with Beyond
  Diagonal-IRS: Optimization and Performance Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asad Mahmood, Thang X. Vu, Symeon Chatzinotas, Björn Ottersten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates the application of Beyond Diagonal Intelligent
Reflective Surface (BD-IRS) to enhance THz downlink communication systems,
operating in a hybrid: reflective and transmissive mode, to simultaneously
provide services to indoor and outdoor users. We propose an optimization
framework that jointly optimizes the beamforming vectors and phase shifts in
the hybrid reflective/transmissive mode, aiming to maximize the system sum
rate. To tackle the challenges in solving the joint design problem, we employ
the conjugate gradient method and propose an iterative algorithm that
successively optimizes the hybrid beamforming vectors and the phase shifts.
Through comprehensive numerical simulations, our findings demonstrate a
significant improvement in rate when compared to existing benchmark schemes,
including time- and frequency-divided approaches, by approximately $30.5\%$ and
$70.28\%$ respectively. This underscores the significant influence of IRS
elements on system performance relative to that of base station antennas,
highlighting their pivotal role in advancing the communication system efficacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reciprocity Calibration of Dual-Antenna Repeaters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik G. Larsson, Joao Vieira, Pål Frenger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a reciprocity calibration method for dual-antenna repeaters in
wireless networks. The method uses bi-directional measurements between two
network nodes, A and B, where for each bi-directional measurement, the
repeaters are configured in different states. The nodes A and B could be two
access points in a distributed MIMO system, or they could be a base station and
a mobile user terminal, for example. From the calibration measurements, the
differences between the repeaters' forward and reverse gains are estimated. The
repeaters are then (re-)configured to compensate for these differences such
that the repeaters appear, transparently to the network, as reciprocal
components of the propagation environment, enabling reciprocity-based
beamforming in the network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Wireless Communications Letters, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing Soundscapes: Leveraging Text-to-Audio Models for
  Environmental Sound Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Ronchini, Luca Comanducci, Fabio Antonacci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past few years, text-to-audio models have emerged as a significant
advancement in automatic audio generation. Although they represent impressive
technological progress, the effectiveness of their use in the development of
audio applications remains uncertain. This paper aims to investigate these
aspects, specifically focusing on the task of classification of environmental
sounds. This study analyzes the performance of two different environmental
classification systems when data generated from text-to-audio models is used
for training. Two cases are considered: a) when the training dataset is
augmented by data coming from two different text-to-audio models; and b) when
the training dataset consists solely of synthetic audio generated. In both
cases, the performance of the classification task is tested on real data.
Results indicate that text-to-audio models are effective for dataset
augmentation, whereas the performance of the models drops when relying on only
generated audio.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Environment Reconstruction based on Multi-User Selection and Multi-Modal
  Fusion in ISAC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Lin, Chuanbin Zhao, Feifei Gao, Geoffrey Ye Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrated sensing and communications (ISAC) has been deemed as a key
technology for the sixth generation (6G) wireless communications systems. In
this paper, we explore the inherent clustered nature of wireless users and
design a multi-user based environment reconstruction scheme. Specifically, we
first select users based on the estimation precision of channel's multipath,
including the line-of-sight (LOS) and the non-line-of-sight (NLOS) paths, to
enhance the accuracy of environment reconstruction. Then, we develop a fusion
strategy that merges communications signalling with camera image to increase
the accuracy and robustness of environment reconstruction. The simulation
results demonstrate that the proposed algorithm can achieve a remarkable
sensing accuracy of centimeter level, which is about 17 times better than the
scheme without user selection. Meanwhile, the fusion of communications data and
vision data leads to a threefold accuracy improvement over the image only
method, especially under challenging weather conditions like raining and
snowing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Analysis of Full-Duplex Two-Way Space Shift Keying With RIS
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xusheng Zhu, Wen Chen, Qingqing Wu, Wen Fang, Chaoying Huang, Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconfigurable intelligent surface (RIS)-assisted index modulation system
schemes are considered a promising technology for sixth-generation (6G)
wireless communication systems, which can enhance various system capabilities
such as coverage and reliability. However, obtaining perfect channel state
information (CSI) is challenging due to the lack of a radio frequency chain in
RIS. In this paper, we investigate the RIS-assisted full-duplex (FD) two-way
space shift keying (SSK) system under imperfect CSI, where the signal emissions
are augmented by deploying RISs in the vicinity of two FD users. The maximum
likelihood detector is utilized to recover the transmit antenna index. With
this in mind, we derive closed-form average bit error probability (ABEP)
expression based on the Gaussian-Chebyshev quadrature (GCQ) method and provide
the upper bound and asymptotic ABEP expressions in the presence of channel
estimation errors. To gain more insights, we also derive the outage probability
and provide the throughput of the proposed scheme with imperfect CSI. The
correctness of the analytical derivation results is confirmed via Monte Carlo
simulations. It is demonstrated that increasing the number of elements of RIS
can significantly improve the ABEP performance of the FD system over the
half-duplex (HD) system. Furthermore, in the high SNR region, the ABEP
performance of the FD system is better than that of the HD system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Spatial Correlation Characteristics of Antenna Arrays
  using Linear Operators and Wide-band Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Miranda, Sebastian Semper, Michael Döbereiner, Reiner Thomä
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The analysis of wireless communication channels at the mmWave, sub-THz and
THz bands gives rise to difficulties in the construction of antenna arrays due
to the small maximum inter-element spacing constraints at these frequencies.
Arrays with uniform spacing greater than half the wavelength for a certain
carrier frequency exhibit aliasing side-lobes in the angular domain,
prohibiting non-ambiguous estimates of a propagating wave-front's angle of
arrival.
  In this paper, we present how wide-band modelling of the array response is
useful in mitigating this spatial aliasing effect. This approach aims to reduce
the grating lobes by exploiting the angle- and frequency-dependent phase-shifts
observed in the response of the array to a planar wave-front travelling across
it.
  Furthermore, we propose a method by which the spatial correlation
characteristics of an array operating at 33 GHz carrier frequency with an
instantaneous bandwidth of 1 GHz can be improved such that the angular-domain
side-lobes are reduced by 5-10 dB. This method, applicable to arbitrary antenna
array manifolds, makes use of a linear operator that is applied to the
base-band samples of the channel transfer function measured in space and
frequency domains. By means of synthetically simulated arrays, we show that
when operating with a bandwidth of 1 GHz, the use of a derived linear operator
applied to the array output results in the spatial correlation characteristics
approaching those of the array operating at a bandwidth of 12 GHz. Hence,
non-ambiguous angle estimates can be obtained in the field without the use of
expensive high-bandwidth RF front-end components.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures, 27th Workshop on Smart Antennas, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging A Variety of Anchors in Cellular Network for Ubiquitous
  Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Liu, Shuowen Zhang, Shuguang Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrated sensing and communication (ISAC) has recently attracted tremendous
attention from both academia and industry, being envisioned as a key part of
the standards for the sixth-generation (6G) cellular network. A key challenge
of 6G-oriented ISAC lies in how to perform ubiquitous sensing based on the
communication signals and devices. Previous works have made great progresses on
studying the signal waveform design that leads to optimal communication-sensing
performance tradeoff. In this article, we aim to focus on issues arising from
the exploitation of the communication devices for sensing in 6G network.
Particularly, we will discuss about how to leverage various nodes available in
the cellular network as anchors to perform ubiquitous sensing. On one hand, the
base stations (BSs) will be the most important anchors in the future 6G ISAC
network, since they can generate/process radio signals with high range/angle
resolutions, and their positions are precisely known. Correspondingly, we will
first study the BS-based sensing technique. On the other hand, the BSs alone
may not enable ubiquitous sensing, since they cannot cover all the places with
strong line-of-sight (LOS) links. This motivates us to investigate the
possibility of using other nodes that are with higher density in the network to
act as the anchors. Along this line, we are interested in two types of new
anchors - user equipments (UEs) and reconfigurable intelligent surfaces (RISs).
This paper will shed light on the opportunities and challenges brought by
UE-assisted sensing and RIS-assisted sensing. Our goal is to devise a novel
6G-oriented sensing architecture where BSs, UEs, and RISs can work together to
provide ubiquitous sensing services.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in IEEE Communications Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable cancer cell detection with phonon microscopy using
  multi-task conditional neural networks for inter-batch calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijie Zheng, Rafael Fuentes-Dominguez, Matt Clark, George S. D. Gordon, Fernando Perez-Cota
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in artificial intelligence (AI) show great potential in revealing
underlying information from phonon microscopy (high-frequency ultrasound) data
to identify cancerous cells. However, this technology suffers from the 'batch
effect' that comes from unavoidable technical variations between each
experiment, creating confounding variables that the AI model may inadvertently
learn. We therefore present a multi-task conditional neural network framework
to simultaneously achieve inter-batch calibration, by removing confounding
variables, and accurate cell classification of time-resolved phonon-derived
signals. We validate our approach by training and validating on different
experimental batches, achieving a balanced precision of 89.22% and an average
cross-validated precision of 89.07% for classifying background, healthy and
cancerous regions. Classification can be performed in 0.5 seconds with only
simple prior batch information required for multiple batch corrections.
Further, we extend our model to reconstruct denoised signals, enabling physical
interpretation of salient features indicating disease state including sound
velocity, sound attenuation and cell-adhesion to substrate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Waveform Design for Joint Communication and SAR Imaging Under Random
  Signaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zheng, Fan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional synthetic aperture radar (SAR) imaging systems typically employ
deterministic signal designs, which lack the capability to convey communication
information and are thus not suitable for integrated sensing and communication
(ISAC) scenarios. In this letter, we propose a joint communication and SAR
imaging (JCASAR) system based on orthogonal frequency-division multiplexing
(OFDM) signal with cyclic prefix (CP), which is capable of reconstructing the
target profile while serving a communication user. In contrast to traditional
matched filters, we propose a least squares (LS) estimator for range profiling.
Then the SAR image is obtained followed by range cell migration correction
(RCMC) and azimuth processing. By minimizing the mean squared error (MSE) of
the proposed LS estimator, we investigate the optimal waveform design for SAR
imaging, and JCASAR under random signaling, where power allocation strategies
are conceived for Gaussian-distributed ISAC signals, in an effort to strike a
flexible performance tradeoff between the communication and SAR imaging tasks.
Numerical results are provided to validate the effectiveness of the proposed
ISAC waveform design for JCASAR systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Channel-Adaptive Pilot Design for FDD-MIMO Systems Utilizing Gaussian
  Mixture Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nurettin Turan, Benedikt Fesl, Benedikt Böck, Michael Joham, Wolfgang Utschick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose to utilize Gaussian mixture models (GMMs) to design
pilots for downlink (DL) channel estimation in frequency division duplex (FDD)
systems. The GMM captures prior information during training that is leveraged
to design a codebook of pilot matrices in an initial offline phase. Once shared
with the mobile terminal (MT), the GMM is utilized to determine a feedback
index at the MT in the online phase. This index selects a pilot matrix from a
codebook, eliminating the need for online pilot optimization. The GMM is
further used for DL channel estimation at the MT via observation-dependent
linear minimum mean square error (LMMSE) filters, parametrized by the GMM. The
analytic representation of the GMM allows adaptation to any signal-to-noise
ratio (SNR) level and pilot configuration without re-training. With extensive
simulations, we demonstrate the superior performance of the proposed GMM-based
pilot scheme compared to state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaultGuard: A Generative Approach to Resilient Fault Prediction in Smart
  Electrical Grids 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emad Efatinasab, Francesco Marchiori, Alessandro Brighente, Mirco Rampazzo, Mauro Conti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting and classifying faults in electricity networks is crucial for
uninterrupted provision and keeping maintenance costs at a minimum. Thanks to
the advancements in the field provided by the smart grid, several data-driven
approaches have been proposed in the literature to tackle fault prediction
tasks. Implementing these systems brought several improvements, such as optimal
energy consumption and quick restoration. Thus, they have become an essential
component of the smart grid. However, the robustness and security of these
systems against adversarial attacks have not yet been extensively investigated.
These attacks can impair the whole grid and cause additional damage to the
infrastructure, deceiving fault detection systems and disrupting restoration.
In this paper, we present FaultGuard, the first framework for fault type and
zone classification resilient to adversarial attacks. To ensure the security of
our system, we employ an Anomaly Detection System (ADS) leveraging a novel
Generative Adversarial Network training layer to identify attacks. Furthermore,
we propose a low-complexity fault prediction model and an online adversarial
training technique to enhance robustness. We comprehensively evaluate the
framework's performance against various adversarial attacks using the
IEEE13-AdvAttack dataset, which constitutes the state-of-the-art for resilient
fault prediction benchmarking. Our model outclasses the state-of-the-art even
without considering adversaries, with an accuracy of up to 0.958. Furthermore,
our ADS shows attack detection capabilities with an accuracy of up to 1.000.
Finally, we demonstrate how our novel training layers drastically increase
performances across the whole framework, with a mean increase of 154% in ADS
accuracy and 118% in model accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A TDD Distributed MIMO Testbed Using a 1-Bit Radio-Over-Fiber Fronthaul
  Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lise Aabel, Sven Jacobsson, Mikael Coldrey, Frida Olofsson, Giuseppe Durisi, Christian Fager
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the uplink and downlink of a time-division duplex distributed
multiple-input multiple-output (D-MIMO) testbed, based on a 1-bit
radio-over-fiber architecture, which is low-cost and scalable. The proposed
architecture involves a central unit (CU) that is equipped with 1-bit
digital-to-analog and analog-to-digital converters, operating at 10 GS/s. The
CU is connected to multiple single-antenna remote radio heads (RRHs) via
optical fibers, over which a binary RF waveform is transmitted. In the uplink,
a binary RF waveform is generated at the RRHs by a comparator, whose inputs are
the received RF signal and a suitably designed dither signal. In the downlink,
a binary RF waveform is generated at the CU via bandpass sigma-delta
modulation. Our measurement results show that low error-vector magnitude (EVM)
can be achieved in both the uplink and the downlink, despite 1-bit sampling at
the CU. Specifically, for point-to-point over-cable transmission between a
single user equipment (UE) and a CU equipped with a single RRH, we report, for
a 10 MBd signal using single-carrier 16QAM modulation, an EVM of 3.3% in the
downlink, and of 4.5% in the uplink. We then consider a CU connected to 3 RRHs
serving over the air 2 UEs, and show that, after over-the-air reciprocity
calibration, a downlink zero-forcing precoder designed on the basis of uplink
channel estimates at the CU, achieves an EVM of 6.4% and 10.9% at UE 1 and UE
2, respectively. Finally, we investigate the ability of the proposed
architecture to support orthogonal frequency-division multiplexing (OFDM)
waveforms, and its robustness against both in-band and out-of-band
interference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Transactions on Microwave Theory and
  Techniques</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance Analysis of Integrated Sensing and Communication Networks
  with Blockage Effects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhong Sun, Shi Yan, Ning Jiang, Jiaen Zhou, Mugen Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Communication-sensing integration represents an up-and-coming area of
research, enabling wireless networks to simultaneously perform communication
and sensing tasks. However, in urban cellular networks, the blockage of
buildings results in a complex signal propagation environment, affecting the
performance analysis of integrated sensing and communication (ISAC) networks.
To overcome this obstacle, this paper constructs a comprehensive framework
considering building blockage and employs a distance-correlated blockage model
to analyze interference from line of sight (LoS), non-line of sight (NLoS), and
target reflection cascading (TRC) links. Using stochastic geometric theory,
expressions for signal-to-interference-plus-noise ratio (SINR) and coverage
probability for communication and sensing in the presence of blockage are
derived, allowing for a comprehensive comparison under the same parameters. The
research findings indicate that blockage can positively impact coverage,
especially in enhancing communication performance. The analysis also suggests
that there exists an optimal base station (BS) density when blockage is of the
same order of magnitude as the BS density, maximizing communication or sensing
coverage probability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Destination-Constrained Linear Dynamical System Modeling in Set-Valued
  Frameworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaowei Yang, Haiqi Liu, Fanqin Meng, Xiaojing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Directional motion towards a specified destination is a common occurrence in
physical processes and human societal activities. Utilizing this prior
information can significantly improve the control and predictive performance of
system models. This paper primarily focuses on reconstructing linear dynamic
system models based on destination constraints in the set-valued framework. We
treat destination constraints as inherent information in the state evolution
process and employ convex optimization techniques to construct a coherent and
robust state model. This refined model effectively captures the impact of
destination constraints on the state evolution at each time step. Furthermore,
we design an optimal weight matrix for the reconstructed model to ensure
smoother and more natural trajectories of state evolution. We also analyze the
theoretical guarantee of optimality for this weight matrix and the properties
of the reconstructed model. Finally, simulation experiments verify that the
reconstructed model has significant advantages over the unconstrained and
unoptimized weighted models and constrains the evolution of state trajectories
with different starting and ending points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Learning for Joint Beamforming Design in RIS-aided ISAC
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Ye, Lei Huang, Zhen Chen, Peichang Zhang, Mohamed Rihan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is critical to design efficient beamforming in reconfigurable intelligent
surface (RIS)-aided integrated sensing and communication (ISAC) systems for
enhancing spectrum utilization. However, conventional methods often have
limitations, either incurring high computational complexity due to iterative
algorithms or sacrificing performance when using heuristic methods. To achieve
both low complexity and high spectrum efficiency, an unsupervised
learning-based beamforming design is proposed in this work. We tailor
image-shaped channel samples and develop an ISAC beamforming neural network
(IBF-Net) model for beamforming. By leveraging unsupervised learning, the loss
function incorporates key performance metrics like sensing and communication
channel correlation and sensing channel gain, eliminating the need of labeling.
Simulations show that the proposed method achieves competitive performance
compared to benchmarks while significantly reduces computational complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, references added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Impact of Random Node Sampling on Adaptive Diffusion Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel G. Tiglea, Renato Candido, Magno T. M. Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we analyze the effects of random sampling on adaptive
diffusion networks. These networks consist in a collection of nodes that can
measure and process data, and that can communicate with each other to pursue a
common goal of estimating an unknown system. In particular, we consider in our
theoretical analysis the diffusion least-mean-squares algorithm in a scenario
in which the nodes are randomly sampled. Hence, each node may or may not adapt
its local estimate at a certain iteration. Our model shows that, if the nodes
cooperate, a reduction in the sampling probability leads to a slight decrease
in the steady-state Network Mean-Square Deviation (NMSD), assuming that the
environment is stationary and that all other parameters of the algorithm are
kept fixed. Furthermore, under certain circumstances, this can also ensure the
stability of the algorithm in situations in which it would otherwise be
unstable. Although counter-intuitive, our findings are backed by simulation
results, which match the theoretical curves well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the IEEE Transactions on Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HOOD: Real-Time Human Presence and Out-of-Distribution Detection Using
  FMCW Radar 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02396v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02396v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabri Mustafa Kahya, Muhammet Sami Yavuz, Eckehard Steinbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting human presence indoors with millimeter-wave frequency-modulated
continuous-wave (FMCW) radar faces challenges from both moving and stationary
clutter. This work proposes a robust and real-time capable human presence and
out-of-distribution (OOD) detection method using 60 GHz short-range FMCW radar.
HOOD solves the human presence and OOD detection problems simultaneously in a
single pipeline. Our solution relies on a reconstruction-based architecture and
works with radar macro and micro range-Doppler images (RDIs). HOOD aims to
accurately detect the presence of humans in the presence or absence of moving
and stationary disturbers. Since HOOD is also an OOD detector, it aims to
detect moving or stationary clutters as OOD in humans' absence and predicts the
current scene's output as "no presence." HOOD performs well in diverse
scenarios, demonstrating its effectiveness across different human activities
and situations. On our dataset collected with a 60 GHz short-range FMCW radar,
we achieve an average AUROC of 94.36%. Additionally, our extensive evaluations
and experiments demonstrate that HOOD outperforms state-of-the-art (SOTA) OOD
detection methods in terms of common OOD detection metrics. Importantly, HOOD
also perfectly fits on Raspberry Pi 3B+ with an ARM Cortex-A53 CPU, which
showcases its versatility across different hardware environments. Videos of our
human presence detection experiments are available at:
https://muskahya.github.io/HOOD
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures, project page: https://muskahya.github.io/HOOD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-Complexity Linear Decoupling of Users for Uplink Massive MU-MIMO
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. Sowmya, Gokularam Muthukrishnan, K. Giridhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Massive MIMO (mMIMO) enables users with different requirements to get
connected to the same base station (BS) on the same set of resources. In the
uplink of Multiuser massive MIMO (MU-mMIMO), while such heterogeneous users are
served, decoupling facilitates the use of user-specific detection schemes. In
this paper, we propose a low-complexity linear decoupling scheme called
Sequential Decoupler (SD), which aids in the parallel detection of each user's
data stream. The proposed algorithm shows significant complexity reduction.
Simulations reveal that the complexity of the proposed scheme is only 0.15% of
the conventional Singular Value Decomposition (SVD) based decoupling and is
about 47% of the pseudo-inverse based decoupling schemes when 80 users with two
antennas each are served by the BS. Also, the proposed scheme is scalable when
new users are added to the system and requires fewer operations than computing
the decoupler all over again. Further numerical analyses indicate that the
proposed scheme achieves significant complexity reduction without any
degradation in performance and is a promising low-complex alternative to the
existing decoupling schemes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-Rate Phase Association with Travel Time Neural Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07572v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07572v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Shi, Maarten V. de Hoop, Ivan Dokmanić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our understanding of regional seismicity from multi-station seismograms
relies on the ability to associate arrival phases with their originating
earthquakes. Deep-learning-based phase detection now detects small, high-rate
arrivals from seismicity clouds, even at negative magnitudes. This new data
could give important insight into earthquake dynamics, but it is presents a
challenging association task. Existing techniques relying on coarsely
approximated, fixed wave speed models fail in this unexplored dense regime
where the complexity of unknown wave speed cannot be ignored. We introduce
Harpa, a high-rate association framework built on deep generative modeling and
neural fields. Harpa incorporates wave physics by using optimal transport to
compare arrival sequences. It is thus robust to unknown wave speeds and
estimates the wave speed model as a by-product of association. Experiments with
realistic, complex synthetic models show that Harpa is the first seismic phase
association framework which is accurate in the high-rate regime, paving the way
for new avenues in exploratory Earth science and improved understanding of
seismicity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Room Transfer Function Reconstruction Using Complex-valued Neural
  Networks and Irregularly Distributed Microphones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Ronchini, Luca Comanducci, Mirco Pezzoli, Fabio Antonacci, Augusto Sarti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing the room transfer functions needed to calculate the complex
sound field in a room has several important real-world applications. However,
an unpractical number of microphones is often required. Recently, in addition
to classical signal processing methods, deep learning techniques have been
applied to reconstruct the room transfer function starting from a very limited
set of measurements at scattered points in the room. In this paper, we employ
complex-valued neural networks to estimate room transfer functions in the
frequency range of the first room resonances, using a few irregularly
distributed microphones. To the best of our knowledge, this is the first time
that complex-valued neural networks are used to estimate room transfer
functions. To analyze the benefits of applying complex-valued optimization to
the considered task, we compare the proposed technique with a state-of-the-art
kernel-based signal processing approach for sound field reconstruction, showing
that the proposed technique exhibits relevant advantages in terms of phase
accuracy and overall quality of the reconstructed sound field. For informative
purposes, we also compare the model with a similarly-structured data-driven
approach that, however, applies a real-valued neural network to reconstruct
only the magnitude of the sound field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Accuracy of Phase Extraction from a Known-Frequency Noisy
  Sinusoidal Signal <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03935v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03935v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanuel Dervieux, Florian Tilquin, Alexis Bisiaux, Wilfried Uhring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate phase extraction from sinusoidal signals is a crucial task in
various signal processing applications. While prior research predominantly
addresses the case of asynchronous sampling with unknown signal frequency, this
study focuses on the more specific situation where synchronous sampling is
possible, and the signal's frequency is known. In this framework, a
comprehensive analysis of phase estimation accuracy in the presence of both
additive and phase noises is presented. A closed-form expression for the
asymptotic Probability Density Function (PDF) of the resulting phase estimator
is presented, and validated by simulations that depict Root Mean Square Error
(RMSE) trends in different noise scenarios. The latter estimator is
asymptotically efficient, exhibiting fast convergence towards its Cram\'er-Rao
Lower Bound (CRLB). Three distinct RMSE behaviours were identified depending on
the Signal to Noise Ratio (SNR), sample count (N), and noise level: (i)
saturation towards a random guess at low SNR values, (ii) linear decreasing
relationship with the square roots of N and SNR at moderate noise levels, and
(iii) saturation at high SNR towards a noise floor function of the phase noise
level. By quantifying the impact of sample count, additive noise, and phase
noise on phase estimation accuracy, this work provides valuable insights for
designing systems that require precise phase extraction, such as phase-based
fluorescence assays or system identification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 11 figures, initially submitted to Analog Integrated
  Circuits and Signal Processing the 19th December 2023, Updated 2024/02/07:
  added watermark, Updated 2024/03/26 : due to a lack of response from AICSP
  despite several follow-up, we withdrew it, and submitted it to IEEE
  Transactions on Signal Processing instead</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discretized Distributed Optimization over Dynamic Digraphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Doostmohammadian, Wei Jiang, Muwahida Liaquat, Alireza Aghasi, Houman Zarrabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a discrete-time model of continuous-time distributed optimization
over dynamic directed-graphs (digraphs) with applications to distributed
learning. Our optimization algorithm works over general strongly connected
dynamic networks under switching topologies, e.g., in mobile multi-agent
systems and volatile networks due to link failures. Compared to many existing
lines of work, there is no need for bi-stochastic weight designs on the links.
The existing literature mostly needs the link weights to be stochastic using
specific weight-design algorithms needed both at the initialization and at all
times when the topology of the network changes. This paper eliminates the need
for such algorithms and paves the way for distributed optimization over
time-varying digraphs. We derive the bound on the gradient-tracking step-size
and discrete time-step for convergence and prove dynamic stability using
arguments from consensus algorithms, matrix perturbation theory, and Lyapunov
theory. This work, particularly, is an improvement over existing
stochastic-weight undirected networks in case of link removal or packet drops.
This is because the existing literature may need to rerun time-consuming and
computationally complex algorithms for stochastic design, while the proposed
strategy works as long as the underlying network is weight-symmetric and
balanced. The proposed optimization framework finds applications to distributed
classification and learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Robotics Meets Wireless Communications: An Introductory Tutorial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.02021v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.02021v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Bonilla Licea, Mounir Ghogho, Martin Saska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of ground Mobile Robots (MRs) and Unmanned Aerial Vehicles
(UAVs) within the research community, industry, and society is growing fast.
Many of these agents are nowadays equipped with communication systems that are,
in some cases, essential to successfully achieve certain tasks. In this
context, we have begun to witness the development of a new interdisciplinary
research field at the intersection of robotics and communications. This
research field has been boosted by the intention of integrating UAVs within the
5G and 6G communication networks. This research will undoubtedly lead to many
important applications in the near future. Nevertheless, one of the main
obstacles to the development of this research area is that most researchers
address these problems by oversimplifying either the robotics or the
communications aspect. This impedes the ability of reaching the full potential
of this new interdisciplinary research area. In this tutorial, we present some
of the modelling tools necessary to address problems involving both robotics
and communication from an interdisciplinary perspective. As an illustrative
example of such problems, we focus in this tutorial on the issue of
communication-aware trajectory planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 192 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Frequency Bin Interval in FFT via Dense Sampling Factor
  $α$ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Fast Fourier Transform (FFT) is a fundamental tool for signal analysis,
widely used across various fields. However, traditional FFT methods encounter
challenges in adjusting the frequency bin interval, which may impede accurate
spectral analysis. In this study, we propose a method for adjusting the
frequency bin interval in FFT by introducing a parameter $\alpha$. We elucidate
the underlying principles of the proposed method and discuss its potential
applications across various contexts. Our findings suggest that the proposed
method offers a promising approach to overcome the limitations of traditional
FFT methods and enhance spectral analysis accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Redundancy Transmission in UAV-Aided LoRa Networks Featuring Wake-Up
  Radios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhartha S. Borkotoky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a LoRa sensor network featuring a UAV-mounted gateway for
collecting sensor data (messages). Wake-up radios (WuR) are employed to inform
the sensors of the UAV's arrival. Building on an existing random access scheme
for such setups, we propose and evaluate two redundancy transmission protocols
for enhancing the reliability of the data transfer. One protocol employs
fountain-coded transmissions, whereas the other performs message replication.
Our results illustrate how redundancy transmission can be beneficial under the
time constraints imposed by the UAV's limited hovering duration, and how node
density, hovering time, and sensor energy budget impact the performance of the
schemes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OTFS-based Robust MMSE Precoding Design in Over-the-air Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongkai Zhou, Jing Guo, Siqiang Wang, Zhong Zheng, Zesong Fei, Weijie Yuan, Xinyi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over-the-air computation (AirComp), as a data aggregation method that can
improve network efficiency by exploiting the superposition characteristics of
wireless channels, has received much attention recently. Meanwhile, the
orthogonal time frequency space (OTFS) modulation can provide a strong Doppler
resilience and facilitate reliable transmission for high-mobility
communications. Hence, in this work, we investigate an OTFS-based AirComp
system in the presence of time-frequency dual-selective channels. In
particular, we commence from the development of a novel transmission framework
for the considered system, where the pilot signal is sent together with data,
and the channel estimation is implemented according to the echo from the access
point to the sensor, thereby reducing the overhead of channel state information
(CSI) feedback. Hereafter, based on the CSI estimated from the previous frame,
a robust precoding matrix aiming at minimizing mean square error in the current
frame is designed, which takes into account the estimation error from the
receiver noise and the outdated CSI. The simulation results demonstrate the
effectiveness of the proposed robust precoding scheme by comparing it with the
non-robust precoding. The performance gain is more obvious in a high
signal-to-noise ratio in case of large channel estimation errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Digital Twin Channel for 6G: Concepts, Architectures and Potential
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Wang, Jianhua Zhang, Gaofeng Nie, Li Yu, Zhiqiang Yuan, Tongjie Li, Jialin Wang, Guangyi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital twin channel (DTC) is the real-time mapping of a wireless channel
from the physical world to the digital world, which is expected to provide
significant performance enhancements for the sixth-generation (6G)
air-interface design. In this work, we first define five evolution levels of
channel twins with the progression of wireless communication. The fifth level,
autonomous DTC, is elaborated with multi-dimensional factors such as
methodology, characterization precision, and data category. Then, we provide
detailed insights into the requirements and architecture of a complete DTC for
6G. Subsequently, a sensing-enhanced real-time channel prediction platform and
experimental validations are exhibited. Finally, drawing from the vision of the
6G network, we explore the potential applications and the open issues in future
DTC research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 15 references. It will be submitted to IEEE
  journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimation of Complex Valued Laplacian Matrices for Topology
  Identification in Power Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morad Halihal, Tirza Routtenberg, H. Vincent Poor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the problem of estimating a complex-valued
Laplacian matrix with a focus on its application in the estimation of
admittance matrices in power systems. The proposed approach is based on a
constrained maximum likelihood estimator (CMLE) of the complex-valued
Laplacian, which is formulated as an optimization problem with Laplacian and
sparsity constraints. The complex-valued Laplacian is a symmetric,
non-Hermitian matrix that exhibits a joint sparsity pattern between its real
and imaginary parts. Thus, we present a group-sparse-based penalized
log-likelihood approach for the Laplacian estimation. Leveraging the mixed \ell
2,1 norm relaxation of the joint sparsity constraint, we develop a new
alternating direction method of multipliers (ADMM) estimation algorithm for the
implementation of the CMLE of the Laplacian matrix under a linear Gaussian
model. Next, we apply the proposed ADMM algorithms for the problem of
estimating the admittance matrix under three commonly used measurement models
that stem from Kirchhoff and Ohm laws, each with different assumptions and
simplifications: 1) the nonlinear alternating current (AC) model; 2) the
decoupled linear power flow (DLPF) model; and 3) the direct current (DC) model.
The performance of the ADMM algorithm is evaluated using data from the IEEE
33-bus power system data under different settings. The numerical experiments
demonstrate that the proposed algorithm outperforms existing methods in terms
of mean-squared-error (MSE) and F-score, thus providing a more accurate
recovery of the admittance matrix.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RIS-assisted Cell-Free Massive MIMO Systems With Two-Timescale Design
  and Hardware Impairments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15588v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15588v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianxin Dai, Jin Ge, Kangda Zhi, Cunhua Pan, Youguo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating the reconfigurable intelligent surface (RIS) into a cell-free
massive multiple-input multiple-output (CF-mMIMO) system is an effective
solution to achieve high system capacity with low cost and power consumption.
However, existing works of RIS-assisted systems mostly assumed perfect
hardware, while the impact of hardware impairments (HWIs) is generally ignored.
In this paper, we consider the general Rician fading channel and uplink
transmission of the RIS-assisted CF-mMIMO system under transceiver impairments
and RIS phase noise. To reduce the feedback overhead and power consumption, we
propose a two-timescale transmission scheme to optimize the passive beamformers
at RISs with statistical channel state information (CSI), while transmit
beamformers at access points (APs) are designed based on instantaneous CSI.
Also, the maximum ratio combining (MRC) detection is applied to the central
processing unit (CPU). On this basis, we derive the closed-form approximate
expression of the achievable rate, based on which the impact of HWIs and the
power scaling laws are analyzed to draw useful theoretical insights. To
maximize the users' sum rate or minimum rate, we first transform our rate
expression into a tractable form, and then optimize the phase shifts of RISs
based on an accelerated gradient ascent method. Finally, numerical results are
presented to demonstrate the correctness of our derived expressions and
validate the previous analysis, which provide some guidelines for the practical
application of the imperfect RISs in the CF-mMIMO with transceiver HWIs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metasurface-Enabled Multifunctional Single-Frequency Sensors without
  External Power 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masaya Tashiro, Kosuke Ide, Kosei Asano, Satoshi Ishii, Yuta Sugiura, Akira Uchiyama, Ashif A. Fathnan, Hiroki Wakatsuchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  IoT sensors are crucial for visualizing multidimensional and multimodal
information and enabling future IT applications/services such as cyber-physical
space, digital twins, autonomous driving, smart cities, and virtual/augmented
reality (VR or AR). However, IoT sensors need to be battery-free to
realistically manage and maintain the growing number of available sensing
devices. Here, we provide a novel sensor design approach that employs
metasurfaces to enable multifunctional sensing without requiring an external
power source. Importantly, unlike existing metasurface-based sensors, our
metasurfaces can sense multiple physical parameters even at a fixed frequency
by breaking classic harmonic oscillations in the time domain, making the
proposed sensors viable for usage with limited frequency resources. Moreover,
we provide a method for predicting physical parameters using the machine
learning-based approach of random forest regression. The sensing performance
was confirmed by estimating temperature and light intensity, and excellent
determination coefficients larger than 0.96 were achieved. Our study affords
new opportunities for sensing multiple physical properties without relying on
an external power source or needing multiple frequencies, which markedly
simplifies and facilitates the design of next-generation wireless communication
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 23 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identification of Craving Maps among Marijuana Users via the Analysis of
  Functional Brain Networks with High-Order Attention Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00033v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00033v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-En Ding, Shihao Yang, Anna Zilverstand, Feng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The excessive consumption of marijuana can induce substantial psychological
and social consequences. In this investigation, we propose an elucidative
framework termed high-order graph attention neural networks (HOGANN) for the
classification of Marijuana addiction, coupled with an analysis of localized
brain network communities exhibiting abnormal activities among chronic
marijuana users. HOGANN integrates dynamic intrinsic functional brain networks,
estimated from resting-state functional magnetic resonance imaging (rs-fMRI),
using long short-term memory (LSTM) to capture temporal network dynamics. We
employ a high-order attention module for information fusion and message passing
among neighboring nodes, enhancing the network community analysis. Our model is
validated across two distinct data cohorts, yielding substantially higher
classification accuracy than benchmark algorithms. Furthermore, we discern the
most pertinent subnetworks and cognitive regions affected by persistent
marijuana consumption, indicating adverse effects on functional brain networks,
particularly within the dorsal attention and frontoparietal networks.
Intriguingly, our model demonstrates superior performance in cohorts exhibiting
prolonged dependence, implying that prolonged marijuana usage induces more
pronounced alterations in brain networks. The model proficiently identifies
craving brain maps, thereby delineating critical brain regions for analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrated Sensing and Communications Towards Proactive Beamforming in
  mmWave V2I via Multi-Modal Feature Fusion (MMFF) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02561v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02561v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Zhang, Shijian Gao, Xiang Cheng, Liuqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The future of vehicular communication networks relies on mmWave massive
multi-input-multi-output antenna arrays for intensive data transfer and massive
vehicle access. However, reliable vehicle-to-infrastructure links require exact
alignment between the narrow beams, which traditionally involves excessive
signaling overhead. To address this issue, we propose a novel proactive
beamforming scheme that integrates multi-modal sensing and communications via
Multi-Modal Feature Fusion Network (MMFF-Net), which is composed of multiple
neural network components with distinct functions. Unlike existing methods that
rely solely on communication processing, our approach obtains comprehensive
environmental features to improve beam alignment accuracy. We verify our scheme
on the Vision-Wireless (ViWi) dataset, which we enriched with realistic vehicle
drifting behavior. Our proposed MMFF-Net achieves more accurate and stable
angle prediction, which in turn increases the achievable rates and reduces the
communication system outage probability. Even in complex dynamic scenarios with
adverse environment conditions, robust prediction results can be guaranteed,
demonstrating the feasibility and practicality of the proposed proactive
beamforming approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 12 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-Frequency-Space Transmit Design and Receiver Processing with
  Dynamic Subarray for Terahertz Integrated Sensing and Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04440v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04440v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongzhi Wu, Chong Han, Meixia Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Terahertz (THz) integrated sensing and communication (ISAC) enables
simultaneous data transmission with Terabit-per-second (Tbps) rate and
millimeter-level accurate sensing. To realize such a blueprint, ultra-massive
antenna arrays with directional beamforming are used to compensate for severe
path loss in the THz band. In this paper, the time-frequency-space transmit
design is investigated for THz ISAC to generate time-varying scanning sensing
beams and stable communication beams. Specifically, with the dynamic
array-of-subarray (DAoSA) hybrid beamforming architecture and multi-carrier
modulation, two ISAC hybrid precoding algorithms are proposed, namely, a
vectorization (VEC) based algorithm that outperforms existing ISAC hybrid
precoding methods and a low-complexity sensing codebook assisted (SCA)
approach. Meanwhile, coupled with the transmit design, parameter estimation
algorithms are proposed to realize high-accuracy sensing, including a wideband
DAoSA MUSIC method for angle estimation and a sum-DFT-GSS approach for range
and velocity estimation. Numerical results indicate that the proposed
algorithms can realize centi-degree-level angle estimation accuracy and
millimeter-level range estimation accuracy, which are one or two orders of
magnitudes better than the methods in the millimeter-wave band. In addition, to
overcome the cyclic prefix limitation and Doppler effects, an inter-symbol
interference- and inter-carrier interference-tackled sensing algorithm is
developed to refine sensing capabilities for THz ISAC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Near-Field Multiuser Beam-Training for Extremely Large-Scale MIMO
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13597v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13597v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang Liu, Cunhua Pan, Hong Ren, Jiangzhou Wang, Robert Schober, Lajos Hanzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extremely large-scale multiple-input multiple-output (XL-MIMO) systems are
capable of improving spectral efficiency by employing far more antennas than
conventional massive MIMO at the base station (BS). However, beam training in
multiuser XL-MIMO systems is challenging. To tackle these issues, we conceive a
three-phase graph neural network (GNN)-based beam training scheme for multiuser
XL-MIMO systems. In the first phase, only far-field wide beams have to be
tested for each user and the GNN is utilized to map the beamforming gain
information of the far-field wide beams to the optimal near-field beam for each
user. In addition, the proposed GNN-based scheme can exploit the
position-correlation between adjacent users for further improvement of the
accuracy of beam training. In the second phase, a beam allocation scheme based
on the probability vectors produced at the outputs of GNNs is proposed to
address the above beam-direction conflicts between users. In the third phase,
the hybrid TBF is designed for further reducing the inter-user interference.
Our simulation results show that the proposed scheme improves the beam training
performance of the benchmarks. Moreover, the performance of the proposed beam
training scheme approaches that of an exhaustive search, despite requiring only
about 7% of the pilot overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Systems and Control
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Code Generation for Conic Model-Predictive Control on Microcontrollers
  with TinyMPC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Schoedel, Khai Nguyen, Elakhya Nedumaran, Brian Plancher, Zachary Manchester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conic constraints appear in many important control applications like legged
locomotion, robotic manipulation, and autonomous rocket landing. However,
current solvers for conic optimization problems have relatively heavy
computational demands in terms of both floating-point operations and memory
footprint, making them impractical for use on small embedded devices. We extend
TinyMPC, an open-source, high-speed solver targeting low-power embedded control
applications, to handle second-order cone constraints. We also present
code-generation software to enable deployment of TinyMPC on a variety of
microcontrollers. We benchmark our generated code against state-of-the-art
embedded QP and SOCP solvers, demonstrating a two-order-of-magnitude speed
increase over ECOS while consuming less memory. Finally, we demonstrate
TinyMPC's efficacy on the Crazyflie, a lightweight, resource-constrained
quadrotor with fast dynamics. TinyMPC and its code-generation tools are
publicly available at https://tinympc.org.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to CDC, 2024. First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiple Model Reference Adaptive Control with Blending for Non-Square
  Multivariable Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Lovi, Baris Fidan, Christopher Nielsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we develop a multiple model reference adaptive controller
(MMRAC) with blending. The systems under consideration are non-square, i.e.,
the number of inputs is not equal to the number of states; multi-input, linear,
time-invariant with uncertain parameters that lie inside of a known, compact,
and convex set. Moreover, the full state of the plant is available for
feedback. A multiple model online identification scheme for the plant's state
and input matrices is developed that guarantees the estimated parameters
converge to the underlying plant model under the assumption of persistence of
excitation. Using an exact matching condition, the parameter estimates are used
in a control law such that the plant's states asymptotically track the
reference signal generated by a state-space model reference. The control
architecture is proven to provide boundedness of all closed-loop signals and to
asymptotically drive the state tracking error to zero. Numerical simulations
illustrate the stability and efficacy of the proposed MMRAC scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, IEEE Journal Submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ANOCA: AC Network-aware Optimal Curtailment Approach for Dynamic Hosting
  Capacity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanuel O. Badmus, Amritanshu Pandey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With exponential growth in distributed energy resources (DERs) coupled with
at-capacity distribution grid infrastructure, prosumers cannot always export
all extra power to the grid without violating technical limits. Consequently, a
slew of dynamic hosting capacity (DHC) algorithms have emerged for optimal
utilization of grid infrastructure while maximizing export from DERs. Most of
these DHC algorithms utilize the concept of operating envelopes (OE)}, where
the utility gives prosumers technical power export limits, and they are free to
export power within these limits. Recent studies have shown that OE-based
frameworks have drawbacks, as most develop power export limits based on convex
or linear grid models. As OEs must capture extreme operating conditions, both
convex and linear models can violate technical limits in practice because they
approximate grid physics. However, AC models are unsuitable because they may
not be feasible within the whole region of OE. We propose a new two-stage
optimization framework for DHC built on three-phase AC models to address the
current gaps. In this approach, the prosumers first run a receding horizon
multi-period optimization to identify optimal export power setpoints to
communicate with the utility. The utility then performs an infeasibility-based
optimization to either accept the prosumer's request or dispatch an optimal
curtail signal such that overall system technical constraints are not violated.
To explore various curtailment strategies, we develop an L1, L2, and Linf
norm-based dispatch algorithm with an exact three-phase AC model. We test our
framework on a 1420 three-phase node meshed distribution network and show that
the proposed algorithm optimally curtails DERs while guaranteeing the AC
feasibility of the network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Sontag s to Cardano-Lyapunov Formula for Systems Not Affine in the
  Control: Convection-Enabled PDE Stabilization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Camil Belhadjoudja, Miroslav Krstic, Mohamed Maghenem, Emmanuel Witrant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the first generalization of Sontag s universal controller to
systems not affine in the control, particularly, to PDEs with boundary
actuation. We assume that the system admits a control Lyapunov function (CLF)
whose derivative, rather than being affine in the control, has either a
depressed cubic, quadratic, or depressed quartic dependence on the control. For
each case, a continuous universal controller that vanishes at the origin and
achieves global exponential stability is derived. We prove our result in the
context of convectionreaction-diffusion PDEs with Dirichlet actuation. We show
that if the convection has a certain structure, then the L2 norm of the state
is a CLF. In addition to generalizing Sontag s formula to some non-affine
systems, we present the first general Lyapunov approach for boundary control of
nonlinear PDEs. We illustrate our results via a numerical example.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be presented at the 2024 American Control Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Path Integral Control with Rollout Clustering and Dynamic Obstacles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Patrick, Efstathios Bakolas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model Predictive Path Integral (MPPI) control has proven to be a powerful
tool for the control of uncertain systems (such as systems subject to
disturbances and systems with unmodeled dynamics). One important limitation of
the baseline MPPI algorithm is that it does not utilize simulated trajectories
to their fullest extent. For one, it assumes that the average of all
trajectories weighted by their performance index will be a safe trajectory. In
this paper, multiple examples are shown where the previous assumption does not
hold, and a trajectory clustering technique is presented that reduces the
chances of the weighted average crossing in an unsafe region. Secondly, MPPI
does not account for dynamic obstacles, so the authors put forward a novel cost
function that accounts for dynamic obstacles without adding significant
computation time to the overall algorithm. The novel contributions proposed in
this paper were evaluated with extensive simulations to demonstrate
improvements upon the state-of-the-art MPPI techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, extended version of ACC 2024 submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Boundary Control of the Kuramoto-Sivashinsky Equation Under
  Intermittent Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Camil Belhadjoudja, Mohamed Maghenem, Emmanuel Witrant, Christophe Prieur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study in this paper boundary stabilization, in the L2 sense, of the
one-dimensional Kuramoto-Sivashinsky equation subject to intermittent sensing.
We assume that we measure the state of this spatio-temporal equation on a given
spatial subdomain during certain intervals of time, while we measure the state
on the remaining spatial subdomain during the remaining intervals of time. As a
result, we assign a feedback law at the boundary of the spatial domain and
force to zero the value of the state at the junction of the two subdomains.
Throughout the study, the destabilizing coefficient is assumed to be
space-dependent and bounded but unknown. Adaptive boundary controllers are
designed under different assumptions on the forcing term. In particular, when
the forcing term is null, we guarantee global exponential stability of the
origin. Furthermore, when the forcing term is bounded and admits a known upper
bound, we guarantee input-to-state stability, and only global uniform ultimate
boundedness is guaranteed when the upper bound is unknown. Numerical
simulations are performed to illustrate our results
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Automatica</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Piecewise Residuals of Control Barrier Functions for Safety of
  Switching Systems using Multi-Output Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Aali, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Control barrier functions (CBFs) have recently been introduced as a
systematic tool to ensure safety by establishing set invariance. When combined
with a control Lyapunov function (CLF), they form a safety-critical control
mechanism. However, the effectiveness of CBFs and CLFs is closely tied to the
system model. In practice, model uncertainty can jeopardize safety and
stability guarantees and may lead to undesirable performance. In this paper, we
develop a safe learning-based control strategy for switching systems in the
face of uncertainty. We focus on the case that a nominal model is available for
a true underlying switching system. This uncertainty results in piecewise
residuals for each switching surface, impacting the CLF and CBF constraints. We
introduce a batch multi-output Gaussian process (MOGP) framework to approximate
these piecewise residuals, thereby mitigating the adverse effects of
uncertainty. A particular structure of the covariance function enables us to
convert the MOGP-based chance constraints CLF and CBF into second-order cone
constraints, which leads to a convex optimization. We analyze the feasibility
of the resulting optimization and provide the necessary and sufficient
conditions for feasibility. The effectiveness of the proposed strategy is
validated through a simulation of a switching adaptive cruise control system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2403.09573</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Constructive Method for Designing Safe Multirate Controllers for
  Differentially-Flat Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devansh R. Agrawal, Hardik Parwana, Ryan K. Cosner, Ugo Rosolia, Aaron D. Ames, Dimitra Panagou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a multi-rate control architecture that leverages fundamental
properties of differential flatness to synthesize controllers for
safety-critical nonlinear dynamical systems. We propose a two-layer
architecture, where the high-level generates reference trajectories using a
linear Model Predictive Controller, and the low-level tracks this reference
using a feedback controller. The novelty lies in how we couple these layers, to
achieve formal guarantees on recursive feasibility of the MPC problem, and
safety of the nonlinear system. Furthermore, using differential flatness, we
provide a constructive means to synthesize the multi-rate controller, thereby
removing the need to search for suitable Lyapunov or barrier functions, or to
approximately linearize/discretize nonlinear dynamics. We show the synthesized
controller is a convex optimization problem, making it amenable to real-time
implementations. The method is demonstrated experimentally on a ground rover
and a quadruped robotic system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, accepted at IEEE Control Systems Letters 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Clarity-Aware Dynamic Coverage with Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devansh R. Agrawal, Dimitra Panagou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents two algorithms for multi-agent dynamic coverage in
spatiotemporal environments, where the coverage algorithms are informed by the
method of data assimilation. In particular, we show that by considering the
information assimilation algorithm, here a Numerical Gaussian Process Kalman
Filter, the influence of measurements taken at one position on the uncertainty
of the estimate at another location can be computed. We use this relationship
to propose new coverage algorithms. Furthermore, we show that the controllers
naturally extend to the multi-agent context, allowing for a distributed-control
central-information paradigm for multi-agent coverage. Finally, we demonstrate
the algorithms through a realistic simulation of a team of UAVs collecting wind
data over a region in Austria.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, submitted to CDC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Resilient Consensus under Intermittent Faulty and Malicious
  Transmissions (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarper Aydın, Orhan Eren Akgün, Stephanie Gil, Angelia Nedić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we consider the consensus problem in which legitimate agents
share their values over an undirected communication network in the presence of
malicious or faulty agents. Different from the previous works, we characterize
the conditions that generalize to several scenarios such as intermittent faulty
or malicious transmissions, based on trust observations. As the standard trust
aggregation approach based on a constant threshold fails to distinguish
intermittent malicious/faulty activity, we propose a new detection algorithm
utilizing time-varying thresholds and the random trust values available to
legitimate agents. Under these conditions, legitimate agents almost surely
determine their trusted neighborhood correctly with geometrically decaying
misclassification probabilities. We further prove that the consensus process
converges almost surely even in the presence of malicious agents. We also
derive the probabilistic bounds on the deviation from the nominal consensus
value that would have been achieved with no malicious agents in the system.
Numerical results verify the convergence among agents and exemplify the
deviation under different scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended Version of CDC '24 submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stealthy Deactivation of Safety Filters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Arnström, André M. H. Teixeira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety filters ensure that only safe control actions are executed. We propose
a simple and stealthy false-data injection attack for deactivating such safety
filters; in particular, we focus on deactivating safety filters that are based
on control-barrier functions. The attack injects false sensor measurements to
bias state estimates to the interior of a safety region, which makes the safety
filter accept unsafe control actions. To detect such attacks, we also propose a
detector that detects biases manufactured by the proposed attack policy, which
complements conventional detectors when safety filters are used. The proposed
attack policy and detector are illustrated on a double integrator example.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECC24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning the Optimal Power Flow: Environment Design Matters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Wolgast, Astrid Nieße
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To solve the optimal power flow (OPF) problem, reinforcement learning (RL)
emerges as a promising new approach. However, the RL-OPF literature is strongly
divided regarding the exact formulation of the OPF problem as an RL
environment. In this work, we collect and implement diverse environment design
decisions from the literature regarding training data, observation space,
episode definition, and reward function choice. In an experimental analysis, we
show the significant impact of these environment design options on RL-OPF
training performance. Further, we derive some first recommendations regarding
the choice of these design decisions. The created environment framework is
fully open-source and can serve as a benchmark for future research in the
RL-OPF field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering Feedback in Dynamic Driving Simulators: The Influence of
  Steering Wheel Vibration and Vehicle Motion Frequency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Böhle, Bernhard Schick, Steffen Müller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The validity of the subjective evaluation of steering feedback in driving
simulators is crucial for modern vehicle development. Although there are
established objective steering characteristics for the assessment of both
stationary and dynamic feedback behaviour, factors such as steering wheel
vibrations and vehicle body motion, particularly in high-frequency ranges,
present challenges in simulator fidelity. This work investigates the influence
of steering wheel vibration and vehicle body motion frequency content on the
subjective evaluation of steering feedback during closed-loop driving in a
dynamic driving simulator. A controlled subject study with 30 participants
consisting of a back-to-back comparison of a reference vehicle with an
electrical power steering system and three variants of its virtual
representation on a dynamic driving simulator was performed. Subjective
evaluation focused on the representation of road feedback in comparison to the
reference vehicle. The statistical analysis of subjective results show that
there is a significant influence of the frequency content of both steering
wheel torque and vehicle motion on the subjective evaluation of steering
feedback in a dynamic driving simulator. The results suggest an influence of
frequency content on the subjective evaluation quality of steering feedback
characteristics that are not associated with the dynamic feedback behaviour in
the context of established performance indicators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, 9 tables, submitted to the IEEE Transactions on
  Intelligent Vehicles</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Exponential Stabilization of Control-affine Nonlinear Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Zakwan, Liang Xu, Giancarlo Ferrari-Trecate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel learning-based approach for achieving exponential
stabilization of nonlinear control-affine systems. We leverage the Control
Contraction Metrics (CCMs) framework to co-synthesize Neural Contraction
Metrics (NCMs) and Neural Network (NN) controllers. First, we transform the
infinite-dimensional semi-definite program (SDP) for CCM computation into a
tractable inequality feasibility problem using element-wise bounds of
matrix-valued functions. The terms in the inequality can be efficiently
computed by our novel algorithms. Second, we propose a free parametrization of
NCMs guaranteeing positive definiteness and the satisfaction of a partial
differential equation, regardless of trainable parameters. Third, this
parametrization and the inequality condition enable the design of
contractivity-enforcing regularizers, which can be incorporated while designing
the NN controller for exponential stabilization of the underlying nonlinear
systems. Furthermore, when the training loss goes to zero, we provide formal
guarantees on verification of the NCM and the exponentional stabilization under
the NN controller. Finally, we validate our method through benchmark
experiments on set-point stabilization and increasing the region of attraction
of a locally pre-stabilized closed-loop system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is submitted in CDC2024 for a possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A PAC-Bayesian Framework for Optimal Control with Stability Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahrokh Ghoddousi Boroujeni, Clara Lucía Galimberti, Andreas Krause, Giancarlo Ferrari-Trecate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic Nonlinear Optimal Control (SNOC) involves minimizing a cost
function that averages out the random uncertainties affecting the dynamics of
nonlinear systems. For tractability reasons, this problem is typically
addressed by minimizing an empirical cost, which represents the average cost
across a finite dataset of sampled disturbances. However, this approach raises
the challenge of quantifying the control performance against out-of-sample
uncertainties. Particularly, in scenarios where the training dataset is small,
SNOC policies are prone to overfitting, resulting in significant discrepancies
between the empirical cost and the true cost, i.e., the average SNOC cost
incurred during control deployment. Therefore, establishing generalization
bounds on the true cost is crucial for ensuring reliability in real-world
applications. In this paper, we introduce a novel approach that leverages
PAC-Bayes theory to provide rigorous generalization bounds for SNOC. Based on
these bounds, we propose a new method for designing optimal controllers,
offering a principled way to incorporate prior knowledge into the synthesis
process, which aids in improving the control policy and mitigating overfitting.
Furthermore, by leveraging recent parametrizations of stabilizing controllers
for nonlinear systems, our framework inherently ensures closed-loop stability.
The effectiveness of our proposed method in incorporating prior knowledge and
combating overfitting is shown by designing neural network controllers for
tasks in cooperative robotics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Distributed Controllers with Port-Hamiltonian Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Zakwan, Giancarlo Ferrari-Trecate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controlling large-scale cyber-physical systems necessitates optimal
distributed policies, relying solely on local real-time data and limited
communication with neighboring agents. However, finding optimal controllers
remains challenging, even in seemingly simple scenarios. Parameterizing these
policies using Neural Networks (NNs) can deliver good performance, but their
sensitivity to small input changes can destabilize the closed-loop system. This
paper addresses this issue for a network of nonlinear dissipative systems.
Specifically, we leverage well-established port-Hamiltonian structures to
characterize deep distributed control policies with closed-loop stability
guarantees and a finite $\mathcal{L}_2$ gain, regardless of specific NN
parameters. This eliminates the need to constrain the parameters during
optimization and enables training with standard methods like stochastic
gradient descent. A numerical study on the consensus control of Kuramoto
oscillators demonstrates the effectiveness of the proposed controllers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is submitted in CDC2024 for a possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optical Flow Based Detection and Tracking of Moving Objects for
  Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        MReza Alipour Sormoli, Mehrdad Dianati, Sajjad Mozaffari, Roger woodman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate velocity estimation of surrounding moving objects and their
trajectories are critical elements of perception systems in
Automated/Autonomous Vehicles (AVs) with a direct impact on their safety. These
are non-trivial problems due to the diverse types and sizes of such objects and
their dynamic and random behaviour. Recent point cloud based solutions often
use Iterative Closest Point (ICP) techniques, which are known to have certain
limitations. For example, their computational costs are high due to their
iterative nature, and their estimation error often deteriorates as the relative
velocities of the target objects increase (>2 m/sec). Motivated by such
shortcomings, this paper first proposes a novel Detection and Tracking of
Moving Objects (DATMO) for AVs based on an optical flow technique, which is
proven to be computationally efficient and highly accurate for such problems.
\textcolor{black}{This is achieved by representing the driving scenario as a
vector field and applying vector calculus theories to ensure spatiotemporal
continuity.} We also report the results of a comprehensive performance
evaluation of the proposed DATMO technique, carried out in this study using
synthetic and real-world data. The results of this study demonstrate the
superiority of the proposed technique, compared to the DATMO techniques in the
literature, in terms of estimation accuracy and processing time in a wide range
of relative velocities of moving objects. Finally, we evaluate and discuss the
sensitivity of the estimation error of the proposed DATMO technique to various
system and environmental parameters, as well as the relative velocities of the
moving objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript has been accepted as a regular paper in Transactions
  on Intelligent Transportation Systems (DOI: 10.1109/TITS.2024.3382495)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Structural Non-commutativity in Affine Feedback of SISO Nonlinear
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Venkatesh G. S.
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The affine feedback connection of SISO nonlinear systems modeled by
Chen--Fliess series is shown to be a group action on the plant which is
isomorphic to the semi-direct product of shuffle and additive group of
non-commutative formal power series. The additive and multiplicative feedback
loops in an affine feedback connection are thus proven to be structurally
non-commutative. A flip in the order of these loops results in a net additive
feedback loop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to $26^{th}$ International Symposium on Mathematical Theory
  of Networks and Systems, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using quantum computers in control: interval matrix properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Schneider, Julian Berberich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum computing provides a powerful framework for tackling computational
problems that are classically intractable. The goal of this paper is to explore
the use of quantum computers for solving relevant problems in systems and
control theory. In the recent literature, different quantum algorithms have
been developed to tackle binary optimization, which plays an important role in
various control-theoretic problems. As a prototypical example, we consider the
verification of interval matrix properties such as non-singularity and
stability on a quantum computer. We present a quantum algorithm solving these
problems and we study its performance in simulation. Our results demonstrate
that quantum computers provide a promising tool for control whose applicability
to further computationally complex problems remains to be explored.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final version, accepted for publication in Proc. European Control
  Conference (ECC), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prioritize Team Actions: Multi-Agent Temporal Logic Task Planning with
  Ordering Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Ye, Jianing Zhao, Shaoyuan Li, Xiang Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the problem of linear temporal logic (LTL) path
planning for multi-agent systems, introducing the new concept of \emph{ordering
constraints}. Specifically, we consider a generic objective function that is
defined for the path of each individual agent. The primary objective is to find
a global plan for the team of agents, ensuring they collectively meet the
specified LTL requirements. Simultaneously, we aim to maintain a pre-determined
order in the values of the objective function for each agent, which we refer to
as the ordering constraints. This new requirement stems from scenarios like
security-aware planning, where relative orders outweigh absolute values in
importance. We present an efficient algorithm to solve this problem, supported
by proofs of correctness that demonstrate the optimality of our solution.
Additionally, we provide a case study in security-aware path planning to
illustrate the practicality and effectiveness of our proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chattering Phenomena in Time-Optimal Control for High-Order
  Chain-of-Integrators Systems with Full State Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunan Wang, Chuxiong Hu, Zeyang Li, Yujie Lin, Shize Lin, Suqin He, Yu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-optimal control for high-order chain-of-integrators systems with full
state constraints and arbitrary given terminal states remains an open and
challenging problem in optimal control theory domain. However, optimal
control's behaviors in high-order problems lack of precision characterization,
even where the existence of chattering phenomena remain unknown and overlooked.
This paper establishes a theoretical framework of chattering phenomena in the
problem, focusing on the uniqueness of state constraints inducing chattering,
the upper bound of switching times in an unconstrained arc during chattering,
and the convergence of states and costates to the chattering limit point. For
the first time, this paper proves the existence of chattering phenomena in the
problems. The chattering optimal control for 4th order problems with velocity
constraints is precisely solved, providing an approach to plan strictly
time-optimal snap-limited trajectories, while other cases of order $n\leq4$ are
proved to not allow chattering. The conclusions correct the longstanding
misconception in the industry regarding the time-optimality of S-shaped
trajectories with minimal switching times.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ultrafast Adaptive Primary Frequency Tuning and Secondary Frequency
  Identification for S/S WPT system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Liu, Wei Han, Guangyu Yan, Bowang Zhang, Chunlin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic resonance wireless power transfer (WPT) technology is increasingly
being adopted across diverse applications. However, its effectiveness can be
significantly compromised by parameter shifts within the resonance network,
owing to its high system quality factor. Such shifts are inherent and
challenging to mitigate during the manufacturing process. In response, this
article introduces a rapid frequency tuning approach. Leveraging
switch-controlled capacitors (SCC) to adjust the resonance network and the
primary side's operating frequency, alongside a current zero-crossing detection
(ZCD) circuit for voltage-current phase determination, this method circumvents
the need for intricate knowledge of WPT system parameters. Moreover, it
obviates the necessity for inter-side communication for real-time
identification of the secondary side resonance frequency. The swift response of
SCC and two-step perturb-and-observe algorithm mitigate output disturbances,
thereby expediting the frequency tuning process. Experimental validation on a
200W Series-Series compensated WPT (SS-WPT) system demonstrates that the
proposed method achieves frequency recognition accuracy within 0.7kHz in less
than 1ms, increasing system efficiency up to 9%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages,16 figures,to be published in IEEE Transactions on
  Industrial Electronics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aerial Robots Carrying Flexible Cables: Dynamic Shape Optimal Control
  via Spectral Method Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaolei Shen, Chiara Gabellieri, Antonio Franchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a model-based optimal boundary control design for an
aerial robotic system composed of a quadrotor carrying a flexible cable. The
whole system is modeled by partial differential equations (PDEs) combined with
boundary conditions described by ordinary differential equations (ODEs). The
proper orthogonal decomposition (POD) method is adopted to project the original
infinite-dimensional system on a subspace spanned by orthogonal basis
functions. Based on the reduced order model, nonlinear model predictive control
(NMPC) is implemented online to realize shape trajectory tracking of the
flexible cable in an optimal predictive fashion. The proposed reduced modeling
and optimal control paradigms are numerically verified against an accurate
high-dimensional FDM-based model in different scenarios and the controller's
superior performance is shown compared to an optimally tuned PID controller.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Stability for Multiagent Systems with Spatio-Temporally
  Correlated Packet Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Hespe, Adwait Datar, Herbert Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A problem with considering correlations in the analysis of multiagent system
with stochastic packet loss is that they induce dependencies between agents
that are otherwise decoupled, preventing the application of decomposition
methods required for efficient evaluation. To circumvent that issue, this paper
is proposing an approach based on analysing sets of networks with independent
communication links, only considering the correlations in an implicit fashion.
Combining ideas from the robust stabilization of Markov jump linear systems
with recently proposed techniques for analysing packet loss in multiagent
systems, we obtain a linear matrix inequality based stability condition which
is independent of the number of agents. The main result is that the set of
stabilized probability distributions has non-empty interior such that small
correlations cannot lead to instability, even though only distributions of
independent links were analysed. Moreover, two examples are provided to
demonstrate the applicability of the results to practically relevant scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cyclic pursuit formation control for arbitrary desired shapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Fujioka, Masaki Ogura, Naoki Wakamiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A multi-agent system comprises numerous agents that autonomously make
decisions to collectively accomplish tasks, drawing significant attention for
their wide-ranging applications. Within this context, formation control emerges
as a prominent task, wherein agents collaboratively shape and maneuver while
preserving formation integrity. Our focus centers on cyclic pursuit, a method
facilitating the formation of circles, ellipses, and figure-eights under the
assumption that agents can only perceive the relative positions of those
preceding them. However, this method's scope has been restricted to these
specific shapes, leaving the feasibility of forming other shapes uncertain. In
response, our study proposes a novel method based on cyclic pursuit capable of
forming a broader array of shapes, enabling agents to individually shape while
pursuing preceding agents, thereby extending the repertoire of achievable
formations. We present two scenarios concerning the information available to
agents and devise formation control methods tailored to each scenario. Through
extensive simulations, we demonstrate the efficacy of our proposed method in
forming multiple shapes, including those represented as Fourier series, thereby
underscoring the versatility and effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reverse Kron reduction of Multi-phase Radial Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven H. Low
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of identifying the admittance matrix of a three-phase
radial network from voltage and current measurements at a subset of nodes.
These measurements are used to estimate a virtual network represented by the
Kron reduction (Schur complement) of the full admittance matrix. We focus on
recovering exactly the full admittance matrix from its Kron reduction, i.e.,
computing the inverse of Schur complement. The key idea is to decompose Kron
reduction into a sequence of iterations that maintains an invariance structure,
and exploit this structure to reverse each step of the iterative Kron
reduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Moreau Envelope Approach for LQR Meta-Policy Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwin Aravind, Mohammad Taha Toghani, César A. Uribe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of policy estimation for the Linear Quadratic Regulator
(LQR) in discrete-time linear time-invariant uncertain dynamical systems. We
propose a Moreau Envelope-based surrogate LQR cost, built from a finite set of
realizations of the uncertain system, to define a meta-policy efficiently
adjustable to new realizations. Moreover, we design an algorithm to find an
approximate first-order stationary point of the meta-LQR cost function.
Numerical results show that the proposed approach outperforms naive averaging
of controllers on new realizations of the linear system. We also provide
empirical evidence that our method has better sample complexity than
Model-Agnostic Meta-Learning (MAML) approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning-based Receding Horizon Control using Adaptive
  Control Barrier Functions for Safety-Critical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Sabouni, H. M. Sabbir Ahmad, Vittorio Giammarino, Christos G. Cassandras, Ioannis Ch. Paschalidis, Wenchao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal control methods provide solutions to safety-critical problems but
easily become intractable. Control Barrier Functions (CBFs) have emerged as a
popular technique that facilitates their solution by provably guaranteeing
safety, through their forward invariance property, at the expense of some
performance loss. This approach involves defining a performance objective
alongside CBF-based safety constraints that must always be enforced.
Unfortunately, both performance and solution feasibility can be significantly
impacted by two key factors: (i) the selection of the cost function and
associated parameters, and (ii) the calibration of parameters within the
CBF-based constraints, which capture the trade-off between performance and
conservativeness. %as well as infeasibility. To address these challenges, we
propose a Reinforcement Learning (RL)-based Receding Horizon Control (RHC)
approach leveraging Model Predictive Control (MPC) with CBFs (MPC-CBF). In
particular, we parameterize our controller and use bilevel optimization, where
RL is used to learn the optimal parameters while MPC computes the optimal
control input. We validate our method by applying it to the challenging
automated merging control problem for Connected and Automated Vehicles (CAVs)
at conflicting roadways. Results demonstrate improved performance and a
significant reduction in the number of infeasible cases compared to traditional
heuristic approaches used for tuning CBF-based controllers, showcasing the
effectiveness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Destination-Constrained Linear Dynamical System Modeling in Set-Valued
  Frameworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaowei Yang, Haiqi Liu, Fanqin Meng, Xiaojing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Directional motion towards a specified destination is a common occurrence in
physical processes and human societal activities. Utilizing this prior
information can significantly improve the control and predictive performance of
system models. This paper primarily focuses on reconstructing linear dynamic
system models based on destination constraints in the set-valued framework. We
treat destination constraints as inherent information in the state evolution
process and employ convex optimization techniques to construct a coherent and
robust state model. This refined model effectively captures the impact of
destination constraints on the state evolution at each time step. Furthermore,
we design an optimal weight matrix for the reconstructed model to ensure
smoother and more natural trajectories of state evolution. We also analyze the
theoretical guarantee of optimality for this weight matrix and the properties
of the reconstructed model. Finally, simulation experiments verify that the
reconstructed model has significant advantages over the unconstrained and
unoptimized weighted models and constrains the evolution of state trajectories
with different starting and ending points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional Inductive Invariant Based Verification of Neural Network
  Controlled Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Zhou, Stavros Tripakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of neural networks into safety-critical systems has shown
great potential in recent years. However, the challenge of effectively
verifying the safety of Neural Network Controlled Systems (NNCS) persists. This
paper introduces a novel approach to NNCS safety verification, leveraging the
inductive invariant method. Verifying the inductiveness of a candidate
inductive invariant in the context of NNCS is hard because of the scale and
nonlinearity of neural networks. Our compositional method makes this
verification process manageable by decomposing the inductiveness proof
obligation into smaller, more tractable subproblems. Alongside the high-level
method, we present an algorithm capable of automatically verifying the
inductiveness of given candidates by automatically inferring the necessary
decomposition predicates. The algorithm significantly outperforms the baseline
method and shows remarkable reductions in execution time in our case studies,
shortening the verification time from hours (or timeout) to seconds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Control of Grid-Interfacing Inverters With Current Magnitude
  Limits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00473v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00473v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trager Joswig-Jones, Baosen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grid-interfacing inverters act as the interface between renewable resources
and the electric grid, and have the potential to offer fast and programmable
controls compared to synchronous generators. With this flexibility there has
been significant research efforts into determining the best way to control
these inverters. Inverters are limited in their maximum current output in order
to protect semiconductor devices, presenting a nonlinear constraint that needs
to be accounted for in their control algorithms. Existing approaches either
simply saturate a controller that is designed for unconstrained systems, or
assume small perturbations and linearize a saturated system. These approaches
can lead to stability issues or limiting the control actions to be too
conservative.
  In this paper, we directly focus on a nonlinear system that explicitly
accounts for the saturation of the current magnitude. We use a Lyapunov
stability approach to determine a stability condition for the system,
guaranteeing that a class of controllers would be stabilizing if they satisfy a
simple SDP condition. With this condition we fit a linear-feedback controller
by sampling the output (offline) model predictive control problems. This
learned controller has improved performances with existing designs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, 1 table. Submitted to CDC'2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster Run-to-Run Feedforward Control of Electromechanical Switching
  Devices: a Sensitivity-Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03300v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03300v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edgar Ramirez-Laboreo, Eduardo Moya-Lasheras, Eloy Serrano-Seco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electromechanical switching devices, such as solenoid valves, contactors, and
relays, suffer from undesirable phenomena like clicking, mechanical wear, and
contact bounce. Despite that, they are still widely used in industry due to
their various economic and technical advantages. This has encouraged the
development of controllers aimed at reducing the collisions that occur at the
end of the switching operations. One of the most successful approaches has been
the use of iterative techniques. However, these algorithms typically require a
large number of operations to converge, which is definitely a clear drawback.
This paper presents a strategy to improve the convergence rate of such
controllers. Our proposal, which is based on the sensitivity of the control law
with respect to the parameters, assumes that the performance of the system is
more heavily affected by some parameters than others. Thus, by avoiding
movements in the directions that have less impact, the search algorithm is
expected to drive the system to near-optimal behaviors using fewer operations.
Results obtained by simulation show significant improvement in the convergence
rate of a state-of-the-art run-to-run feedforward controller, which
demonstrates the high potential of the proposal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures. Final version, after peer review and acceptance,
  submitted to the 22nd European Control Conference (ECC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Resilient source seeking with robot swarms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Acuaviva, Jesus Bautista, Weijia Yao, Juan Jimenez, Hector Garcia de Marina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a solution for locating the source, or maximum, of an unknown
scalar field using a swarm of mobile robots. Unlike relying on the traditional
gradient information, the swarm determines an ascending direction to approach
the source with arbitrary precision. The ascending direction is calculated from
measurements of the field strength at the robot locations and their relative
positions concerning the centroid. Rather than focusing on individual robots,
we focus the analysis on the density of robots per unit area to guarantee a
more resilient swarm, i.e., the functionality remains even if individuals go
missing or are misplaced during the mission. We reinforce the robustness of the
algorithm by providing sufficient conditions for the swarm shape so that the
ascending direction is almost parallel to the gradient. The swarm can respond
to an unexpected environment by morphing its shape and exploiting the existence
of multiple ascending directions. Finally, we validate our approach numerically
with hundreds of robots. The fact that a large number of robots always
calculate an ascending direction compensates for the loss of individuals and
mitigates issues arising from the actuator and sensor noises.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, submitted to CDC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frequency Regulation with Storage: On Losses and Profits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02987v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02987v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dirk Lauinger, François Vuille, Daniel Kuhn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-carbon societies will need to store vast amounts of electricity to
balance intermittent generation from wind and solar energy, for example,
through frequency regulation. Here, we derive an analytical solution to the
decision-making problem of storage operators who sell frequency regulation
power to grid operators and trade electricity on day-ahead markets.
Mathematically, we treat future frequency deviation trajectories as functional
uncertainties in a receding horizon robust optimization problem. We constrain
the expected terminal state-of-charge to be equal to some target to allow
storage operators to make good decisions not only for the present but also the
future. Thanks to this constraint, the amount of electricity traded on
day-ahead markets is an implicit function of the regulation power sold to grid
operators. The implicit function quantifies the amount of power that needs to
be purchased to cover the expected energy loss that results from providing
frequency regulation. We show how the marginal cost associated with the
expected energy loss decreases with roundtrip efficiency and increases with
frequency deviation dispersion. We find that the profits from frequency
regulation over the lifetime of energy-constrained storage devices are roughly
inversely proportional to the length of time for which regulation power must be
committed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Safe Preference Learning Approach for Personalization with
  Applications to Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02099v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02099v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruya Karagulle, Nikos Arechiga, Andrew Best, Jonathan DeCastro, Necmiye Ozay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces a preference learning method that ensures adherence to
given specifications, with an application to autonomous vehicles. Our approach
incorporates the priority ordering of Signal Temporal Logic (STL) formulas
describing traffic rules into a learning framework. By leveraging Parametric
Weighted Signal Temporal Logic (PWSTL), we formulate the problem of
safety-guaranteed preference learning based on pairwise comparisons and propose
an approach to solve this learning problem. Our approach finds a feasible
valuation for the weights of the given PWSTL formula such that, with these
weights, preferred signals have weighted quantitative satisfaction measures
greater than their non-preferred counterparts. The feasible valuation of
weights given by our approach leads to a weighted STL formula that can be used
in correct-and-custom-by-construction controller synthesis. We demonstrate the
performance of our method with a pilot human subject study in two different
simulated driving scenarios involving a stop sign and a pedestrian crossing.
Our approach yields competitive results compared to existing preference
learning methods in terms of capturing preferences and notably outperforms them
when safety is considered.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, 2 tables. This work has been published at IEEE
  Robotics and Automation Letters. Copyright may be transferred without notice,
  after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Closing the Gap to Quadratic Invariance: a Regret Minimization Approach
  to Optimal Distributed Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Martinelli, Andrea Martin, Giancarlo Ferrari-Trecate, Luca Furieri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we focus on the design of optimal controllers that must comply
with an information structure. State-of-the-art approaches do so based on the
H2 or Hinfty norm to minimize the expected or worst-case cost in the presence
of stochastic or adversarial disturbances. Large-scale systems often experience
a combination of stochastic and deterministic disruptions (e.g., sensor
failures, environmental fluctuations) that spread across the system and are
difficult to model precisely, leading to sub-optimal closed-loop behaviors.
Hence, we propose improving performance for these scenarios by minimizing the
regret with respect to an ideal policy that complies with less stringent
sensor-information constraints. This endows our controller with the ability to
approach the improved behavior of a more informed policy, which would detect
and counteract heterogeneous and localized disturbances more promptly.
Specifically, we derive convex relaxations of the resulting regret minimization
problem that are compatible with any desired controller sparsity, while we
reveal a renewed role of the Quadratic Invariance (QI) condition in designing
informative benchmarks to measure regret. Last, we validate our proposed method
through numerical simulations on controlling a multi-agent distributed system,
comparing its performance with traditional H2 and Hinfty policies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation and publication in the proceedings of the
  2024 European Control Conference (ECC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-Optimal Control for High-Order Chain-of-Integrators Systems with
  Full State Constraints and Arbitrary Terminal States (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07039v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07039v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunan Wang, Chuxiong Hu, Zeyang Li, Shize Lin, Suqin He, Ze Wang, Yu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-optimal control for high-order chain-of-integrators systems with full
state constraints and arbitrary given terminal states remains a challenging
problem in the optimal control theory domain, yet to be resolved. To enhance
further comprehension of the problem, this paper establishes a novel notation
system and theoretical framework, providing the switching manifold for
high-order problems in the form of switching laws. Through deriving properties
of switching laws on signs and dimension, this paper proposes a definite
condition for time-optimal control. Guided by the developed theory, a
trajectory planning method named the manifold-intercept method (MIM) is
developed. The proposed MIM can plan time-optimal jerk-limited trajectories
with full state constraints, and can also plan near-optimal non-chattering
higher-order trajectories with negligible extra motion time compared to optimal
profiles. Numerical results indicate that the proposed MIM outperforms all
baselines in computational time, computational accuracy, and trajectory quality
by a large gap.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discretized Distributed Optimization over Dynamic Digraphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Doostmohammadian, Wei Jiang, Muwahida Liaquat, Alireza Aghasi, Houman Zarrabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a discrete-time model of continuous-time distributed optimization
over dynamic directed-graphs (digraphs) with applications to distributed
learning. Our optimization algorithm works over general strongly connected
dynamic networks under switching topologies, e.g., in mobile multi-agent
systems and volatile networks due to link failures. Compared to many existing
lines of work, there is no need for bi-stochastic weight designs on the links.
The existing literature mostly needs the link weights to be stochastic using
specific weight-design algorithms needed both at the initialization and at all
times when the topology of the network changes. This paper eliminates the need
for such algorithms and paves the way for distributed optimization over
time-varying digraphs. We derive the bound on the gradient-tracking step-size
and discrete time-step for convergence and prove dynamic stability using
arguments from consensus algorithms, matrix perturbation theory, and Lyapunov
theory. This work, particularly, is an improvement over existing
stochastic-weight undirected networks in case of link removal or packet drops.
This is because the existing literature may need to rerun time-consuming and
computationally complex algorithms for stochastic design, while the proposed
strategy works as long as the underlying network is weight-symmetric and
balanced. The proposed optimization framework finds applications to distributed
classification and learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Robotics Meets Wireless Communications: An Introductory Tutorial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.02021v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.02021v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Bonilla Licea, Mounir Ghogho, Martin Saska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of ground Mobile Robots (MRs) and Unmanned Aerial Vehicles
(UAVs) within the research community, industry, and society is growing fast.
Many of these agents are nowadays equipped with communication systems that are,
in some cases, essential to successfully achieve certain tasks. In this
context, we have begun to witness the development of a new interdisciplinary
research field at the intersection of robotics and communications. This
research field has been boosted by the intention of integrating UAVs within the
5G and 6G communication networks. This research will undoubtedly lead to many
important applications in the near future. Nevertheless, one of the main
obstacles to the development of this research area is that most researchers
address these problems by oversimplifying either the robotics or the
communications aspect. This impedes the ability of reaching the full potential
of this new interdisciplinary research area. In this tutorial, we present some
of the modelling tools necessary to address problems involving both robotics
and communication from an interdisciplinary perspective. As an illustrative
example of such problems, we focus in this tutorial on the issue of
communication-aware trajectory planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 192 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable Linear Subspace Identification: A Machine Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03197v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03197v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loris Di Natale, Muhammad Zakwan, Bratislav Svetozarevic, Philipp Heer, Giancarlo Ferrari-Trecate, Colin N. Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) and linear System Identification (SI) have been
historically developed independently. In this paper, we leverage
well-established ML tools - especially the automatic differentiation framework
- to introduce SIMBa, a family of discrete linear multi-step-ahead state-space
SI methods using backpropagation. SIMBa relies on a novel
Linear-Matrix-Inequality-based free parametrization of Schur matrices to ensure
the stability of the identified model.
  We show how SIMBa generally outperforms traditional linear state-space SI
methods, and sometimes significantly, although at the price of a higher
computational burden. This performance gap is particularly remarkable compared
to other SI methods with stability guarantees, where the gain is frequently
above 25% in our investigations, hinting at SIMBa's ability to simultaneously
achieve state-of-the-art fitting performance and enforce stability.
Interestingly, these observations hold for a wide variety of input-output
systems and on both simulated and real-world data, showcasing the flexibility
of the proposed approach. We postulate that this new SI paradigm presents a
great extension potential to identify structured nonlinear models from data,
and we hence open-source SIMBa on https://github.com/Cemempamoi/simba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modelling Irrational Behaviour of Residential End Users using
  Non-Stationary Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nam Trong Dinh, Sahand Karimi-Arpanahi, Rui Yuan, S. Ali Pourmousavi, Mingyu Guo, Jon A. R. Liisberg, Julian Lemos-Vinasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Demand response (DR) plays a critical role in ensuring efficient electricity
consumption and optimal use of network assets. Yet, existing DR models often
overlook a crucial element, the irrational behaviour of electricity end users.
In this work, we propose a price-responsive model that incorporates key aspects
of end-user irrationality, specifically loss aversion, time inconsistency, and
bounded rationality. To this end, we first develop a framework that uses
Multiple Seasonal-Trend decomposition using Loess (MSTL) and non-stationary
Gaussian processes to model the randomness in the electricity consumption by
residential consumers. The impact of this model is then evaluated through a
community battery storage (CBS) business model. Additionally, we apply a
chance-constrained optimisation model for CBS operation that deals with the
unpredictability of the end-user irrationality. Our simulations using
real-world data show that the proposed DR model provides a more realistic
estimate of end-user price-responsive behaviour when considering irrationality.
Compared to a deterministic model that cannot fully take into account the
irrational behaviour of end users, the chance-constrained CBS operation model
yields an additional 19% revenue. Lastly, the business model reduces the
electricity costs of solar end users by 11%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript has been accepted for publication in IEEE
  Transactions on Smart Grid</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Full Attitude Intelligent Controller Design of a Heliquad under Complete
  Failure of an Actuator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.07529v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.07529v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eeshan Kulkarni, Suresh Sundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we design a reliable Heliquad and develop an intelligent
controller to handle one actuators complete failure. Heliquad is a multi-copter
similar to Quadcopter, with four actuators diagonally symmetric from the
center. Each actuator has two control inputs; the first input changes the
propeller blades collective pitch (also called variable pitch), and the other
input changes the rotation speed. For reliable operation and high torque
characteristic requirement for yaw control, a cambered airfoil is used to
design propeller blades. A neural network-based control allocation is designed
to provide complete control authority even under a complete loss of one
actuator. Nonlinear quaternion based outer loop position control, with
proportional-derivative inner loop for attitude control and neural
network-based control allocation is used in controller design. The proposed
controller and Heliquad designs performance is evaluated using a
software-in-loop simulation to track the position reference command under
failure. The results clearly indicate that the Heliquad with an intelligent
controller provides necessary tracking performance even under a complete loss
of one actuator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, For video go to
  https://indianinstituteofscience-my.sharepoint.com/:v:/g/personal/eeshank_iisc_ac_in/EcMg2uTtE91AsHDejNkb6YMBNckaXGjeh_YMzDV6sAHZAQ?e=DrRqmN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autonomous Hook-Based Grasping and Transportation with Quadcopters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Péter Antal, Tamás Péni, Roland Tóth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Payload grasping and transportation with quadcopters is an active research
area that has rapidly developed over the last decade. To grasp a payload
without human interaction, most state-of-the-art approaches apply robotic arms
that are attached to the quadcopter body. However, due to the large weight and
power consumption of these aerial manipulators, their agility and flight time
are limited. This paper proposes a motion control and planning method for
transportation with a lightweight, passive manipulator structure that consists
of a hook attached to a quadrotor using a 1 DoF revolute joint. To perform
payload grasping, transportation, and release, first, time-optimal reference
trajectories are designed through specific waypoints to ensure the fast and
reliable execution of the tasks. Then, a two-stage motion control approach is
developed based on a robust geometric controller for precise and reliable
reference tracking and a linear--quadratic payload regulator for rapid setpoint
stabilization of the payload swing. Furthermore, stability of the closed-loop
system is mathematically proven to give safety guarantee for its operation. The
proposed control architecture and design are evaluated in a high-fidelity
physical simulator, and also in real flight experiments, using a custom-made
quadrotor--hook manipulator platform.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustness Evaluation of Localization Techniques for Autonomous Racing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07658v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07658v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Yi Lim, Edoardo Ghignone, Nicolas Baumann, Michele Magno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces SynPF, an MCL-based algorithm tailored for high-speed
racing environments. Benchmarked against Cartographer, a state-of-the-art
pose-graph SLAM algorithm, SynPF leverages synergies from previous
particle-filtering methods and synthesizes them for the high-performance racing
domain. Our extensive in-field evaluations reveal that while Cartographer
excels under nominal conditions, it struggles when subjected to wheel-slip, a
common phenomenon in a racing scenario due to varying grip levels and
aggressive driving behaviour. Conversely, SynPF demonstrates robustness in
these challenging conditions and a low-latency computation time of 1.25 ms on
on-board computers without a GPU. Using the F1TENTH platform, a 1:10 scaled
autonomous racing vehicle, this work not only highlights the vulnerabilities of
existing algorithms in high-speed scenarios, tested up until 7.6 m/s, but also
emphasizes the potential of SynPF as a viable alternative, especially in
deteriorating odometry conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Design, Automation and Test in Europe Conference 2024
  as an extended abstract</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Credit vs. Discount-Based Congestion Pricing: A Comparison Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13923v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13923v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Yuan Chiu, Devansh Jalota, Marco Pavone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tolling, or congestion pricing, offers a promising traffic management policy
for regulating congestion, but has also attracted criticism for placing
outsized financial burdens on low-income users. Credit-based congestion pricing
(CBCP) and discount-based congestion pricing (DBCP) policies, which
respectively provide travel credits and toll discounts to low-income users on
tolled roads, have emerged as promising mechanisms for reducing traffic
congestion without worsening societal inequities. However, the optimal design
of CBCP and DBCP policies, as well as their relative advantages and
disadvantages, remain poorly understood. To address this, we study the effects
of implementing CBCP and DBCP policies to route users on a network of
multi-lane highways with tolled express lanes. We formulate a non-atomic
routing game framework in which a subset of eligible users is granted toll
relief in the form of a fixed budget or toll discount, while the remaining
ineligible users must pay out-of-pocket. We prove the existence of Nash
equilibrium traffic flow patterns corresponding to any given CBCP or DBCP
policy. Under the additional assumption that eligible users have time-invariant
VoTs, we provide a convex program to efficiently compute these equilibria. For
networks consisting of a single edge, we identify conditions under which CBCP
policies outperform DBCP policies (and vice versa), in the sense of improving
eligible users' access to the express lane. Finally, we present empirical
results from a CBCP pilot study of the San Mateo 101 Express Lane Project in
California. Our empirical results corroborate our theoretical analysis of the
impact of deploying credit-based and discount-based policies, and lend insights
into the sensitivity of their impact with respect to the travel demand and
users' VoTs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Achievable Sum Rate Optimization on NOMA-aided Cell-Free Massive MIMO
  with Finite Blocklength Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baolin Chong, Hancheng Lu, Yuang Chen, Langtian Qin, Fengqian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-orthogonal multiple access (NOMA)-aided cell-free massive multiple-input
multiple-output (CFmMIMO) has been considered as a promising technology to
fulfill strict quality of service requirements for ultra-reliable low-latency
communications (URLLC). However, finite blocklength coding (FBC) in URLLC makes
it challenging to achieve the optimal performance in the NOMA-aided CFmMIMO
system. In this paper, we investigate the performance of the NOMA-aided CFmMIMO
system with FBC in terms of achievable sum rate (ASR). Firstly, we derive a
lower bound (LB) on the ergodic data rate. Then, we formulate an ASR
maximization problem by jointly considering power allocation and user equipment
(UE) clustering. To tackle such an intractable problem, we decompose it into
two sub-problems, i.e., the power allocation problem and the UE clustering
problem. A successive convex approximation (SCA) algorithm is proposed to solve
the power allocation problem by transforming it into a series of geometric
programming problems. Meanwhile, two algorithms based on graph theory are
proposed to solve the UE clustering problem by identifying negative loops.
Finally, alternative optimization is performed to find the maximum ASR of the
NOMA-aided CFmMIMO system with FBC. The simulation results demonstrate that the
proposed algorithms significantly outperform the benchmark algorithms in terms
of ASR under various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Logic in Computer Science
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proceedings Sixth Workshop on Models for Formal Analysis of Real Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frédéric Lang, Matthias Volk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This volume contains the proceedings of MARS 2024, the sixth workshop on
Models for Formal Analysis of Real Systems, held as part of ETAPS 2024, the
European Joint Conferences on Theory and Practice of Software.
  The MARS workshops bring together researchers from different communities who
are developing formal models of real systems in areas where complex models
occur, such as networks, cyber-physical systems, hardware/software co-design,
biology, etc. The motivation and aim for MARS stem from the following two
observations:
  (1) Large case studies are essential to show that specification formalisms
and modelling techniques are applicable to real systems, whereas many research
papers only consider toy examples or tiny case studies.
  (2) Developing an accurate model of a real system takes a large amount of
time, often months or years. In most scientific papers, however, salient
details of the model need to be skipped due to lack of space, and to leave room
for formal verification methodologies and results.
  The MARS workshops aim at remedying these issues, emphasising modelling over
verification, so as to retain lessons learnt from formal modelling, which are
not usually discussed elsewhere.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formal Verification of the Empty Hexagon Number 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernardo Subercaseaux, Wojciech Nawrocki, James Gallicchio, Cayden Codel, Mario Carneiro, Marijn J. H. Heule
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent breakthrough in computer-assisted mathematics showed that every set
of $30$ points in the plane in general position (i.e., without three on a
common line) contains an empty convex hexagon, thus closing a line of research
dating back to the 1930s. Through a combination of geometric insights and
automated reasoning techniques, Heule and Scheucher constructed a CNF formula
$\phi_n$, with $O(n^4)$ clauses, whose unsatisfiability implies that no set of
$n$ points in general position can avoid an empty convex hexagon. An
unsatisfiability proof for n = 30 was then found with a SAT solver using 17300
CPU hours of parallel computation, thus implying that the empty hexagon number
h(6) is equal to 30. In this paper, we formalize and verify this result in the
Lean theorem prover. Our formalization covers discrete computational geometry
ideas and SAT encoding techniques that have been successfully applied to
similar Erd\H{o}s-Szekeres-type problems. In particular, our framework provides
tools to connect standard mathematical objects to propositional assignments,
which represents a key step towards the formal verification of other SAT-based
mathematical results. Overall, we hope that this work sets a new standard for
verification when extensive computation is used for discrete geometry problems,
and that it increases the trust the mathematical community has in
computer-assisted proofs in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to ITP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uniform Preorders and Partial Combinatory Algebras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Frey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uniform preorders are a class of combinatory representations of Set-indexed
preorders that generalize Pieter Hofstra's basic relational objects. An indexed
preorder is representable by a uniform preorder if and only if it has as
generic predicate. We study the $\exists$-completion of indexed preorders on
the level of uniform preorders, and identify a combinatory condition (called
'relational completeness') which characterizes those uniform preorders with
finite meets whose $\exists$-completions are triposes. The class of triposes
obtained this way contains relative realizability triposes, for which we derive
a characterization as a fibrational analogue of the characterization of
realizability toposes given in earlier work.
  Besides relative partial combinatory algebras, the class of relationally
complete uniform preorders contains filtered ordered partial combinatory
algebras, and it is unclear if there are any others.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional Inductive Invariant Based Verification of Neural Network
  Controlled Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Zhou, Stavros Tripakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of neural networks into safety-critical systems has shown
great potential in recent years. However, the challenge of effectively
verifying the safety of Neural Network Controlled Systems (NNCS) persists. This
paper introduces a novel approach to NNCS safety verification, leveraging the
inductive invariant method. Verifying the inductiveness of a candidate
inductive invariant in the context of NNCS is hard because of the scale and
nonlinearity of neural networks. Our compositional method makes this
verification process manageable by decomposing the inductiveness proof
obligation into smaller, more tractable subproblems. Alongside the high-level
method, we present an algorithm capable of automatically verifying the
inductiveness of given candidates by automatically inferring the necessary
decomposition predicates. The algorithm significantly outperforms the baseline
method and shows remarkable reductions in execution time in our case studies,
shortening the verification time from hours (or timeout) to seconds.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-25T00:00:00Z">2024-03-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Signal Processing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 200Gb/s VCSEL transmission using 60m OM4 MMF and KP4 FEC for AI
  computing clusters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Wettlin, Youxi Lin, Nebojsa Stojanovic, Stefano Calabrò, Ruoxu Wang, Lewei Zhang, Maxim Kuschnerov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show a beyond 200Gb/s VCSEL transmission experiment. Results are based on
35GHz VCSEL and advanced DSP. We show an AIR of 245Gb/s PAM-6 back-to-back, and
200Gb/s PAM-4 over 60m OM4 fiber assuming KP4-FEC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cache-Enabled Millimetre-Wave Fluid Antenna Systems: Modeling and
  Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farshad Rostami Ghadi, Kai-Kit Wong, Kin-Fai Tong, Yangyang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This letter investigates the performance of content caching in a
heterogeneous cellular network (HetNet) consisting of fluid antenna system
(FAS)-equipped mobile users (MUs) and millimeter-wave (mm-wave) single-antenna
small base stations (SBSs), distributed according to the independent
homogeneous Poisson point processes (HPPP). In particular, it is assumed that
the most popular contents are cached in the SBSs to serve the FAS-equipped MUs
requests. To assess the system performance, we derive compact expressions for
the successful content delivery probability (SCDP) and the content delivery
delay (CDD) using the Gauss-Laguerre quadrature technique. Our numerical
results show that the performance of cache-enabled mm-wave HetNets can be
greatly improved, when the FAS is utilized at the MUs instead of traditional
fixed-antenna system deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latency-Aware Generative Semantic Communications with <span class="highlight-title">Pre-Train</span>ed
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Qiao, Mahdi Boloursaz Mashhadi, Zhen Gao, Chuan Heng Foh, Pei Xiao, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative foundation AI models have recently shown great success in
synthesizing natural signals with high perceptual quality using only textual
prompts and conditioning signals to guide the generation process. This enables
semantic communications at extremely low data rates in future wireless
networks. In this paper, we develop a latency-aware semantic communications
framework with pre-trained generative models. The transmitter performs
multi-modal semantic decomposition on the input signal and transmits each
semantic stream with the appropriate coding and communication schemes based on
the intent. For the prompt, we adopt a re-transmission-based scheme to ensure
reliable transmission, and for the other semantic modalities we use an adaptive
modulation/coding scheme to achieve robustness to the changing wireless
channel. Furthermore, we design a semantic and latency-aware scheme to allocate
transmission power to different semantic modalities based on their importance
subjected to semantic quality constraints. At the receiver, a pre-trained
generative model synthesizes a high fidelity signal using the received
multi-stream semantics. Simulation results demonstrate ultra-low-rate,
low-latency, and channel-adaptive semantic communications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-compliant diagonal representation of beyond-diagonal RIS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp del Hougne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-compliant models of RIS-parametrized channels assign a
load-terminated port to each RIS element. For conventional diagonal RIS
(D-RIS), each auxiliary port is terminated by its own independent and
individually tunable load (i.e., independent of the other auxiliary ports). For
beyond-diagonal RIS (BD-RIS), the auxiliary ports are terminated by a tunable
load circuit which couples the auxiliary ports to each other. Here, we point
out that a physics-compliant model of the load circuit of a BD-RIS takes the
same form as a physics-compliant model of a D-RIS-parametrized radio
environment: a multi-port network with a subset of ports terminated by
individually tunable loads (independent of each other). Consequently, we
recognize that a BD-RIS-parametrized radio environment can be understood as a
multi-port cascade network (i.e., the cascade of radio environment with load
circuit) terminated by individually tunable loads (independent of each other).
Hence, the BD-RIS problem can be mapped into the original D-RIS problem by
replacing the radio environment with the cascade of radio environment and load
circuit. The insight that BD-RIS can be physics-compliantly analyzed with the
conventional D-RIS formalism implies that (i) the same optimization protocols
as for D-RIS can be used for the BD-RIS case, and (ii) it is unclear if
existing comparisons between BD-RIS and D-RIS are fair because for a fixed
number of RIS elements, a BD-RIS has usually more tunable lumped elements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, submitted to an IEEE Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Intersection of Signal Processing and Machine Learning: A Use
  Case-Driven Analysis Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sulaiman Aburakhia, Abdallah Shami, George K. Karagiannidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in sensing, measurement, and computing technologies have
significantly expanded the potential for signal-based applications, leveraging
the synergy between signal processing and Machine Learning (ML) to improve both
performance and reliability. This fusion represents a critical point in the
evolution of signal-based systems, highlighting the need to bridge the existing
knowledge gap between these two interdisciplinary fields. Despite many attempts
in the existing literature to bridge this gap, most are limited to specific
applications and focus mainly on feature extraction, often assuming extensive
prior knowledge in signal processing. This assumption creates a significant
obstacle for a wide range of readers. To address these challenges, this paper
takes an integrated article approach. It begins with a detailed tutorial on the
fundamentals of signal processing, providing the reader with the necessary
background knowledge. Following this, it explores the key stages of a standard
signal processing-based ML pipeline, offering an in-depth review of feature
extraction techniques, their inherent challenges, and solutions. Differing from
existing literature, this work offers an application-independent review and
introduces a novel classification taxonomy for feature extraction techniques.
Furthermore, it aims at linking theoretical concepts with practical
applications, and demonstrates this through two specific use cases: a
spectral-based method for condition monitoring of rolling bearings and a
wavelet energy analysis for epilepsy detection using EEG signals. In addition
to theoretical contributions, this work promotes a collaborative research
culture by providing a public repository of relevant Python and MATLAB signal
processing codes. This effort is intended to support collaborative research
efforts and ensure the reproducibility of the results presented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 6D Movable Antenna Enhanced Wireless Network Via Discrete Position and
  Rotation Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodan Shao, Rui Zhang, Qijun Jiang, Robert Schober
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Six-dimensional movable antenna (6DMA) is an effective approach to improve
wireless network capacity by adjusting the 3D positions and 3D rotations of
distributed antenna surfaces based on the users' spatial distribution and
statistical channel information. Although continuously positioning/rotating
6DMA surfaces can achieve the greatest flexibility and thus the highest
capacity improvement, it is difficult to implement due to the discrete movement
constraints of practical stepper motors. Thus, in this paper, we consider a
6DMA-aided base station (BS) with only a finite number of possible discrete
positions and rotations for the 6DMA surfaces. We aim to maximize the average
network capacity for random numbers of users at random locations by jointly
optimizing the 3D positions and 3D rotations of multiple 6DMA surfaces at the
BS subject to discrete movement constraints. In particular, we consider the
practical cases with and without statistical channel knowledge of the users,
and propose corresponding offline and online optimization algorithms, by
leveraging the Monte Carlo and conditional sample mean (CSM) methods,
respectively. Simulation results verify the effectiveness of our proposed
offline and online algorithms for discrete position/rotation optimization of
6DMA surfaces as compared to various benchmark schemes with fixed-position
antennas (FPAs) and 6DMAs with limited movability. It is shown that 6DMA-BS can
significantly enhance wireless network capacity, even under discrete
position/rotation constraints, by exploiting the spatial distribution
characteristics of the users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, double column</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient generation of realistic guided wave signals for reliability
  estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panpan Xu, Robin Jones, Georgios Sarris, Peter Huthwaite
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Across non-destructive testing (NDT) and structural health monitoring (SHM),
accurate knowledge of the systems' reliability for detecting defects, such as
Probability of Detection (POD) analysis is essential to enabling widespread
adoption. Traditionally this relies on access to extensive experimental data to
cover all critical areas of the parametric space, which becomes expensive, and
heavily undermines the benefit such systems bring. In response to these
challenges, reliability estimation based on numerical simulation emerges as a
practical solution, offering enhanced efficiency and cost-effectiveness.
Nevertheless, precise reliability estimation demands that the simulated data
faithfully represents the real-world performance. In this context, a numerical
framework tailored to generate realistic signals for reliability estimation
purposes is presented here, focusing on the application of guided wave SHM for
pipe monitoring. It specifically incorporates key characteristics of real
signals: random noise and coherent noise caused by the imbalance in transducer
performance within guided wave monitoring systems. The effectiveness of our
proposed methodology is demonstrated through a comprehensive comparative
analysis between simulation-generated signals and experimental signals both
individually and statistically. Furthermore, to assess the reliability of a
guided wave system in terms of the inspection range for pipe monitoring, a
series of POD analyses using simulation-generated data were conducted. The
comparison of POD curves derived from ideal and realistic simulation data
underscores the necessity of considering coherent noise for accurate POD curve
calculations. Moreover, the POD analysis based on realistic
simulation-generated data provides a quantitative estimation of the inspection
range with more details compared to the current industry practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing UAV Security Through Zero Trust Architecture: An Advanced Deep
  Learning and Explainable AI Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekramul Haque, Kamrul Hasan, Imtiaz Ahmed, Md. Sahabul Alam, Tariqul Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the dynamic and ever-changing domain of Unmanned Aerial Vehicles (UAVs),
the utmost importance lies in guaranteeing resilient and lucid security
measures. This study highlights the necessity of implementing a Zero Trust
Architecture (ZTA) to enhance the security of unmanned aerial vehicles (UAVs),
hence departing from conventional perimeter defences that may expose
vulnerabilities. The Zero Trust Architecture (ZTA) paradigm requires a rigorous
and continuous process of authenticating all network entities and
communications. The accuracy of our methodology in detecting and identifying
unmanned aerial vehicles (UAVs) is 84.59\%. This is achieved by utilizing Radio
Frequency (RF) signals within a Deep Learning framework, a unique method.
Precise identification is crucial in Zero Trust Architecture (ZTA), as it
determines network access. In addition, the use of eXplainable Artificial
Intelligence (XAI) tools such as SHapley Additive exPlanations (SHAP) and Local
Interpretable Model-agnostic Explanations (LIME) contributes to the improvement
of the model's transparency and interpretability. Adherence to Zero Trust
Architecture (ZTA) standards guarantees that the classifications of unmanned
aerial vehicles (UAVs) are verifiable and comprehensible, enhancing security
within the UAV field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Backpropagation through space, time, and the brain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Ellenberger, Paul Haider, Jakob Jordan, Kevin Max, Ismael Jaras, Laura Kriener, Federico Benitez, Mihai A. Petrovici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective learning in neuronal networks requires the adaptation of individual
synapses given their relative contribution to solving a task. However, physical
neuronal systems -- whether biological or artificial -- are constrained by
spatio-temporal locality. How such networks can perform efficient credit
assignment, remains, to a large extent, an open question. In Machine Learning,
the answer is almost universally given by the error backpropagation algorithm,
through both space (BP) and time (BPTT). However, BP(TT) is well-known to rely
on biologically implausible assumptions, in particular with respect to
spatiotemporal (non-)locality, while forward-propagation models such as
real-time recurrent learning (RTRL) suffer from prohibitive memory constraints.
We introduce Generalized Latent Equilibrium (GLE), a computational framework
for fully local spatio-temporal credit assignment in physical, dynamical
networks of neurons. We start by defining an energy based on neuron-local
mismatches, from which we derive both neuronal dynamics via stationarity and
parameter dynamics via gradient descent. The resulting dynamics can be
interpreted as a real-time, biologically plausible approximation of BPTT in
deep cortical networks with continuous-time neuronal dynamics and continuously
active, local synaptic plasticity. In particular, GLE exploits the ability of
biological neurons to phase-shift their output rate with respect to their
membrane potential, which is essential in both directions of information
propagation. For the forward computation, it enables the mapping of
time-continuous inputs to neuronal space, performing an effective
spatiotemporal convolution. For the backward computation, it permits the
temporal inversion of feedback signals, which consequently approximate the
adjoint states necessary for useful parameter updates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably Robust Score-Based Diffusion Posterior Sampling for
  Plug-and-Play Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Xu, Yuejie Chi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a great number of tasks in science and engineering, the goal is to infer
an unknown image from a small number of measurements collected from a known
forward model describing certain sensing or imaging modality. Due to resource
constraints, this task is often extremely ill-posed, which necessitates the
adoption of expressive prior information to regularize the solution space.
Score-based diffusion models, due to its impressive empirical success, have
emerged as an appealing candidate of an expressive prior in image
reconstruction. In order to accommodate diverse tasks at once, it is of great
interest to develop efficient, consistent and robust algorithms that
incorporate {\em unconditional} score functions of an image prior distribution
in conjunction with flexible choices of forward models.
  This work develops an algorithmic framework for employing score-based
diffusion models as an expressive data prior in general nonlinear inverse
problems. Motivated by the plug-and-play framework in the imaging community, we
introduce a diffusion plug-and-play method (\textsf{DPnP}) that alternatively
calls two samplers, a proximal consistency sampler based solely on the
likelihood function of the forward model, and a denoising diffusion sampler
based solely on the score functions of the image prior. The key insight is that
denoising under white Gaussian noise can be solved {\em rigorously} via both
stochastic (i.e., DDPM-type) and deterministic (i.e., DDIM-type) samplers using
the unconditional score functions. We establish both asymptotic and
non-asymptotic performance guarantees of \textsf{DPnP}, and provide numerical
experiments to illustrate its promise in solving both linear and nonlinear
image reconstruction tasks. To the best of our knowledge, \textsf{DPnP} is the
first provably-robust posterior sampling method for nonlinear inverse problems
using unconditional diffusion priors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Movable-Antenna Position Optimization: A Graph-based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weidong Mei, Xin Wei, Boyu Ning, Zhi Chen, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fluid antennas (FAs) and movable antennas (MAs) have emerged as promising
technologies in wireless communications, which offer the flexibility to improve
channel conditions by adjusting transmit/receive antenna positions within a
spatial region. In this letter, we focus on an MA-enhanced multiple-input
single-output (MISO) communication system, aiming to optimize the positions of
multiple transmit MAs to maximize the received signal power. Unlike the prior
works on continuously searching for the optimal MA positions, we propose to
sample the transmit region into discrete points, such that the continuous
antenna position optimization problem is transformed to a discrete sampling
point selection problem based on the point-wise channel information. However,
such a point selection problem is combinatory and challenging to be optimally
solved. To tackle this challenge, we ingeniously recast it as an equivalent
fixed-hop shortest path problem in graph theory and propose a customized
algorithm to solve it optimally in polynomial time. To further reduce the
complexity, a linear-time sequential update algorithm is also proposed to
obtain a high-quality suboptimal solution. Numerical results demonstrate that
the proposed algorithms can yield considerable performance gains over the
conventional fixed-position antennas with/without antenna selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures. We propose a graph-based algorithm that is able
  to optimally solve the fluid-/movable-antenna position optimization problem
  in polynomial time</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proprioception Is All You Need: Terrain Classification for Boreal
  Forests <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damien LaRocque, William Guimont-Martin, David-Alexandre Duclos, Philippe Giguère, François Pomerleau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works in field robotics highlighted the importance of resiliency
against different types of terrains. Boreal forests, in particular, are home to
many mobility-impeding terrains that should be considered for off-road
autonomous navigation. Also, being one of the largest land biomes on Earth,
boreal forests are an area where autonomous vehicles are expected to become
increasingly common. In this paper, we address this issue by introducing
BorealTC, a publicly available dataset for proprioceptive-based terrain
classification (TC). Recorded with a Husky A200, our dataset contains 116 min
of Inertial Measurement Unit (IMU), motor current, and wheel odometry data,
focusing on typical boreal forest terrains, notably snow, ice, and silty loam.
Combining our dataset with another dataset from the state-of-the-art, we
evaluate both a Convolutional Neural Network (CNN) and the novel state space
model (SSM)-based Mamba architecture on a TC task. Interestingly, we show that
while CNN outperforms Mamba on each separate dataset, Mamba achieves greater
accuracy when trained on a combination of both. In addition, we demonstrate
that Mamba's learning capacity is greater than a CNN for increasing amounts of
data. We show that the combination of two TC datasets yields a latent space
that can be interpreted with the properties of the terrains. We also discuss
the implications of merging datasets on classification. Our source code and
dataset are publicly available online:
https://github.com/norlab-ulaval/BorealTC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 2024 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Communication Technologies, Standards, and Challenges in
  Electrified Vehicle Charging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Ma, Yuan Zhou, Hanwen Zhang, Qun Wang, Haijian Sun, Hongjie Wang, Rose Qingyang Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As public awareness of environmental protection continues to grow, the trend
of integrating more electric vehicles (EVs) into the transportation sector is
rising. Unlike conventional internal combustion engine (ICE) vehicles, EVs can
minimize carbon emissions and potentially achieve autonomous driving. However,
several obstacles hinder the widespread adoption of EVs, such as their
constrained driving range and the extended time required for charging. One
alternative solution to address these challenges is implementing dynamic
wireless power transfer (DWPT), charging EVs in motion on the road. Moreover,
charging stations with static wireless power transfer (SWPT) infrastructure can
replace existing gas stations, enabling users to charge EVs in parking lots or
at home. This paper surveys the communication infrastructure for static and
dynamic wireless charging in electric vehicles. It encompasses all
communication aspects involved in the wireless charging process. The
architecture and communication requirements for static and dynamic wireless
charging are presented separately. Additionally, a comprehensive comparison of
existing communication standards is provided. The communication with the grid
is also explored in detail. The survey gives attention to security and privacy
issues arising during communications. In summary, the paper addresses the
challenges and outlines upcoming trends in communication for EV wireless
charging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IET Communication as a survey paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-Dimensional Frequency-Difference-of-Arrival Varieties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeanne Duflot, Margaret Cheney, James A. Given
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies Frequency-Difference-of-Arrival (FDOA) curves for the
2-dimensional, 2-sensor case. The primary focus of this paper is to give a
description of curves associated to the FDOA problem from the algebro-geometric
point of view. To be more precise, the complex projective picture of the family
of FDOA curves for all possible relative velocities is described.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>64 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RKFNet: A Novel Neural Network Aided Robust Kalman Filter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng Hao, Oktay Karakus, Alin Achim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by the filtering challenges in linear systems disturbed by
non-Gaussian heavy-tailed noise, the robust Kalman filters (RKFs) leveraging
diverse heavy-tailed distributions have been introduced. However, the RKFs rely
on precise noise models, and large model errors can degrade their filtering
performance. Also, the posterior approximation by the employed variational
Bayesian (VB) method can further decrease the estimation precision. Here, we
introduce an innovative RKF method, the RKFNet, which combines the
heavy-tailed-distribution-based RKF framework with the deep learning (DL)
technique and eliminates the need for the precise parameters of the
heavy-tailed distributions. To reduce the VB approximation error, the
mixing-parameter-based function and the scale matrix are estimated by the
incorporated neural network structures. Also, the stable training process is
achieved by our proposed unsupervised scheduled sampling (USS) method, where a
loss function based on the Student's t (ST) distribution is utilised to
overcome the disturbance of the noise outliers and the filtering results of the
traditional RKFs are employed as reference sequences. Furthermore, the RKFNet
is evaluated against various RKFs and recurrent neural networks (RNNs) under
three kinds of heavy-tailed measurement noises, and the simulation results
showcase its efficacy in terms of estimation accuracy and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resonant Beam Communications: A New Design Paradigm and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanming Tian, Dongxu Li, Chuan Huang, Qingwen Liu, Shengli Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resonant beam communications (RBCom), which adopt oscillating photons between
two separate retroreflectors for information transmission, exhibit potential
advantages over other types of wireless optical communications (WOC). However,
echo interference generated by the modulated beam reflected from the receiver
affects the transmission of the desired information. To tackle this challenge,
a synchronization-based point-to-point RBCom system is proposed to eliminate
the echo interference, and the design for the transmitter and receiver is
discussed. Subsequently, the performance of the proposed RBCom is evaluated and
compared with that of visible light communications (VLC) and free space optical
communications (FOC). Finally, future research directions are outlined and
several implementation challenges of RBCom systems are highlighted.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design and Performance of Resonant Beam Communications -- Part II:
  Mobile Scenario 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongxu Li, Yuanming Tian, Chuan Huang, Qingwen Liu, Shengli Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This two-part paper focuses on the system design and performance analysis for
a point-to-point resonant beam communication (RBCom) system under both the
quasi-static and mobile scenarios. Part I of this paper proposes a
synchronization-based information transmission scheme and derives the capacity
upper and lower bounds for the quasi-static channel case. In Part II, we
address the mobile scenario, where the receiver is in relative motion to the
transmitter, and derive a mobile RBCom channel model that jointly considers the
Doppler effect, channel variation, and echo interference. With the obtained
channel model, we prove that the channel gain of the mobile RBCom decreases as
the number of transmitted frames increases, and thus show that the considered
mobile RBCom terminates after the transmitter sends a certain number of frames
without frequency compensation. By deriving an upper bound on the number of
successfully transmitted frames, we formulate the throughput maximization
problem for the considered mobile RBCom system, and solve it via a sequential
parametric convex approximation (SPCA) method. Finally, simulation results
validate the analysis of our proposed method in some typical scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design and Performance of Resonant Beam Communications -- Part I:
  Quasi-Static Scenario 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongxu Li, Yuanming Tian, Chuan Huang, Qingwen Liu, Shengli Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This two-part paper studies a point-to-point resonant beam communication
(RBCom) system, where two separately deployed retroreflectors are adopted to
generate the resonant beam between the transmitter and the receiver, and
analyzes the transmission rate of the considered system under both the
quasi-static and mobile scenarios. Part I of this paper focuses on the
quasi-static scenario where the locations of the transmitter and the receiver
are relatively fixed. Specifically, we propose a new information-bearing scheme
which adopts a synchronization-based amplitude modulation method to mitigate
the echo interference caused by the reflected resonant beam. With this scheme,
we show that the quasi-static RBCom channel is equivalent to a Markov channel
and can be further simplified as an amplitude-constrained additive white
Gaussian noise channel. Moreover, we develop an algorithm that jointly employs
the bisection and exhaustive search to maximize its capacity upper and lower
bounds. Finally, numerical results validate our analysis. Part II of this paper
discusses the performance of the RBCom system under the mobile scenario.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Near-field Beam Steering with Planar Antenna Array 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleš Simončič, Andrej Hrovat, Grega Morano, Teodora Kocevska, Tomaž Javornik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beam steering enables manipulation of the electromagnetic radiation patterns
in antenna array systems. A methodology for steering beams in the near field of
a planar antenna array with known phase wavefront functions towards arbitrary
azimuth and elevation angles is described in this paper. Rotation of the phase
wavefront function is used while preserving the shape. The phase shifts for
antenna element excitation currents are determined based on the distances from
antenna elements to the nearest point on the rotated wavefront. Beam steering
utilizing a Gaussian and a Bessel beam is studied. Phase distribution examples
for various steering directions are considered. For Bessel beam steering, a
discussion on the resulting beam shapes and the steering impact on the
polarization mismatch is provided. The results show a non-negligible magnitude
of the cross-polarization with values depending on the steering direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was submitted to Seventh International Balkan Conference
  on Communications and Networkingm (BalkanCom24), Ljubljana, Slovenia, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploit High-Dimensional RIS Information to Localization: What Is the
  Impact of Faulty Element? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuo Wu, Cunhua Pan, Kangda Zhi, Hong Ren, Maged Elkashlan, Cheng-Xiang Wang, Robert Schober, Xiaohu You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel localization algorithm using the reconfigurable
intelligent surface (RIS) received signal, i.e., RIS information. Compared with
BS received signal, i.e., BS information, RIS information offers higher
dimension and richer feature set, thereby providing an enhanced capacity to
distinguish positions of the mobile users (MUs). Additionally, we address a
practical scenario where RIS contains some unknown (number and places) faulty
elements that cannot receive signals. Initially, we employ transfer learning to
design a two-phase transfer learning (TPTL) algorithm, designed for accurate
detection of faulty elements. Then our objective is to regain the information
lost from the faulty elements and reconstruct the complete high-dimensional RIS
information for localization. To this end, we propose a transfer-enhanced
dual-stage (TEDS) algorithm. In \emph{Stage I}, we integrate the CNN and
variational autoencoder (VAE) to obtain the RIS information, which in
\emph{Stage II}, is input to the transferred DenseNet 121 to estimate the
location of the MU. To gain more insight, we propose an alternative algorithm
named transfer-enhanced direct fingerprint (TEDF) algorithm which only requires
the BS information. The comparison between TEDS and TEDF reveals the
effectiveness of faulty element detection and the benefits of utilizing the
high-dimensional RIS information for localization. Besides, our empirical
results demonstrate that the performance of the localization algorithm is
dominated by the high-dimensional RIS information and is robust to unoptimized
phase shifts and signal-to-noise ratio (SNR).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Employing High-Dimensional RIS Information for RIS-aided Localization
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuo Wu, Cunhua Pan, Kangda Zhi, Hong Ren, Maged Elkashlan, Jiangzhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconfigurable intelligent surface (RIS)-aided localization systems have
attracted extensive research attention due to their accuracy enhancement
capabilities. However, most studies primarily utilized the base stations (BS)
received signal, i.e., BS information, for localization algorithm design,
neglecting the potential of RIS received signal, i.e., RIS information.
Compared with BS information, RIS information offers higher dimension and
richer feature set, thereby significantly improving the ability to extract
positions of the mobile users (MUs). Addressing this oversight, this paper
explores the algorithm design based on the high-dimensional RIS information.
Specifically, we first propose a RIS information reconstruction (RIS-IR)
algorithm to reconstruct the high-dimensional RIS information from the
low-dimensional BS information. The proposed RIS-IR algorithm comprises a data
processing module for preprocessing BS information, a convolution neural
network (CNN) module for feature extraction, and an output module for
outputting the reconstructed RIS information. Then, we propose a transfer
learning based fingerprint (TFBF) algorithm that employs the reconstructed
high-dimensional RIS information for MU localization. This involves adapting a
pre-trained DenseNet-121 model to map the reconstructed RIS signal to the MU's
three-dimensional (3D) position. Empirical results affirm that the localization
performance is significantly influenced by the high-dimensional RIS information
and maintains robustness against unoptimized phase shifts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BackCom Assisted Hybrid NOMA Uplink Transmission for Ambient IoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiguo Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid non-orthogonal multiple access (H-NOMA) has recently received
significant attention as a general framework of multiple access, where both
conventional orthogonal multiple access (OMA) and pure NOMA are its special
cases. This paper focuses on the application of H-NOMA to ambient Internet of
Things (IoT) with energy-constrained devices, where a new backscatter
communication (BackCom) assisted H-NOMA uplink scheme is developed. Resource
allocation for H-NOMA uplink transmission is also considered, where an overall
power minimization problem is formulated. Insightful understandings for the key
features of BackCom assisted H-NOMA and its difference from conventional H-NOMA
are illustrated by developing analytical results for the two-user special case.
For the general multi-user scenario, two algorithms, one based on the
branch-bound (BB) principle and the other based on successive convex
approximation (SCA), are developed to realize different tradeoffs between the
system performance and complexity. The numerical results are also provided to
verify the accuracy of the developed analytical results and demonstrate the
performance gain of H-NOMA over OMA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safeguarding Next Generation Multiple Access Using Physical Layer
  Security Techniques: A Tutorial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Lv, Dongyang Xu, Rose Qingyang Hu, Yinghui Ye, Long Yang, Xianfu Lei, Xianbin Wang, Dong In Kim, Arumugam Nallanathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by the ever-increasing requirements of ultra-high spectral efficiency,
ultra-low latency, and massive connectivity, the forefront of wireless research
calls for the design of advanced next generation multiple access schemes to
facilitate provisioning of these stringent demands. This inspires the embrace
of non-orthogonal multiple access (NOMA) in future wireless communication
networks. Nevertheless, the support of massive access via NOMA leads to
additional security threats, due to the open nature of the air interface, the
broadcast characteristic of radio propagation as well as intertwined
relationship among paired NOMA users. To address this specific challenge, the
superimposed transmission of NOMA can be explored as new opportunities for
security aware design, for example, multiuser interference inherent in NOMA can
be constructively engineered to benefit communication secrecy and privacy. The
purpose of this tutorial is to provide a comprehensive overview on the
state-of-the-art physical layer security techniques that guarantee wireless
security and privacy for NOMA networks, along with the opportunities, technical
challenges, and future research trends.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Invited paper by Proceedings of the IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Power-Aware Sparse Reflect Beamforming in Active RIS-aided Interference
  Channels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhe Long, Hu Zhou, Ying-Chang Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active reconfigurable intelligent surface (RIS) has attracted significant
attention in wireless communications, due to its reflecting elements (REs)
capable of reflecting incident signals with not only phase shifts but also
amplitude amplifications. In this paper, we are interested in active RIS-aided
interference channels in which $K$ user pairs share the same time and frequency
resources with the aid of active RIS. Thanks to the promising amplitude
amplification capability, activating a moderate number of REs, rather than all
of them, is sufficient for the active RIS to mitigate cross-channel
interferences. Motivated by this, we propose a power-aware sparse reflect
beamforming design for the active RIS-aided interference channels, which allows
the active RIS to flexibly adjust the number of activated REs for the sake of
reducing hardware and power costs. Specifically, we establish the power
consumption model in which only those activated REs consume the biasing and
operation power that supports the amplitude amplification, yielding an
$\ell_0$-norm power consumption function. Based on the proposed model, we
investigate a sum-rate maximization problem and an active RIS power
minimization problem by carefully designing the sparse reflect beamforming
vector. To solve these problems, we first replace the nonconvex $\ell_0$-norm
function with an iterative reweighted $\ell_1$-norm function. Then, fractional
programming is used to solve the sum-rate maximization, while semidefinite
programming together with the difference-of-convex algorithm (DCA) is used to
solve the active RIS power minimization. Numerical results show that the
proposed sparse designs can notably increase the sum rate of user pairs and
decrease the power consumption of active RIS in interference channels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Integrated Sensing and Communication Signal Design: A Sphere
  Packing Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuaishuai Guo, Kaiqian Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The design of communication signal sets is fundamentally a sphere packing
problem. It aims to identify a set of M points in an N -dimensional space, with
the objective of maximizing the separability of points that represent different
bits.In contrast, signals used for sensing targets should ideally be
asdeterministic as possible. This paper explores the inherent conflict and
trade-off between communication and sensing when these functions are combined
within the same signal set. We present a unified approach to signal design in
the time, frequency, and space domains for integrated sensing and communication
(ISAC), framing it as a modified sphere packing problem. Through adept formula
manipulation, this problem is transformed into a large-scale quadratic
constrained quadratic programming (QCQP) challenge. We propose an augmented
Lagrangian and dual ascent (ALDA) algorithm for iterative problem-solving. The
computational complexity of this approach is analyzed and found to be daunting
for large, high-dimensional signal set designs. To address this, we introduce a
bit-dimension-power splitting (BDPS) method. This method decomposes the
large-scale QCQP into a series of smaller-scale problems that can be solved
more efficiently and in parallel, significantly reducing the overall
computational load. Extensive simulations have been conducted to validate the
effectiveness of our proposed signal design methods in the context of ISAC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE TCOM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Next Generation Advanced Transceiver Technologies for 6G 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changsheng You, Yunlong Cai, Yuanwei Liu, Marco Di Renzo, Tolga M. Duman, Aylin Yener, A. Lee Swindlehurst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To accommodate new applications such as extended reality, fully autonomous
vehicular networks and the metaverse, next generation wireless networks are
going to be subject to much more stringent performance requirements than the
fifth-generation (5G) in terms of data rates, reliability, latency, and
connectivity. It is thus necessary to develop next generation advanced
transceiver (NGAT) technologies for efficient signal transmission and
reception. In this tutorial, we explore the evolution of NGAT from three
different perspectives. Specifically, we first provide an overview of new-field
NGAT technology, which shifts from conventional far-field channel models to new
near-field channel models. Then, three new-form NGAT technologies and their
design challenges are presented, including reconfigurable intelligent surfaces,
flexible antennas, and holographic multi-input multi-output (MIMO) systems.
Subsequently, we discuss recent advances in semantic-aware NGAT technologies,
which can utilize new metrics for advanced transceiver designs. Finally, we
point out other promising transceiver technologies for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper gives a comprehensive tutorial overview of next generation
  advanced transceiver (NGAT) technologies for 6G</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-Carrier Delay-Doppler Domain Equalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuto Hama, Hideki Ochiai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For doubly-selective channels, delay-Doppler (DD) modulation, mostly known as
orthogonal time frequency space (OTFS) modulation, enables simultaneous
compensation of delay and Doppler shifts. However, OTFS modulated signal has
high peak-to-average power ratio (PAPR) because of its precoding operation
performed over the DD domain. In order to deal with this problem, we propose a
single-carrier transmission with delay-Doppler domain equalization (SC-DDE). In
this system, the discretized time-domain SC signal is converted to the DD
domain by discrete Zak transform (DZT) at the receiver side, followed by
delay-Doppler domain equalization (DDE). Since equalization is performed in the
DD domain, the SC-DDE receiver should acquire the channel delay-Doppler
response. To this end, we introduce an embedded pilot-aided channel estimation
scheme designed for SC-DDE, which does not affect the peak power property of
transmitted signals. Through computer simulation, distribution of PAPR and bit
error rate (BER) performance of the proposed system are compared with those of
the conventional OTFS and SC with frequency-domain equalization (SC-FDE). As a
result, our proposed SC-DDE significantly outperforms SC-FDE in terms of BER at
the expense of additional computational complexity at the receiver.
Furthermore, SC-DDE shows much lower PAPR than OTFS even though they achieve
comparable coded BER performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures, submitted to IEEE journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resolution Limit of Single-Photon LiDAR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stanley H. Chan, Hashan K. Weerasooriya, Weijian Zhang, Pamela Abshire, Istvan Gyongy, Robert K. Henderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single-photon Light Detection and Ranging (LiDAR) systems are often equipped
with an array of detectors for improved spatial resolution and sensing speed.
However, given a fixed amount of flux produced by the laser transmitter across
the scene, the per-pixel Signal-to-Noise Ratio (SNR) will decrease when more
pixels are packed in a unit space. This presents a fundamental trade-off
between the spatial resolution of the sensor array and the SNR received at each
pixel. Theoretical characterization of this fundamental limit is explored. By
deriving the photon arrival statistics and introducing a series of new
approximation techniques, the Mean Squared Error (MSE) of the
maximum-likelihood estimator of the time delay is derived. The theoretical
predictions align well with simulations and real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accuracy-Aware Cooperative Sensing and Computing for Connected
  Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuehan Ye, Kaige Qu, Weihua Zhuang, Xuemin Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To maintain high perception performance among connected and autonomous
vehicles (CAVs), in this paper, we propose an accuracy-aware and
resource-efficient raw-level cooperative sensing and computing scheme among
CAVs and road-side infrastructure. The scheme enables fined-grained partial raw
sensing data selection, transmission, fusion, and processing in per-object
granularity, by exploiting the parallelism among object classification subtasks
associated with each object. A supervised learning model is trained to capture
the relationship between the object classification accuracy and the data
quality of selected object sensing data, facilitating accuracy-aware sensing
data selection. We formulate an optimization problem for joint sensing data
selection, subtask placement and resource allocation among multiple object
classification subtasks, to minimize the total resource cost while satisfying
the delay and accuracy requirements. A genetic algorithm based iterative
solution is proposed for the optimization problem. Simulation results
demonstrate the accuracy awareness and resource efficiency achieved by the
proposed cooperative sensing and computing scheme, in comparison with benchmark
solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RadioGAT: A Joint Model-based and Data-driven Framework for Multi-band
  Radiomap Reconstruction via Graph Attention Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojie Li, Songyang Zhang, Hang Li, Xiaoyang Li, Lexi Xu, Haigao Xu, Hui Mei, Guangxu Zhu, Nan Qi, Ming Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-band radiomap reconstruction (MB-RMR) is a key component in wireless
communications for tasks such as spectrum management and network planning.
However, traditional machine-learning-based MB-RMR methods, which rely heavily
on simulated data or complete structured ground truth, face significant
deployment challenges. These challenges stem from the differences between
simulated and actual data, as well as the scarcity of real-world measurements.
To address these challenges, our study presents RadioGAT, a novel framework
based on Graph Attention Network (GAT) tailored for MB-RMR within a single
area, eliminating the need for multi-region datasets. RadioGAT innovatively
merges model-based spatial-spectral correlation encoding with data-driven
radiomap generalization, thus minimizing the reliance on extensive data
sources. The framework begins by transforming sparse multi-band data into a
graph structure through an innovative encoding strategy that leverages radio
propagation models to capture the spatial-spectral correlation inherent in the
data. This graph-based representation not only simplifies data handling but
also enables tailored label sampling during training, significantly enhancing
the framework's adaptability for deployment. Subsequently, The GAT is employed
to generalize the radiomap information across various frequency bands.
Extensive experiments using raytracing datasets based on real-world
environments have demonstrated RadioGAT's enhanced accuracy in supervised
learning settings and its robustness in semi-supervised scenarios. These
results underscore RadioGAT's effectiveness and practicality for MB-RMR in
environments with limited data availability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE journal for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SignSGD with Federated Voting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanho Park, H. Vincent Poor, Namyoon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed learning is commonly used for accelerating model training by
harnessing the computational capabilities of multiple-edge devices. However, in
practical applications, the communication delay emerges as a bottleneck due to
the substantial information exchange required between workers and a central
parameter server. SignSGD with majority voting (signSGD-MV) is an effective
distributed learning algorithm that can significantly reduce communication
costs by one-bit quantization. However, due to heterogeneous computational
capabilities, it fails to converge when the mini-batch sizes differ among
workers. To overcome this, we propose a novel signSGD optimizer with
\textit{federated voting} (signSGD-FV). The idea of federated voting is to
exploit learnable weights to perform weighted majority voting. The server
learns the weights assigned to the edge devices in an online fashion based on
their computational capabilities. Subsequently, these weights are employed to
decode the signs of the aggregated local gradients in such a way to minimize
the sign decoding error probability. We provide a unified convergence rate
analysis framework applicable to scenarios where the estimated weights are
known to the parameter server either perfectly or imperfectly. We demonstrate
that the proposed signSGD-FV algorithm has a theoretical convergence guarantee
even when edge devices use heterogeneous mini-batch sizes. Experimental results
show that signSGD-FV outperforms signSGD-MV, exhibiting a faster convergence
rate, especially in heterogeneous mini-batch sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy-Efficient Hybrid Beamforming with Dynamic On-off Control for
  Integrated Sensing, Communications, and Powering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Hao, Yuan Fang, Xianghao Yu, Jie Xu, Ling Qiu, Lexi Xu, Shuguang Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the energy-efficient hybrid beamforming design for a
multi-functional integrated sensing, communications, and powering (ISCAP)
system. In this system, a base station (BS) with a hybrid analog-digital (HAD)
architecture sends unified wireless signals to communicate with multiple
information receivers (IRs), sense multiple point targets, and wirelessly
charge multiple energy receivers (ERs) at the same time. To facilitate the
energy-efficient design, we present a novel HAD architecture for the BS
transmitter, which allows dynamic on-off control of its radio frequency (RF)
chains and analog phase shifters (PSs) through a switch network. We also
consider a practical and comprehensive power consumption model for the BS, by
taking into account the power-dependent non-linear power amplifier (PA)
efficiency, and the on-off non-transmission power consumption model of RF
chains and PSs. We jointly design the hybrid beamforming and dynamic on-off
control at the BS, aiming to minimize its total power consumption, while
guaranteeing the performance requirements on communication rates, sensing
Cram\'er-Rao bound (CRB), and harvested power levels. The formulation also
takes into consideration the per-antenna transmit power constraint and the
constant modulus constraints for the analog beamformer at the BS. The resulting
optimization problem for ISCAP is highly non-convex. Please refer to the paper
for a complete abstract.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, submitted to IEEE Transactions on Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Learning Using Three-Operator ADMM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.04152v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.04152v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashi Kant, José Mairton B. da Silva Jr., Gabor Fodor, Bo Göransson, Mats Bengtsson, Carlo Fischione
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has emerged as an instance of distributed machine
learning paradigm that avoids the transmission of data generated on the users'
side. Although data are not transmitted, edge devices have to deal with limited
communication bandwidths, data heterogeneity, and straggler effects due to the
limited computational resources of users' devices. A prominent approach to
overcome such difficulties is FedADMM, which is based on the classical
two-operator consensus alternating direction method of multipliers (ADMM). The
common assumption of FL algorithms, including FedADMM, is that they learn a
global model using data only on the users' side and not on the edge server.
However, in edge learning, the server is expected to be near the base station
and have direct access to rich datasets. In this paper, we argue that
leveraging the rich data on the edge server is much more beneficial than
utilizing only user datasets. Specifically, we show that the mere application
of FL with an additional virtual user node representing the data on the edge
server is inefficient. We propose FedTOP-ADMM, which generalizes FedADMM and is
based on a three-operator ADMM-type technique that exploits a smooth cost
function on the edge server to learn a global model parallel to the edge
devices. Our numerical experiments indicate that FedTOP-ADMM has substantial
gain up to 33\% in communication efficiency to reach a desired test accuracy
with respect to FedADMM, including a virtual user on the edge server.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to IEEE Journal of Selected Topics in Signal Processing,
  2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finite-Precision Arithmetic Transceiver for Massive MIMO Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13442v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13442v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Fang, Li Chen, Yunfei Chen, Huarui Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient implementation of massive multiple-input-multiple-output (MIMO)
transceivers is essential for the next-generation wireless networks. To reduce
the high computational complexity of the massive MIMO transceiver, in this
paper, we propose a new massive MIMO architecture using finite-precision
arithmetic. First, we conduct the rounding error analysis and derive the lower
bound of the achievable rate for single-input-multiple-output (SIMO) using
maximal ratio combining (MRC) and multiple-input-single-output (MISO) systems
using maximal ratio transmission (MRT) with finite-precision arithmetic. Then,
considering the multi-user scenario, the rounding error analysis of
zero-forcing (ZF) detection and precoding is derived by using the normal
equations (NE) method. The corresponding lower bounds of the achievable sum
rate are also derived and asymptotic analyses are presented. Built upon
insights from these analyses and lower bounds, we propose a mixed-precision
architecture for massive MIMO systems to offset performance gaps due to
finite-precision arithmetic. The corresponding analysis of rounding errors and
computational costs is obtained. Simulation results validate the derived bounds
and underscore the superiority of the proposed mixed-precision architecture to
the conventional structure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 figures. Submitted to IEEE JSAC for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Massive Uncoordinated Multiple Access for Beyond 5G 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.00098v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.00098v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Mohammadkarimi, Octavia A. Dobre, Moe Z. Win
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing wireless communication systems have been mainly designed to provide
substantial gain in terms of data rates. However, 5G and Beyond will depart
from this scheme, with the objective not only to provide services with higher
data rates. One of the main goals is to support massive machine-type
communications (mMTC) in the IoT applications. Supporting massive uplink (UP)
communications for devices with sporadic traffic pattern and short-packet size,
as it is in many mMTC use cases, is a challenging task, particularly when the
control signaling is not negligible in size compared to the payload. Also,
channel estimation is challenging for sporadic and short-packet transmission
due to the limited number of employed pilots. In this paper, a new UP multiple
access (MA) scheme is proposed for mMTC, which can support a large number of
uncoordinated IoT devices with short-packet and sporadic traffic. The proposed
UP MA scheme removes the overheads associated with the device identifier as
well as pilots related to channel estimation. An alternative mechanism for
device identification is proposed, where a unique spreading code is dedicated
to each IoT device. This unique code is simultaneously used for the spreading
purpose and device identification. Two IoT device identification algorithms
which employ sparse signal reconstruction methods are proposed to determine the
active IoT devices prior to data detection. Specifically, the BIC model order
selection method is employed to develop an IoT device identification algorithm
for unknown and time-varying probability of device activity. Our proposed MA
scheme benefits from a non-coherent multiuser detection algorithm based on
machine learning to enable data detection without a priori knowledge on channel
state information. The effectiveness of the proposed MA scheme for known and
unknown probability of activity is supported by simulation results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computation-Limited Signals: A Channel Capacity Regime Constrained by
  Computational Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05794v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05794v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saulo Queiroz, João P. Vilela, Edmundo Monteiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this letter, we introduce the computational-limited (comp-limited)
signals, a communication capacity regime in which the signal time computational
complexity overhead is the key constraint -- rather than power or bandwidth --
to the overall communication capacity. We present the Spectro-Computational
(SC) analysis, a novel mathematical framework that enhances classic concepts of
information theory -- such as throughput, spectral efficiency and capacity --
to account for the signal processing computational complexity overhead. We
consider a specific Shannon regime under which capacity is expected to get
arbitrarily large as channel resources grow. Under that regime, we identify the
conditions under which the time complexity overhead causes capacity to decrease
rather than increasing, thereby creating the case for the comp-limited regime.
We also provide examples of the SC analysis and show the OFDM waveform is
comp-limited unless the lower-bound computational complexity of the $N$-point
DFT problem verifies as $\Omega(N)$, which remains an open challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Performance Analysis of In-Band-Full-Duplex Multi-Cell Wideband IAB
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Zhang, Tharmalingam Ratnarajah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper analyzes the performance of the 3rd Generation Partnership Project
(3GPP)-inspired multi-cell wideband single-hop backhaul
millimeter-wave-in-band-full-duplex (IBFD)-integrated access and backhaul (IAB)
networks by using stochastic geometry. We model the wired-connected Next
Generation NodeBs (gNBs) as the Mat\'ern hard-core point process (MHCPP) to
meet the real-world deployment requirement and reduce the cost caused by wired
connection in the network. We first derive association probabilities that
reflect how likely the typical user-equipment is served by a gNB or an IAB-node
based on the maximum long-term averaged biased-received-desired-signal power
criteria. Further, by leveraging the composite Gamma-Lognormal distribution, we
derive the closed-form signal to interference plus noise ratio coverage,
capacity with outage, and ergodic capacity of the network. In order to avoid
underestimating the noise, we consider the sidelobe gain on inter-cell
interference links and the analog to digital converter quantization noise.
Compared with the half-duplex transmission, numerical results show an enhanced
capacity with outage and ergodic capacity provided by IBFD under successful
self-interference cancellation. We also study how the power bias and density
ratio of the IAB-node to gNB, and the hard-core distance can affect system
performances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic positioning via ray tracing with noisy angle of arrival
  measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Corlay, Viet-Hoa Nguyen, Nicolas Gresset
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the problems of interference prediction and sensing
for efficient spectrum access and link adaptation. The considered approach for
interference prediction relies on a parametric model. However, we assume that
the number of observations available to learn theses parameters is limited.
This implies that they should be treated as random variables rather than fixed
values. We show how this can impact the spectrum access and link adaptation
strategies. We also introduce the notion of "interferer-coherence time" to
establish the number of independent interferer state realizations experienced
by a codeword. We explain how it can be computed taking into account the model
uncertainty and how this impacts the link adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Communications Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analysis and Compensation of Carrier Frequency Offset Impairments in
  Unique Word OFDM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.05367v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.05367v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Hofbauer, Werner Haselmayr, Hans-Peter Bernhard, Mario Huemer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unique Word-orthogonal frequency division multiplexing (UW-OFDM) is known to
provide various performance benefits over conventional cyclic prefix (CP) based
OFDM. Most important, UW-OFDM features excellent spectral sidelobe suppression
properties and an outstanding bit error ratio (BER) performance. Carrier
frequency offset (CFO) induced impairments denote a challenging task for OFDM
systems of any kind. In this work we investigate the CFO effects on UW-OFDM and
compare it to conventional multi-carrier and single-carrier systems. Different
CFO compensation approaches with different computational complexity are
considered throughout this work and assessed against each other. A mean squared
error analysis carried out after data estimation reveals a significant higher
robustness of UW-OFDM over CP-OFDM against CFO effects. Additionally, the
conducted BER simulations generally support this conclusion for various
scenarios, ranging from uncoded to coded transmission in a frequency selective
environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymptotic Error Rates for Point Process Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhui Rong, Victor Solo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point processes are finding growing applications in numerous fields, such as
neuroscience, high frequency finance and social media. So classic problems of
classification and clustering are of increasing interest. However, analytic
study of misclassification error probability in multi-class classification has
barely begun. In this paper, we tackle the multi-class likelihood
classification problem for point processes and develop, for the first time,
both asymptotic upper and lower bounds on the error rate in terms of computable
pair-wise affinities. We apply these general results to classifying renewal
processes. Under some technical conditions, we show that the bounds have
exponential decay and give explicit associated constants. The results are
illustrated with a non-trivial simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Systems and Control
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Operation of Reconfigurable Active Distribution Networks Aiming
  at Resiliency Improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saeed Behzadi, Amir Bagheri, Abbas Rabiee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As natural disasters bring about power outage and financial losses, network
resiliency is an important challenge for distribution network operators (DNOs).
On the other side, power loss reduction during normal operating condition is a
major concern of DNOs. In this paper, optimal scheduling of active distribution
network (ADN) is addressed through simultaneous minimization of power loss in
normal condition and load shedding in critical condition after natural
disasters. A new formulation is developed for the network reconfiguration to
optimize the system operation in both normal and emergency conditions in the
presence of conventional and renewable-energy-based distributed generation (DG)
as well as energy storage systems (ESSs). The line flow based (LFB) algorithm
is used for the AC power flow calculations, and all the developed relations
have been convexified to construct a mixed-integer quadratically-constrained
programming (MIQCP) optimization model. The simulations have been implemented
on the IEEE 33-bus system in GAMS, and the results are investigated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact-Aware Bimanual Catching of Large-Momentum Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yan, Theodoros Stouraitis, João Moura, Wenfu Xu, Michael Gienger, Sethu Vijayakumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates one of the most challenging tasks in dynamic
manipulation -- catching large-momentum moving objects. Beyond the realm of
quasi-static manipulation, dealing with highly dynamic objects can
significantly improve the robot's capability of interacting with its
surrounding environment. Yet, the inevitable motion mismatch between the fast
moving object and the approaching robot will result in large impulsive forces,
which lead to the unstable contacts and irreversible damage to both the object
and the robot. To address the above problems, we propose an online optimization
framework to: 1) estimate and predict the linear and angular motion of the
object; 2) search and select the optimal contact locations across every surface
of the object to mitigate impact through sequential quadratic programming
(SQP); 3) simultaneously optimize the end-effector motion, stiffness, and
contact force for both robots using multi-mode trajectory optimization (MMTO);
and 4) realise the impact-aware catching motion on the compliant robotic system
based on indirect force controller. We validate the impulse distribution,
contact selection, and impact-aware MMTO algorithms in simulation and
demonstrate the benefits of the proposed framework in real-world experiments
including catching large-momentum moving objects with well-defined motion,
constrained motion and free-flying motion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DASA: Delay-Adaptive Multi-Agent Stochastic Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolo Dal Fabbro, Arman Adibi, H. Vincent Poor, Sanjeev R. Kulkarni, Aritra Mitra, George J. Pappas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a setting in which $N$ agents aim to speedup a common Stochastic
Approximation (SA) problem by acting in parallel and communicating with a
central server. We assume that the up-link transmissions to the server are
subject to asynchronous and potentially unbounded time-varying delays. To
mitigate the effect of delays and stragglers while reaping the benefits of
distributed computation, we propose \texttt{DASA}, a Delay-Adaptive algorithm
for multi-agent Stochastic Approximation. We provide a finite-time analysis of
\texttt{DASA} assuming that the agents' stochastic observation processes are
independent Markov chains. Significantly advancing existing results,
\texttt{DASA} is the first algorithm whose convergence rate depends only on the
mixing time $\tmix$ and on the average delay $\tau_{avg}$ while jointly
achieving an $N$-fold convergence speedup under Markovian sampling. Our work is
relevant for various SA applications, including multi-agent and distributed
temporal difference (TD) learning, Q-learning and stochastic optimization with
correlated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Discrete-Time Least-Squares Adaptive State Tracking Control Scheme
  with A Mobile-Robot System Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianhong Zhao, Gang Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops an adaptive state tracking control scheme for
discrete-time systems, using the least-squares algorithm, as the new solution
to the long-standing discrete-time adaptive state tracking control problem to
which the Lyapunov method (well-developed for the continuous-time adaptive
state tracking problem) is not applicable. The new adaptive state tracking
scheme is based on a recently-developed new discrete-time error model which has
been used for gradient algorithm based state tracking control schemes, and uses
the least-squares algorithm for parameter adaptation. The new least-squares
algorithm is derived to minimize an accumulative estimation error, to ensure
certain optimality for parameter estimation. The system stability and output
tracking properties are studied. Technical results are presented in terms of
plant-model matching, error model, adaptive law, optimality formulation, and
stability and tracking analysis. The developed adaptive control scheme is
applied to a discrete-time multiple mobile robot system to meet an adaptive
state tracking objective. In addition, a collision avoidance mechanism is
proposed to prevent collisions in the whole tracking process. Simulation
results are presented, which verify the desired system state tracking
properties under the developed least-squares algorithm based adaptive control
scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Learning of Dynamics Using Prior Domain Knowledge in the Sampling
  Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin S. Miller, Adam J. Thorpe, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an active learning algorithm for learning dynamics that leverages
side information by explicitly incorporating prior domain knowledge into the
sampling process. Our proposed algorithm guides the exploration toward regions
that demonstrate high empirical discrepancy between the observed data and an
imperfect prior model of the dynamics derived from side information. Through
numerical experiments, we demonstrate that this strategy explores regions of
high discrepancy and accelerates learning while simultaneously reducing model
uncertainty. We rigorously prove that our active learning algorithm yields a
consistent estimate of the underlying dynamics by providing an explicit rate of
convergence for the maximum predictive variance. We demonstrate the efficacy of
our approach on an under-actuated pendulum system and on the half-cheetah
MuJoCo environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-dimensional continuification control of large-scale multi-agent
  systems under limited sensing and perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gian Carlo Maffettone, Mario di Bernardo, Maurizio Porfiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the robustness of a novel high-dimensional
continuification control method for complex multi-agent systems. We begin by
formulating a partial differential equation describing the spatio-temporal
density dynamics of swarming agents. A stable control action for the density is
then derived and validated under nominal conditions. Subsequently, we
discretize this macroscopic strategy into actionable velocity inputs for the
system's agents. Our analysis demonstrates the robustness of the approach
beyond idealized assumptions of unlimited sensing and absence of perturbations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2310.01573</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Finite-time Stabilization of Linear Systems with Limited State
  Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhou, Andrey Polyakov, Gang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the robust asymptotic stabilization of a linear
time-invariant (LTI) system by a static feedback with a static state
quantization. It is shown that the controllable LTI system can be stabilized to
zero in a finite time by means of a nonlinear feedback with a quantizer having
a limited (finite) number of values (quantization seeds) even when all
parameters of the controller and the quantizer are time-invariant. The control
design is based on generalized homogeneity. A homogeneous spherical quantizer
is introduced. The static homogeneous feedback is shown to be local (or global)
finite-time stabilizer for the linear system (dependently of the system
matrix). The tuning rules for both the quantizer and the feedback law are
obtained in the form of Linear Matrix Inequalities (LMIs). The closed-loop
system is proven to be robust with respect to some bounded matched and
vanishing mismatched perturbations. Theoretical results are supported by
numerical simulations. \
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Belief Samples Are All You Need For Social Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahyar JafariNodeh, Amir Ajorlou, Ali Jadbabaie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the problem of social learning, where a group of
agents embedded in a social network are interested in learning an underlying
state of the world. Agents have incomplete, noisy, and heterogeneous sources of
information, providing them with recurring private observations of the
underlying state of the world. Agents can share their learning experience with
their peers by taking actions observable to them, with values from a finite
feasible set of states. Actions can be interpreted as samples from the beliefs
which agents may form and update on what the true state of the world is.
Sharing samples, in place of full beliefs, is motivated by the limited
communication, cognitive, and information-processing resources available to
agents especially in large populations. Previous work (Salhab et al.) poses the
question as to whether learning with probability one is still achievable if
agents are only allowed to communicate samples from their beliefs. We provide a
definite positive answer to this question, assuming a strongly connected
network and a ``collective distinguishability'' assumption, which are both
required for learning even in full-belief-sharing settings. In our proposed
belief update mechanism, each agent's belief is a normalized weighted geometric
interpolation between a fully Bayesian private belief -- aggregating
information from the private source -- and an ensemble of empirical
distributions of the samples shared by her neighbors over time. By carefully
constructing asymptotic almost-sure lower/upper bounds on the frequency of
shared samples matching the true state/or not, we rigorously prove the
convergence of all the beliefs to the true state, with probability one.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Output-feedback Synthesis Orbit Geometry: Quotient Manifolds and LQG
  Direct Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spencer Kraisler, Mehran Mesbahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider direct policy optimization for the
linear-quadratic Gaussian (LQG) setting. Over the past few years, it has been
recognized that the landscape of stabilizing output-feedback controllers of
relevance to LQG has an intricate geometry, particularly as it pertains to the
existence of spurious stationary points. In order to address such challenges,
in this paper, we first adopt a Riemannian metric for the space of stabilizing
full-order minimal output-feedback controllers. We then proceed to prove that
the orbit of such controllers modulo coordinate transformation admits a
Riemannian quotient manifold structure. This geometric structure is then used
to develop a Riemannian gradient descent for the direct LQG policy
optimization. We prove a local convergence guarantee with linear rate and show
the proposed approach exhibits significantly faster and more robust numerical
performance as compared with ordinary gradient descent for LQG. Subsequently,
we provide reasons for this observed behavior; in particular, we argue that
optimizing over the orbit space of controllers is the right theoretical and
computational setup for direct LQG policy optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximation with Random Shallow ReLU Networks with Applications to
  Model Reference Adaptive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Lamperski, Tyler Lekang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks are regularly employed in adaptive control of nonlinear
systems and related methods o reinforcement learning. A common architecture
uses a neural network with a single hidden layer (i.e. a shallow network), in
which the weights and biases are fixed in advance and only the output layer is
trained. While classical results show that there exist neural networks of this
type that can approximate arbitrary continuous functions over bounded regions,
they are non-constructive, and the networks used in practice have no
approximation guarantees. Thus, the approximation properties required for
control with neural networks are assumed, rather than proved. In this paper, we
aim to fill this gap by showing that for sufficiently smooth functions, ReLU
networks with randomly generated weights and biases achieve $L_{\infty}$ error
of $O(m^{-1/2})$ with high probability, where $m$ is the number of neurons. It
suffices to generate the weights uniformly over a sphere and the biases
uniformly over an interval. We show how the result can be used to get
approximations of required accuracy in a model reference adaptive control
application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review for Conference on Decision and Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Step Duration for Precise Foot Placement: Achieving Robust
  Bipedal Locomotion on Terrains with Restricted Footholds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Xiang, Victor Paredes, Ayonga Hereid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel multi-step preview foot placement planning
algorithm designed to enhance the robustness of bipedal robotic walking across
challenging terrains with restricted footholds. Traditional one-step preview
planning struggles to maintain stability when stepping areas are severely
limited, such as with random stepping stones. In this work, we developed a
discrete-time Model Predictive Control (MPC) based on the step-to-step discrete
evolution of the Divergent Component of Motion (DCM) of bipedal locomotion.
This approach adaptively changes the step duration for optimal foot placement
under constraints, thereby ensuring the robot's operational viability over
multiple future steps and significantly improving its ability to navigate
through environments with tight constraints on possible footholds. The
effectiveness of this planning algorithm is demonstrated through simulations
that include a variety of complex stepping-stone configurations and external
perturbations. These tests underscore the algorithm's improved performance for
navigating foothold-restricted environments, even with the presence of external
disturbances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures, submitted to CDC 2024, for associated simulation
  video, see https://youtu.be/2jhikPlZmbE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State-Augmented Linear Games with Antagonistic Error for
  High-Dimensional, Nonlinear Hamilton-Jacobi Reachability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Will Sharpless, Yat Tin Chow, Sylvia Herbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hamilton-Jacobi Reachability (HJR) is a popular method for analyzing the
liveness and safety of a dynamical system with bounded control and disturbance.
The corresponding HJ value function offers a robust controller and
characterizes the reachable sets, but is traditionally solved with Dynamic
Programming (DP) and limited to systems of dimension less than six. Recently,
the space-parallelizeable, generalized Hopf formula has been shown to also
solve the HJ value with a nearly three-log increase in dimension limit, but is
limited to linear systems. To extend this potential, we demonstrate how
state-augmented (SA) spaces, which are well-known for their improved
linearization accuracy, may be used to solve tighter, conservative
approximations of the value function with any linear model in this SA space.
Namely, we show that with a representation of the true dynamics in the SA
space, a series of inequalities confirms that the value of a SA linear game
with antagonistic error is a conservative envelope of the true value function.
It follows that if the optimal controller for the HJ SA linear game with error
may succeed, it will also succeed in the true system. Unlike previous methods,
this result offers the ability to safely approximate reachable sets and their
corresponding controllers with the Hopf formula in a non-convex manner.
Finally, we demonstrate this in the slow manifold system for clarity, and in
the controlled Van der Pol system with different lifting functions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Optimal Solution to Infinite Horizon Nonlinear Control Problems: Part
  II 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Naveed Gul Mohamed, Aayushman Sharma, Raman Goyal, Suman Chakravorty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the infinite horizon optimal control problem for
nonlinear systems. Under the condition of nonlinear controllability of the
system to any terminal set containing the origin and forward invariance of the
terminal set, we establish a regularized solution approach consisting of a
``finite free final time" optimal transfer problem to the terminal set which
renders the set globally asymptotically stable. Further, we show that the
approximations converge to the optimal infinite horizon cost as the size of the
terminal set decreases to zero. We also perform the analysis for the discounted
problem and show that the terminal set is asymptotically stable only for a
subset of the state space and not globally. The theory is empirically evaluated
on various nonholonomic robotic systems to show that the cost of our
approximate problem converges and the transfer time into the terminal set is
dependent on the initial state of the system, necessitating the free final time
formulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Methods for Trust in Collaborative Multi-Agent Autonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. Spencer Hallyburton, Miroslav Pajic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent, collaborative sensor fusion is a vital component of a
multi-national intelligence toolkit. In safety-critical and/or contested
environments, adversaries may infiltrate and compromise a number of agents. We
analyze state of the art multi-target tracking algorithms under this
compromised agent threat model. We prove that the track existence probability
test ("track score") is significantly vulnerable to even small numbers of
adversaries. To add security awareness, we design a trust estimation framework
using hierarchical Bayesian updating. Our framework builds beliefs of trust on
tracks and agents by mapping sensor measurements to trust pseudomeasurements
(PSMs) and incorporating prior trust beliefs in a Bayesian context. In case
studies, our trust estimation algorithm accurately estimates the
trustworthiness of tracks/agents, subject to observability limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spline Trajectory Tracking and Obstacle Avoidance for Mobile Agents via
  Convex Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akua Dickson, Christos G. Cassandras, Roberto Tron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an output feedback control-based motion planning technique for
agents to enable them to converge to a specified polynomial trajectory while
imposing a set of safety constraints on our controller to avoid collisions
within the free configuration space (polygonal environment). To achieve this,
we 1) decompose our polygonal environment into different overlapping cells 2)
write out our polynomial trajectories as the output of a reference dynamical
system with given initial conditions 3) formulate convergence and safety
constraints as Linear Matrix Inequalities (LMIs) on our controller using
Control Lyapunov Functions (CLFs) and Control Barrier Functions (CBFs) and 4)
solve a semi-definite programming (SDP) problem with convergence and safety
constraints imposed to synthesize a controller for each convex cell. Extensive
simulations are included to test our motion planning method under different
initial conditions and different reference trajectories. The synthesized
controller is robust to changes in initial conditions and is always safe
relative to the boundaries of the polygonal environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State Space Models as Foundation Models: A Control Theoretic <span class="highlight-title">Overview</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carmen Amo Alonso, Jerome Sieber, Melanie N. Zeilinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been a growing interest in integrating linear
state-space models (SSM) in deep neural network architectures of foundation
models. This is exemplified by the recent success of Mamba, showing better
performance than the state-of-the-art Transformer architectures in language
tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a
latent space in order to learn a compressed representation of the data. The
same goal has been pursued by control theorists using SSMs to efficiently model
dynamical systems. Therefore, SSMs can be naturally connected to deep sequence
modeling, offering the opportunity to create synergies between the
corresponding research areas. This paper is intended as a gentle introduction
to SSM-based architectures for control theorists and summarizes the latest
research developments. It provides a systematic review of the most successful
SSM proposals and highlights their main features from a control theoretic
perspective. Additionally, we present a comparative analysis of these models,
evaluating their performance on a standardized benchmark designed for assessing
a model's efficiency at learning long sequences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Semi-Lagrangian Approach for Time and Energy Path Planning
  Optimization in Static Flow Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Víctor C. da S. Campos, Armando A. Neto, Douglas G. Macharet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient path planning for autonomous mobile robots is a critical problem
across numerous domains, where optimizing both time and energy consumption is
paramount. This paper introduces a novel methodology that considers the dynamic
influence of an environmental flow field and considers geometric constraints,
including obstacles and forbidden zones, enriching the complexity of the
planning problem. We formulate it as a multi-objective optimal control problem,
propose a novel transformation called Harmonic Transformation, and apply a
semi-Lagrangian scheme to solve it. The set of Pareto efficient solutions is
obtained considering two distinct approaches: a deterministic method and an
evolutionary-based one, both of which are designed to make use of the proposed
Harmonic Transformation. Through an extensive analysis of these approaches, we
demonstrate their efficacy in finding optimized paths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, initial paper submission; Preprint submitted to the IEEE
  Transactions on Intelligent Transportation Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic-Aware Remote Estimation of Multiple Markov Sources Under
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiping Luo, Nikolaos Pappas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies semantic-aware communication for remote estimation of
multiple Markov sources over a lossy and rate-constrained channel. Unlike most
existing studies that treat all source states equally, we exploit the semantics
of information and consider that the remote actuator has different tolerances
for the estimation errors of different states. We aim to find an optimal
scheduling policy that minimizes the long-term state-dependent costs of
estimation errors under a transmission frequency constraint. We theoretically
show the structure of the optimal policy by leveraging the average-cost
Constrained Markov Decision Process (CMDP) theory and the Lagrangian dynamic
programming. By exploiting the optimal structural results, we develop a novel
policy search algorithm, termed intersection search plus relative value
iteration (Insec-RVI), that can find the optimal policy using only a few
iterations. To avoid the ``curse of dimensionality'' of MDPs, we propose an
online low-complexity drift-plus-penalty (DPP) scheduling algorithm based on
the Lyapunov optimization theorem. We also design an efficient average-cost
Q-learning algorithm to estimate the optimal policy without knowing a priori
the channel and source statistics. Numerical results show that continuous
transmission is inefficient, and remarkably, our semantic-aware policies can
attain the optimum by strategically utilizing fewer transmissions by exploiting
the timing of the important information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy Efficiency Optimization Method of WDM Visible Light Communication
  System for Indoor Broadcasting Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayu Shi, Xun Zhang, Ziqi Liu, Xuanbang Chen, Jianghao Li, Xiaodong Liu, William Shieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel approach to optimize energy efficiency in
wavelength division multiplexing (WDM) Visible Light Communication (VLC)
systems designed for indoor broadcasting networks. A physics-based LED model is
integrated into system energy efficiency optimization, enabling quantitative
analysis of the critical issue of VLC energy efficiency: the nonlinear
interplay between illumination and communication performance. The optimization
jointly incorporates constraints on communication quality of each channel, and
illumination performance, standardized by the International Commission on
Illumination (CIE). The formulated nonlinear optimization problem is solved by
the Sequential Quadratic Programming (SQP) algorithm in an experiment-based
simulation. An integrated Red-Green-Blue-Yellow Light Emitting Diode (RGBY-LED)
is measured for model calibration and three different scenarios are simulated
to evaluate the generality of the proposed method. Results demonstrate a double
enhancement in performance and a high versatility in accommodating various
scenarios. Furthermore, it highlights the importance of balancing communication
and illumination imperatives in VLC systems, challenging conventional
perceptions focused solely on minimizing power consumption.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resource and Mobility Management in Hybrid LiFi and WiFi Networks: A
  User-Centric Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Ji, Xiping Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks (HLWNets)
are an emerging indoor wireless communication paradigm, which combines the
advantages of the capacious optical spectra of LiFi and ubiquitous coverage of
WiFi. Meanwhile, load balancing (LB) becomes a key challenge in resource
management for such hybrid networks. The existing LB methods are mostly
network-centric, relying on a central unit to make a solution for the users all
at once. Consequently, the solution needs to be updated for all users at the
same pace, regardless of their moving status. This would affect the network
performance in two aspects: i) when the update frequency is low, it would
compromise the connectivity of fast-moving users; ii) when the update frequency
is high, it would cause unnecessary handovers as well as hefty feedback costs
for slow-moving users. Motivated by this, we investigate user-centric LB which
allows users to update their solutions at different paces. The research is
developed upon our previous work on adaptive target-condition neural network
(ATCNN), which can conduct LB for individual users in quasi-static channels. In
this paper, a deep neural network (DNN) model is designed to enable an adaptive
update interval for each individual user. This new model is termed as
mobility-supporting neural network (MSNN). Associating MSNN with ATCNN, a
user-centric LB framework named mobility-supporting ATCNN (MS-ATCNN) is
proposed to handle resource management and mobility management simultaneously.
Results show that at the same level of average update interval, MS-ATCNN can
achieve a network throughput up to 215\% higher than conventional LB methods
such as game theory, especially for a larger number of users. In addition,
MS-ATCNN costs an ultra low runtime at the level of 100s $\mu$s, which is two
to three orders of magnitude lower than game theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 12 figures, 3 tables, submitted to IEEE TWC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scheduling Power-Intensive Operations of Battery Energy Storage Systems
  and Application to Hybrid Hydropower Plants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Cassano, Fabrizio Sossan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical schedulers for Battery Energy Storage Systems (BESSs) use static
power constraints, assuming that the BESS can provide the rated power at any
State-Of-Charge (SOC) level and that these are representative of the underlying
physical constraints of the system (BESS voltage and current). Static power
constraints, however, can generate unfeasible schedules, especially in
power-intensive applications, as demonstrated in this paper. This paper derives
a set of alternative constraints for the BESS power that are cognizant of the
physical limits of the BESS. It is shown that these constraints, developed by
leveraging an equivalent circuit model of the BESS, can be formulated as linear
inequalities in scheduling problems, thus leaving the properties of the
original problem (i.e., linearity and convexity) unaltered. A comparative
analysis against traditional schedulers from the literature shows significant
reductions in current violations and the generation of feasible schedules.
These findings underscore the crucial role of implementing more advanced power
constraints of BESSs in power-intensive applications, thereby enhancing the
reliability of BESS scheduling strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Yang, Marie Siew, Carlee Joe-Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing prevalence of Cyber-Physical Systems and the Internet of
Things (CPS-IoT) applications and Foundation Models are enabling new
applications that leverage real-time control of the environment. For example,
real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems
can reduce its usage when not needed for the comfort of human occupants, hence
reducing energy consumption. Collecting real-time feedback on human preferences
in such human-in-the-loop (HITL) systems, however, is difficult in practice. We
propose the use of large language models (LLMs) to deal with the challenges of
dynamic environments and difficult-to-obtain data in CPS optimization. In this
paper, we present a case study that employs LLM agents to mimic the behaviors
and thermal preferences of various population groups (e.g. young families, the
elderly) in a shopping mall. The aggregated thermal preferences are integrated
into an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which
employs the LLM as a dynamic simulation of the physical environment to learn
how to balance between energy savings and occupant comfort. Our results show
that LLMs are capable of simulating complex population movements within large
open spaces. Besides, AitL-RL demonstrates superior performance compared to the
popular existing policy of set point control, suggesting that adaptive and
personalized decision-making is critical for efficient optimization in CPS-IoT
applications. Through this case study, we demonstrate the potential of
integrating advanced Foundation Models like LLMs into CPS-IoT to enhance system
adaptability and efficiency. The project's code can be found on our GitHub
repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Workshop on Foundation Models for
  Cyber-Physical Systems & Internet of Things (FMSys) 2024, Co-located at
  CPS-IoT Week 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy Preservation by Intermittent Transmission in Cooperative LQG
  Control Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Lin, Yuqing Ni, Wen Yang, Chao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study a cooperative linear quadratic Gaussian (LQG) control
system with a single user and a server. In this system, the user runs a process
and employs the server to meet the needs of computation. However, the user
regards its state trajectories as privacy. Therefore, we propose a privacy
scheme, in which the user sends data to the server intermittently. By this
scheme, the server's received information of the user is reduced, and
consequently the user's privacy is preserved. In this paper, we consider a
periodic transmission scheme. We analyze the performance of privacy
preservation and LQG control of different transmission periods. Under the given
threshold of the control performance loss, a trade-off optimization problem is
proposed. Finally, we give the solution to the optimization problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DBPF: A Framework for Efficient and Robust Dynamic Bin-Picking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichuan Li, Junkai Zhao, Yixiao Li, Zheng Wu, Rui Cao, Masayoshi Tomizuka, Yunhui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiency and reliability are critical in robotic bin-picking as they
directly impact the productivity of automated industrial processes. However,
traditional approaches, demanding static objects and fixed collisions, lead to
deployment limitations, operational inefficiencies, and process unreliability.
This paper introduces a Dynamic Bin-Picking Framework (DBPF) that challenges
traditional static assumptions. The DBPF endows the robot with the reactivity
to pick multiple moving arbitrary objects while avoiding dynamic obstacles,
such as the moving bin. Combined with scene-level pose generation, the proposed
pose selection metric leverages the Tendency-Aware Manipulability Network
optimizing suction pose determination. Heuristic task-specific designs like
velocity-matching, dynamic obstacle avoidance, and the resight policy, enhance
the picking success rate and reliability. Empirical experiments demonstrate the
importance of these components. Our method achieves an average 84% success
rate, surpassing the 60% of the most comparable baseline, crucially, with zero
collisions. Further evaluations under diverse dynamic scenarios showcase DBPF's
robust performance in dynamic bin-picking. Results suggest that our framework
offers a promising solution for efficient and reliable robotic bin-picking
under dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures. This paper has been accepted by IEEE RA-L on
  2024-03-24. See the supplementary video at youtube:
  https://youtu.be/n5af2VsKhkg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Policy Gradient-based Model Free Optimal LQG Control with a
  Probabilistic Risk Constraint 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arunava Naha, Subhrakanti Dey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate a model-free optimal control design that
minimizes an infinite horizon average expected quadratic cost of states and
control actions subject to a probabilistic risk or chance constraint using
input-output data. In particular, we consider linear time-invariant systems and
design an optimal controller within the class of linear state feedback control.
Three different policy gradient (PG) based algorithms, natural policy gradient
(NPG), Gauss-Newton policy gradient (GNPG), and deep deterministic policy
gradient (DDPG), are developed, and compared with the optimal risk-neutral
linear-quadratic regulator (LQR) and a scenario-based model predictive control
(MPC) technique via numerical simulations. The convergence properties and the
accuracy of all the algorithms are compared numerically. We also establish
analytical convergence properties of the NPG and GNPG algorithms under the
known model scenario, while the proof of convergence for the unknown model
scenario is part of our ongoing work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE CDC2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Blotto Game Approach to Ride-hailing Markets with Electric Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marko Maljkovic, Gustav Nilsson, Nikolas Geroliminis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When a centrally operated ride-hailing company considers to enter a market
already served by another company, it has to make a strategic decision about
how to distribute its fleet among different regions in the area. This decision
will be influenced by the market share the company can secure and the costs
associated with charging the vehicles in each region, all while competing with
the company already operating in the area. In this paper, we propose a Colonel
Blotto-like game to model this decision-making. For the class of games that we
study, we first prove the existence and uniqueness of a Nash Equilibrium.
Subsequently, we provide its general characterization and present an algorithm
for computing the ones in the feasible set's interior. Additionally, for a
simplified scenario involving two regions, which would correspond to a city
area with a downtown and a suburban region, we also provide a method to check
for the equilibria on the feasible set's boundary. Finally, through a numerical
case study, we illustrate the impact of charging prices on the position of the
Nash equilibrium.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper accepted for presentation at the 2024
  European Control Conference (ECC2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Branch and Bound method for the exact parameter identification of the
  PK/PD model for anesthetic drugs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulia Di Credico, Luca Consolini, Mattia Laurini, Marco Locatelli, Marco Milanesi, Michele Schiavo, Antonio Visioli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of parameter identification for the standard
pharmacokinetic/pharmacodynamic (PK/PD) model for anesthetic drugs. Our main
contribution is the development of a global optimization method that guarantees
finding the parameters that minimize the one-step ahead prediction error. The
method is based on a branch-and-bound algorithm, that can be applied to solve a
more general class of nonlinear regression problems. We present some simulation
results, based on a dataset of twelve patients. In these simulations, we are
always able to identify the exact parameters, despite the non-convexity of the
overall identification problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Looking back and forward: A retrospective and future directions on
  Software Engineering for systems-of-systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Everton Cavalcante, Thais Batista, Flavio Oquendo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern systems are increasingly connected and more integrated with other
existing systems, giving rise to systems-of-systems (SoS). An SoS consists of a
set of independent, heterogeneous systems that interact to provide new
functionalities and accomplish global missions through emergent behavior
manifested at runtime. The distinctive characteristics of SoS, when contrasted
to traditional systems, pose significant research challenges within Software
Engineering. These challenges motivate the need for a paradigm shift and the
exploration of novel approaches for designing, developing, deploying, and
evolving these systems. The International Workshop on Software Engineering for
Systems-of-Systems (SESoS) series started in 2013 to fill a gap in scientific
forums addressing SoS from the Software Engineering perspective, becoming the
first venue for this purpose. This article presents a study aimed at outlining
the evolution and future trajectory of Software Engineering for SoS based on
the examination of 57 papers spanning the 11 editions of the SESoS workshop
(2013-2023). The study combined scoping review and scientometric analysis
methods to categorize and analyze the research contributions concerning
temporal and geographic distribution, topics of interest, research
methodologies employed, application domains, and research impact. Based on such
a comprehensive overview, this article discusses current and future directions
in Software Engineering for SoS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A data-based comparison of methods for reducing the peak volume flow
  rate in a district heating system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Agner, Ulrich Trabert, Anders Rantzer, Janybek Orozaliev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work concerns reduction of the peak flow rate of a district heating
grid, a key system property which is bounded by pipe dimensions and pumping
capacity. The peak flow rate constrains the number of additional consumers that
can be connected, and may be a limiting factor in reducing supply temperatures
when transitioning to the 4th generation of district heating. We evaluate a
full year of operational data from a subset of customer meters in a district
heating system in Germany. We consider the peak flow rate reduction that could
be achieved with full a posteriori knowledge of this data. Three strategies for
reducing the peak flow rate are investigated: A load shifting demand response
strategy, an upper limitation in substation return temperatures, and an upper
limitation on each substation's volume flow rate. We show that imposing up to
to 18 % load flexibility for the customers provides an equal reduction in the
peak system flow rate under the load shifting strategy. The limited return
temperature strategy is less efficient at curtailing the peak flow rate, but
provides an overall reduction of volume flow rates. Finally, the flow rate
limitation method can introduce new, higher flow rate peaks, reducing
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIS epidemics on open networks: A replacement-based approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renato Vizuete, Paolo Frasca, Elena Panteley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we analyze continuous-time SIS epidemics subject to arrivals
and departures of agents, by using an approximated process based on
replacements. In defining the SIS dynamics in an open network, we consider a
stochastic setting in which arrivals and departures take place according to
Poisson processes with similar rates, and the new value of the infection
probability of an arriving agent is drawn from a continuous distribution. Since
the system size changes with time, we define an approximated process, in which
replacements take place instead of arrivals and departures, and we focus on the
evolution of an aggregate measure of the level of infection. So long as the
reproduction number is less than one, the long-term behavior of this function
measures the impact of the changes of the set of agents in the epidemic. We
derive upper bounds for the expectation and variance of this function and we
include a numerical example to show that the approximated process is close to
the original SIS process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, to appear in European Control Conference (ECC
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictable Interval MDPs through Entropy Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Menno van Zutphen, Giannis Delimpaltadakis, Maurice Heemels, Duarte Antunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regularization of control policies using entropy can be instrumental in
adjusting predictability of real-world systems. Applications benefiting from
such approaches range from, e.g., cybersecurity, which aims at maximal
unpredictability, to human-robot interaction, where predictable behavior is
highly desirable. In this paper, we consider entropy regularization for
interval Markov decision processes (IMDPs). IMDPs are uncertain MDPs, where
transition probabilities are only known to belong to intervals. Lately, IMDPs
have gained significant popularity in the context of abstracting stochastic
systems for control design. In this work, we address robust minimization of the
linear combination of entropy and a standard cumulative cost in IMDPs, thereby
establishing a trade-off between optimality and predictability. We show that
optimal deterministic policies exist, and devise a value-iteration algorithm to
compute them. The algorithm solves a number of convex programs at each step.
Finally, through an illustrative example we show the benefits of penalizing
entropy in IMDPs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BatDeck: Advancing Nano-drone Navigation with Low-power Ultrasound-based
  Obstacle Avoidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanna Müller, Victor Kartsch, Michele Magno, Luca Benini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nano-drones, distinguished by their agility, minimal weight, and
cost-effectiveness, are particularly well-suited for exploration in confined,
cluttered and narrow spaces. Recognizing transparent, highly reflective or
absorbing materials, such as glass and metallic surfaces is challenging, as
classical sensors, such as cameras or laser rangers, often do not detect them.
Inspired by bats, which can fly at high speeds in complete darkness with the
help of ultrasound, this paper introduces \textit{BatDeck}, a pioneering
sensor-deck employing a lightweight and low-power ultrasonic sensor for
nano-drone autonomous navigation. This paper first provides insights about
sensor characteristics, highlighting the influence of motor noise on the
ultrasound readings, then it introduces the results of extensive experimental
tests for obstacle avoidance (OA) in a diverse environment. Results show that
\textit{BatDeck} allows exploration for a flight time of 8 minutes while
covering 136m on average before crash in a challenging environment with
transparent and reflective obstacles, proving the effectiveness of ultrasonic
sensors for OA on nano-drones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trajectory Planning of Robotic Manipulator in Dynamic Environment
  Exploiting DRL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osama Ahmad, Zawar Hussain, Hammad Naeem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study is about the implementation of a reinforcement learning algorithm
in the trajectory planning of manipulators. We have a 7-DOF robotic arm to pick
and place the randomly placed block at a random target point in an unknown
environment. The obstacle is randomly moving which creates a hurdle in picking
the object. The objective of the robot is to avoid the obstacle and pick the
block with constraints to a fixed timestamp. In this literature, we have
applied a deep deterministic policy gradient (DDPG) algorithm and compared the
model's efficiency with dense and sparse rewards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICIESTR-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbolic and User-friendly Geometric Algebra Routines (SUGAR) for
  Computations in Matlab 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manel Velasco, Isiah Zaplana, Arnau Dória-Cerezo, Pau Martí
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometric algebra (GA) is a mathematical tool for geometric computing,
providing a framework that allows a unified and compact approach to geometric
relations which in other mathematical systems are typically described using
different more complicated elements. This fact has led to an increasing
adoption of GA in applied mathematics and engineering problems. However, the
scarcity of symbolic implementations of GA and its inherent complexity,
requiring a specific mathematical background, make it challenging and less
intuitive for engineers to work with. This prevents wider adoption among more
applied professionals. To address this challenge, this paper introduces SUGAR
(Symbolic and User-friendly Geometric Algebra Routines), an open-source toolbox
designed for Matlab and licensed under the MIT License. SUGAR facilitates the
translation of GA concepts into Matlab and provides a collection of
user-friendly functions tailored for GA computations, including support for
symbolic operations. It supports both numeric and symbolic computations in
high-dimensional GAs. Specifically tailored for applied mathematics and
engineering applications, SUGAR has been meticulously engineered to represent
geometric elements and transformations within two and three-dimensional
projective and conformal geometric algebras, aligning with established
computational methodologies in the literature. Furthermore, SUGAR efficiently
handles functions of multivectors, such as exponential, logarithmic,
sinusoidal, and cosine functions, enhancing its applicability across various
engineering domains, including robotics, control systems, and power
electronics. Finally, this work includes four distinct validation examples,
demonstrating SUGAR's capabilities across the above-mentioned fields and its
practical utility in addressing real-world applied mathematics and engineering
problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 6 figures, journal paper submitted to ACM TOMS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guided Bayesian Optimization: Data-Efficient Controller Tuning with
  Digital Twin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Nobar, Jürg Keller, Alisa Rupenyan, Mohammad Khosravi, John Lygeros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents the guided Bayesian optimization algorithm as an
efficient data-driven method for iteratively tuning closed-loop controller
parameters using an event-triggered digital twin of the system based on
available closed-loop data. We define a controller tuning framework independent
of the controller or the plant structure. Our proposed methodology is
model-free, making it suitable for nonlinear and unmodelled plants with
measurement noise. The objective function consists of performance metrics
modeled by Gaussian processes. We utilize the available information in the
closed-loop system to identify and progressively maintain a digital twin that
guides the optimizer, improving the data efficiency of our method. Switching
the digital twin on and off is triggered by data-driven criteria related to the
digital twin's uncertainty estimations in the BO tuning framework. Effectively,
it replaces much of the exploration of the real system with exploration
performed on the digital twin. We analyze the properties of our method in
simulation and demonstrate its performance on two real closed-loop systems with
different plant and controller structures. The experimental results show that
our method requires fewer experiments on the physical plant than Bayesian
optimization to find the optimal controller parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Adaptive Workplace: Orchestrating Architectural Services around the
  Wellbeing of Individual Occupants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Vande Moere, Sara Arko, Alena Safrova Drasilova, Tomáš Ondráček, Ilaria Pigliautile, Benedetta Pioppi, Anna Laura Pisello, Jakub Prochazka, Paula Acuna Roncancio, Davide Schaumann, Marcel Schweiker, Binh Vinh Duc Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the academic consortia members of the EU Horizon project SONATA
("Situation-aware OrchestratioN of AdapTive Architecture"), we respond to the
workshop call for "Office Wellbeing by Design: Don't Stand for Anything Less"
by proposing the "Adaptive Workplace" concept. In essence, our vision aims to
adapt a workplace to the ever-changing needs of individual occupants, instead
of that occupants are expected to adapt to their workplace.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counter-example guided Imitation Learning of Feedback Controllers from
  Temporal Logic Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thao Dang, Alexandre Donzé, Inzemamul Haque, Nikolaos Kekatos, Indranil Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel method for imitation learning for control requirements
expressed using Signal Temporal Logic (STL). More concretely we focus on the
problem of training a neural network to imitate a complex controller. The
learning process is guided by efficient data aggregation based on
counter-examples and a coverage measure. Moreover, we introduce a method to
evaluate the performance of the learned controller via parameterization and
parameter estimation of the STL requirements. We demonstrate our approach with
a flying robot case study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparsity-Constrained Linear Quadratic Regulation Problem: Greedy
  Approach with Performance Guarantee 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shumpei Nishida, Kunihisa Okano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a linear quadratic regulation problem with a constraint where the
control input can be nonzero only at a limited number of times. Given that this
constraint leads to a combinational optimization problem, we adopt a greedy
method to find a suboptimal solution. To quantify the performance of the greedy
algorithm, we employ two metrics that reflect the submodularity level of the
objective function: The submodularity ratio and curvature. We first present an
explicit form of the optimal control input that is amenable to evaluating these
metrics. Subsequently, we establish bounds on the submodularity ratio and
curvature, which enable us to offer a practical performance guarantee for the
greedy algorithm. The effectiveness of our guarantee is further demonstrated
through numerical simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupling parameter variation from noise: Biquadratic Lyapunov forms in
  data-driven LPV control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Verhoek, Jaap Eising, Florian Dörfler, Roland Tóth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A promising step from linear towards nonlinear data-driven control is via the
design of controllers for linear parameter-varying (LPV) systems, which are
linear systems whose parameters are varying along a measurable scheduling
signal. However, the interplay between uncertainty arising from corrupted data
and the parameter-varying nature of these systems impacts the stability
analysis, and limits the generalization of well-understood data-driven methods
for linear time-invariant systems. In this work, we decouple this interplay
using a recently developed variant of the Fundamental Lemma for LPV systems and
the viewpoint of data-informativity, in combination with biquadratic Lyapunov
forms. Together, these allow us to develop novel linear matrix inequality
conditions for the existence of scheduling-dependent Lyapunov functions,
incorporating the intrinsic nonlinearity. Appealingly, these results are stated
purely in terms of the collected data and bounds on the noise, and they are
computationally favorable to check.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for CDC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid low-dimensional limiting state of charge estimator for multi-cell
  lithium-ion batteries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mira Khalil, Romain Postoyan, Stéphane Raël, Dragan Nešić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The state of charge (SOC) of lithium-ion batteries needs to be accurately
estimated for safety and reliability purposes. For battery packs made of a
large number of cells, it is not always feasible to design one SOC estimator
per cell due to limited computational resources. Instead, only the minimum and
the maximum SOC need to be estimated. The challenge is that the cells having
minimum and maximum SOC typically change over time. In this context, we present
a low-dimensional hybrid estimator of the minimum (maximum) SOC, whose
convergence is analytically guaranteed. We consider for this purpose a battery
consisting of cells interconnected in series, which we model by electric
equivalent circuit models. We then present the hybrid estimator, which runs an
observer designed for a single cell at any time instant, selected by a
switching-like logic mechanism. We establish a practical exponential stability
property for the estimation error on the minimum (maximum) SOC thereby
guaranteeing the ability of the hybrid scheme to generate accurate estimates of
the minimum (maximum) SOC. The analysis relies on non-smooth hybrid Lyapunov
techniques. A numerical illustration is provided to showcase the relevance of
the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatially temporally distributed informative path planning for
  multi-robot systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binh Nguyen, Linh Nguyen, Truong X. Nghiem, Hung La, Jose Baca, Pablo Rangel, Miguel Cid Montoya, Thang Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the problem of informative path planning for a mobile
robotic sensor network in spatially temporally distributed mapping. The robots
are able to gather noisy measurements from an area of interest during their
movements to build a Gaussian Process (GP) model of a spatio-temporal field.
The model is then utilized to predict the spatio-temporal phenomenon at
different points of interest. To spatially and temporally navigate the group of
robots so that they can optimally acquire maximal information gains while their
connectivity is preserved, we propose a novel multistep prediction informative
path planning optimization strategy employing our newly defined local cost
functions. By using the dual decomposition method, it is feasible and practical
to effectively solve the optimization problem in a distributed manner. The
proposed method was validated through synthetic experiments utilizing
real-world data sets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Driven Extrusion Force Control Tuning for 3D Printing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Guidetti, Ankita Mukne, Marvin Rueppel, Yannick Nagel, Efe C. Balta, John Lygeros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of 3D prints often varies due to different conditions inherent to
each print, such as filament type, print speed, and nozzle size. Closed-loop
process control methods improve the accuracy and repeatability of 3D prints.
However, optimal tuning of controllers for given process parameters and design
geometry is often a challenge with manually tuned controllers resulting in
inconsistent and suboptimal results. This work employs Bayesian optimization to
identify the optimal controller parameters. Additionally, we explore transfer
learning in the context of 3D printing by leveraging prior information from
past trials. By integrating optimized extrusion force control and transfer
learning, we provide a novel framework for closed-loop 3D printing and propose
an automated calibration routine that produces high-quality prints for a
desired combination of print settings, material, and shape.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE CASE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Geometric Perspective on Fusing Gaussian Distributions on Lie Groups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiao Ge, Pieter van Goor, Robert Mahony
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic inference on Lie groups plays a key role in state estimation
problems, such as inertial navigation, visual inertial odometry, pose
estimation in virtual reality, etc. A key problem is fusing independent
concentrated Gaussian distributions defined at different reference points on
the group. In this paper we approximate distributions at different points in
the group in a single set of exponential coordinates and then use classical
Gaussian fusion to obtain the fused posteriori in those coordinates. We
consider several approximations including the exact Jacobian of the change of
coordinate map, first and second order Taylor's expansions of the Jacobian, and
parallel transport with and without curvature correction associated with the
underlying geometry of the Lie group. Preliminary results on SO(3) demonstrate
that a novel approximation using parallel transport with curvature correction
achieves similar accuracy to the state-of-the-art optimisation based algorithms
at a fraction of the computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint for L-CSS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Distributionally Robust Model Predictive Control for Static and
  Dynamic Uncertainties in Smart Grids 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Li, Ye Shi, Yuning Jiang, Yuanming Shi, Haoyu Wang, H. Vincent Poor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of various power sources, including renewables and electric
vehicles, into smart grids is expanding, introducing uncertainties that can
result in issues like voltage imbalances, load fluctuations, and power losses.
These challenges negatively impact the reliability and stability of online
scheduling in smart grids. Existing research often addresses uncertainties
affecting current states but overlooks those that impact future states, such as
the unpredictable charging patterns of electric vehicles. To distinguish
between these, we term them static uncertainties and dynamic uncertainties,
respectively. This paper introduces WDR-MPC, a novel approach that stands for
two-stage Wasserstein-based Distributionally Robust (WDR) optimization within a
Model Predictive Control (MPC) framework, aimed at effectively managing both
types of uncertainties in smart grids. The dynamic uncertainties are first
reformulated into ambiguity tubes and then the distributionally robust bounds
of both dynamic and static uncertainties can be established using WDR
optimization. By employing ambiguity tubes and WDR optimization, the stochastic
MPC system is converted into a nominal one. Moreover, we develop a convex
reformulation method to speed up WDR computation during the two-stage
optimization. The distinctive contribution of this paper lies in its holistic
approach to both static and dynamic uncertainties in smart grids. Comprehensive
experiment results on IEEE 38-bus and 94-bus systems reveal the method's
superior performance and the potential to enhance grid stability and
reliability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-informed RL for Maximal Safety Probability Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hikaru Hoshino, Yorie Nakahira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate risk quantification and reachability analysis are crucial for safe
control and learning, but sampling from rare events, risky states, or long-term
trajectories can be prohibitively costly. Motivated by this, we study how to
estimate the long-term safety probability of maximally safe actions without
sufficient coverage of samples from risky states and long-term trajectories.
The use of maximal safety probability in control and learning is expected to
avoid conservative behaviors due to over-approximation of risk. Here, we first
show that long-term safety probability, which is multiplicative in time, can be
converted into additive costs and be solved using standard reinforcement
learning methods. We then derive this probability as solutions of partial
differential equations (PDEs) and propose Physics-Informed Reinforcement
Learning (PIRL) algorithm. The proposed method can learn using sparse rewards
because the physics constraints help propagate risk information through
neighbors. This suggests that, for the purpose of extracting more information
for efficient learning, physics constraints can serve as an alternative to
reward shaping. The proposed method can also estimate long-term risk using
short-term samples and deduce the risk of unsampled states. This feature is in
stark contrast with the unconstrained deep RL that demands sufficient data
coverage. These merits of the proposed method are demonstrated in numerical
simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Adaptation for Condition Monitoring Signal Prediction using
  Label-aware Neural Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhyun Chung, Raed Al Kontar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building a predictive model that rapidly adapts to real-time condition
monitoring (CM) signals is critical for engineering systems/units.
Unfortunately, many current methods suffer from a trade-off between
representation power and agility in online settings. For instance, parametric
methods that assume an underlying functional form for CM signals facilitate
efficient online prediction updates. However, this simplification leads to
vulnerability to model specifications and an inability to capture complex
signals. On the other hand, approaches based on over-parameterized or
non-parametric models can excel at explaining complex nonlinear signals, but
real-time updates for such models pose a challenging task. In this paper, we
propose a neural process-based approach that addresses this trade-off. It
encodes available observations within a CM signal into a representation space
and then reconstructs the signal's history and evolution for prediction. Once
trained, the model can encode an arbitrary number of observations without
requiring retraining, enabling on-the-spot real-time predictions along with
quantified uncertainty and can be readily updated as more online data is
gathered. Furthermore, our model is designed to incorporate qualitative
information (i.e., labels) from individual units. This integration not only
enhances individualized predictions for each unit but also enables joint
inference for both signals and their associated labels. Numerical studies on
both synthetic and real-world data in reliability engineering highlight the
advantageous features of our model in real-time adaptation, enhanced signal
prediction with uncertainty quantification, and joint prediction for labels and
signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fault Localization for Buggy Deep Learning Framework Conversions in
  Image Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06157v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06157v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Louloudakis, Perry Gibson, José Cano, Ajitha Rajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When deploying Deep Neural Networks (DNNs), developers often convert models
from one deep learning framework to another (e.g., TensorFlow to PyTorch).
However, this process is error-prone and can impact target model accuracy. To
identify the extent of such impact, we perform and briefly present a
differential analysis against three DNNs widely used for image recognition
(MobileNetV2, ResNet101, and InceptionV3) converted across four well-known deep
learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which
revealed numerous model crashes and output label discrepancies of up to 100%.
To mitigate such errors, we present a novel approach towards fault localization
and repair of buggy deep learning framework conversions, focusing on
pre-trained image recognition models. Our technique consists of four stages of
analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters,
and 4) graph representation. In addition, we propose various strategies towards
fault repair of the faults detected. We implement our technique on top of the
Apache TVM deep learning compiler, and we test it by conducting a preliminary
fault localization analysis for the conversion of InceptionV3 from TF to
TFLite. Our approach detected a fault in a common DNN converter tool, which
introduced precision errors in weights, reducing model accuracy. After our
fault localization, we repaired the issue, reducing our conversion error to
zero.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeltaNN: Assessing the Impact of Computational Environment Parameters on
  the Performance of Image Recognition Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06208v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06208v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Louloudakis, Perry Gibson, José Cano, Ajitha Rajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image recognition tasks typically use deep learning and require enormous
processing power, thus relying on hardware accelerators like GPUs and TPUs for
fast, timely processing. Failure in real-time image recognition tasks can occur
due to sub-optimal mapping on hardware accelerators during model deployment,
which may lead to timing uncertainty and erroneous behavior. Mapping on
hardware accelerators is done using multiple software components like deep
learning frameworks, compilers, and device libraries, that we refer to as the
computational environment. Owing to the increased use of image recognition
tasks in safety-critical applications like autonomous driving and medical
imaging, it is imperative to assess their robustness to changes in the
computational environment, as the impact of parameters like deep learning
frameworks, compiler optimizations, and hardware devices on model performance
and correctness is not yet well understood.
  In this paper we present a differential testing framework, DeltaNN, that
allows us to assess the impact of different computational environment
parameters on the performance of image recognition models during deployment,
post training. DeltaNN generates different implementations of a given image
recognition model for variations in environment parameters, namely, deep
learning frameworks, compiler optimizations and hardware devices and analyzes
differences in model performance as a result. Using DeltaNN, we conduct an
empirical study of robustness analysis of three popular image recognition
models using the ImageNet dataset. We report the impact in terms of
misclassifications and inference time differences across different settings. In
total, we observed up to 100% output label differences across deep learning
frameworks, and up to 81% unexpected performance degradation in terms of
inference time, when applying compiler optimizations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 10 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long Solution Times or Low Solution Quality: On Trade-Offs in Choosing a
  Power Flow Formulation for the Optimal Power Shutoff Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13843v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13843v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Haag, Noah Rhodes, Line Roald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Optimal Power Shutoff (OPS) problem is an optimization problem that makes
power line de-energization decisions in order to reduce the risk of igniting a
wildfire, while minimizing the load shed of customers. This problem, with DC
linear power flow equations, has been used in many studies in recent years.
However, using linear approximations for power flow when making decisions on
the network topology is known to cause challenges with AC feasibility of the
resulting network, as studied in the related contexts of optimal transmission
switching or grid restoration planning. This paper explores the accuracy of the
DC OPS formulation and the ability to recover an AC-feasible power flow
solution after de-energization decisions are made. We also extend the OPS
problem to include variants with the AC, Second-Order-Cone, and Network-Flow
power flow equations, and compare them to the DC approximation with respect to
solution quality and time. The results highlight that the DC approximation
overestimates the amount of load that can be served, leading to poor
de-energization decisions. The AC and SOC-based formulations are better, but
prohibitively slow to solve for even modestly sized networks thus demonstrating
the need for new solution methods with better trade-offs between computational
time and solution quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Robustness in Cyber-Physical Systems:
  Specification-Centric Analysis in the face of System Deviations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changjian Zhang, Parv Kapoor, Romulo Meira-Goes, David Garlan, Eunsuk Kang, Akila Ganlath, Shatadal Mishra, Nejib Ammar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adoption of cyber-physical systems (CPS) is on the rise in complex
physical environments, encompassing domains such as autonomous vehicles, the
Internet of Things (IoT), and smart cities. A critical attribute of CPS is
robustness, denoting its capacity to operate safely despite potential
disruptions and uncertainties in the operating environment. This paper proposes
a novel specification-based robustness, which characterizes the effectiveness
of a controller in meeting a specified system requirement, articulated through
Signal Temporal Logic (STL) while accounting for possible deviations in the
system. This paper also proposes the robustness falsification problem based on
the definition, which involves identifying minor deviations capable of
violating the specified requirement. We present an innovative two-layer
simulation-based analysis framework designed to identify subtle robustness
violations. To assess our methodology, we devise a series of benchmark problems
wherein system parameters can be adjusted to emulate various forms of
uncertainties and disturbances. Initial evaluations indicate that our
falsification approach proficiently identifies robustness violations, providing
valuable insights for comparing robustness between conventional and
reinforcement learning (RL)-based controllers
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Feedback Law in Stochastic Optimal Nonlinear Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.01041v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.01041v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Naveed Gul Mohamed, Suman Chakravorty, Raman Goyal, Ran Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of nonlinear stochastic optimal control. This problem
is thought to be fundamentally intractable owing to Bellman's ``curse of
dimensionality". We present a result that shows that repeatedly solving an
open-loop deterministic problem from the current state with progressively
shorter horizons, similar to Model Predictive Control (MPC), results in a
feedback policy that is $O(\epsilon^4)$ near to the true global stochastic
optimal policy, \nxx{where $\epsilon$ is a perturbation parameter modulating
the noise.} We show that the optimal deterministic feedback problem has a
perturbation structure in that higher-order terms of the feedback law do not
affect lower-order terms, and that this structure is lost in the optimal
stochastic feedback problem. Consequently, solving the Stochastic Dynamic
Programming problem is highly susceptible to noise, even when tractable, and in
practice, the MPC-type feedback law offers superior performance even for
stochastic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2002.10505,
  arXiv:2002.09478</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Carbon Footprint Reduction for Sustainable Data Centers in Real-Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumyendu Sarkar, Avisek Naug, Ricardo Luna, Antonio Guillen, Vineet Gundecha, Sahand Ghorbanpour, Sajad Mousavi, Dejan Markovikj, Ashwin Ramesh Babu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As machine learning workloads significantly increase energy consumption,
sustainable data centers with low carbon emissions are becoming a top priority
for governments and corporations worldwide. This requires a paradigm shift in
optimizing power consumption in cooling and IT loads, shifting flexible loads
based on the availability of renewable energy in the power grid, and leveraging
battery storage from the uninterrupted power supply in data centers, using
collaborative agents. The complex association between these optimization
strategies and their dependencies on variable external factors like weather and
the power grid carbon intensity makes this a hard problem. Currently, a
real-time controller to optimize all these goals simultaneously in a dynamic
real-world setting is lacking. We propose a Data Center Carbon Footprint
Reduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that
optimizes data centers for the multiple objectives of carbon footprint
reduction, energy consumption, and energy cost. The results show that the
DC-CFR MARL agents effectively resolved the complex interdependencies in
optimizing cooling, load shifting, and energy storage in real-time for various
locations under real-world dynamic weather and grid carbon intensity
conditions. DC-CFR significantly outperformed the industry standard ASHRAE
controller with a considerable reduction in carbon emissions (14.5%), energy
usage (14.4%), and energy cost (13.7%) when evaluated over one year across
multiple geographical regions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identification of Energy Management Configuration Concepts from a Set of
  Pareto-optimal Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Lanfermann, Qiqi Liu, Yaochu Jin, Sebastian Schmitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implementing resource efficient energy management systems in facilities and
buildings becomes increasingly important in the transformation to a sustainable
society. However, selecting a suitable configuration based on multiple,
typically conflicting objectives, such as cost, robustness with respect to
uncertainty of grid operation, or renewable energy utilization, is a difficult
multi-criteria decision making problem. The recently developed concept
identification technique can facilitate a decision maker by sorting
configuration options into semantically meaningful groups (concepts). In this
process, the partitioning of the objectives and design parameters into
different sets (called description spaces) is a very important step. In this
study we focus on utilizing the concept identification technique for finding
relevant and viable energy management configurations from a very large data set
of Pareto-optimal solutions. The data set consists of 20000 realistic
Pareto-optimal building energy management configurations generated by a
many-objective evolutionary optimization of a high quality Digital Twin energy
management simulator. We analyze how the choice of description spaces, i.e.,
the partitioning of the objectives and parameters, impacts the type of
information that can be extracted. We show that the decision maker can
introduce constraints and biases into that process to meet expectations and
preferences. The iterative approach presented in this work allows for the
generation of valuable insights into trade-offs between specific objectives,
and constitutes a powerful and flexible tool to support the decision making
process when designing large and complex energy management systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures, accepted at Energy Conversion and Management: X</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid physics-informed metabolic cybergenetics: process rates augmented
  with machine-learning surrogates informed by flux balance analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastián Espinel-Ríos, José L. Avalos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metabolic cybergenetics is a promising concept that interfaces gene
expression and cellular metabolism with computers for real-time dynamic
metabolic control. The focus is on control at the transcriptional level,
serving as a means to modulate intracellular metabolic fluxes. Recent
strategies in this field have employed constraint-based dynamic models for
process optimization, control, and estimation. However, this results in bilevel
dynamic optimization problems, which pose considerable numerical and conceptual
challenges. In this study, we present an alternative hybrid physics-informed
dynamic modeling framework for metabolic cybergenetics, aimed at simplifying
optimization, control, and estimation tasks. By utilizing machine-learning
surrogates, our approach effectively embeds the physics of metabolic networks
into the process rates of structurally simpler macro-kinetic models coupled
with gene expression. These surrogates, informed by flux balance analysis, link
the domains of manipulatable intracellular enzymes to metabolic exchange
fluxes. This ensures that critical knowledge captured by the system's metabolic
network is preserved. The resulting models can be integrated into metabolic
cybergenetic schemes involving single-level optimizations. Additionally, the
hybrid modeling approach maintains the number of system states at a necessary
minimum, easing the burden of process monitoring and estimation. Our hybrid
physics-informed metabolic cybergenetic framework is demonstrated using a
computational case study on the optogenetically-assisted production of
itaconate by $\textit{Escherichia coli}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 10 figures, journal submission (reviewed/accepted version)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Peak Estimation of Rational Systems using Convex Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jared Miller, Roy S. Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents algorithms that upper-bound the peak value of a state
function along trajectories of a continuous-time system with rational dynamics.
The finite-dimensional but nonconvex peak estimation problem is cast as a
convex infinite-dimensional linear program in occupation measures. This
infinite-dimensional program is then truncated into finite-dimensions using the
moment-Sum-of-Squares (SOS) hierarchy of semidefinite programs. Prior work on
treating rational dynamics using the moment-SOS approach involves clearing
dynamics to common denominators or adding lifting variables to handle
reciprocal terms under new equality constraints. Our solution method uses a
sum-of-rational method based on absolute continuity of measures. The Moment-SOS
truncations of our program possess lower computational complexity and
(empirically demonstrated) higher accuracy of upper bounds on example systems
as compared to prior approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Integral Consensus Control of Multi-Agent Networks Perturbed by
  Matched and Unmatched Disturbances: The Case of Directed Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Guadalupe Romero, David Navarro-Alarcon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a new method to design consensus controllers for perturbed
double integrator systems whose interconnection is described by a directed
graph containing a rooted spanning tree. We propose new robust controllers to
solve the consensus and synchronization problems when the systems are under the
effects of matched and unmatched disturbances. In both problems, we present
simple continuous controllers, whose integral actions allow us to handle the
disturbances. A rigorous stability analysis based on Lyapunov's direct method
for unperturbed networked systems is presented. To assess the performance of
our result, a representative simulation study is presented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Ranging and Phase Offset Estimation for Multiple Drones using
  ADS-B Signatures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Mohammadkarimi, Geert Leus, Raj Thilak Rajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A new method for joint ranging and Phase Offset (PO) estimation of multiple
drones/aircrafts is proposed in this paper. The proposed method employs the
superimposed uncoordinated Automatic Dependent Surveillance Broadcast (ADS-B)
packets broadcasted by drones/aircrafts for joint range and PO estimation. It
jointly estimates range and PO prior to ADS-B packet decoding; thus, it can
improve air safety when packet decoding is infeasible due to packet collision.
Moreover, it enables coherent detection of ADS-B packets, which can result in
more reliable multiple target tracking in aviation systems using cooperative
sensors for detect and avoid (DAA). By minimizing the Kullback Leibler
Divergence (KLD) statistical distance measure, we show that the received
complex baseband signal coming from K uncoordinated drones corrupted by
Additive White Gaussian Noise (AWGN) at a single antenna receiver can be
approximated by an independent and identically distributed Gaussian Mixture
(GM) with 2 power K mixture components in the two dimensional (2D) plane. While
direct joint Maximum Likelihood Estimation (MLE) of range and PO from the
derived GM Probability Density Function (PDF) leads to an intractable
maximization, our proposed method employs the Expectation Maximization (EM)
algorithm to estimate the modes of the 2D Gaussian mixture followed by a
reordering estimation technique through combinatorial optimization to estimate
range and PO. An extension to a multiple antenna receiver is also investigated
in this paper. While the proposed estimator can estimate the range of multiple
drones with a single receive antenna, a larger number of drones can be
supported with higher accuracy by the use of multiple antennas at the receiver.
The effectiveness of the proposed estimator is supported by simulation results.
We show that the proposed estimator can jointly estimate the range of three
drones accurately.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PowerSimulationsDynamics.jl -- An Open Source Modeling Package for
  Modern Power Systems with Inverter-Based Resources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02921v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02921v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Daniel Lara, Rodrigo Henriquez-Auba, Matthew Bossart, Duncan S. Callaway, Clayton Barrows
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present the development of an open-source simulation
toolbox, PowerSimulationsDynamics.jl, to study the dynamic response of power
systems, focusing on the requirements to model systems with high penetrations
of Inverter-Based Resources (IBRs). PowerSimulationsDynamics.jl is implemented
in Julia and features a rich library of synchronous generator, inverter, and
load models. In addition, it allows the study of quasi-static phasors and
electromagnetic dq models that use a dynamic network representation. Case
studies and validation exercises show that PowerSimulationsDynamics.jl results
closely match other commercial and open-source simulation tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Secure Control of Connected and Automated Vehicles Using Trust-Aware
  Robust Event-Triggered Control Barrier Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02306v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02306v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H M Sabbir Ahmad, Ehsan Sabouni, Akua Dickson, Wei Xiao, Christos G. Cassandras, Wenchao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the security of a network of Connected and Automated Vehicles
(CAVs) cooperating to safely navigate through a conflict area (e.g., traffic
intersections, merging roadways, roundabouts). Previous studies have shown that
such a network can be targeted by adversarial attacks causing traffic jams or
safety violations ending in collisions. We focus on attacks targeting the V2X
communication network used to share vehicle data and consider as well
uncertainties due to noise in sensor measurements and communication channels.
To combat these, motivated by recent work on the safe control of CAVs, we
propose a trust-aware robust event-triggered decentralized control and
coordination framework that can provably guarantee safety. We maintain a trust
metric for each vehicle in the network computed based on their behavior and
used to balance the tradeoff between conservativeness (when deeming every
vehicle as untrustworthy) and guaranteed safety and security. It is important
to highlight that our framework is invariant to the specific choice of the
trust framework. Based on this framework, we propose an attack detection and
mitigation scheme which has twofold benefits: (i) the trust framework is immune
to false positives, and (ii) it provably guarantees safety against false
positive cases. We use extensive simulations (in SUMO and CARLA) to validate
the theoretical guarantees and demonstrate the efficacy of our proposed scheme
to detect and mitigate adversarial attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2305.16818</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thermally-Resilient Soft Gripper for On-Orbit Operations <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Ruiz, Begona Arrue, Anibal Ollero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in soft manipulators has significantly enhanced object grasping
capabilities, thanks to their adaptability to various shapes and sizes.
Applying this technology to on-orbit servicing, especially during the capture
and containment stages of active space debris removal missions, might offer a
secure, adaptable, and cost-effective solution compared to the trend of
increasing the degrees of freedom and complexity of the manipulator (e.g.
ClearSpace, Astroscale). This work aims to conduct an experimental proof of
concept, for which challenges such as radiation, vacuum, and microgravity are
significant, but the predominant issue is ensuring effective operation in the
extreme temperature swings, where flexible materials may exhibit cryogenic
crystallization or drastic shifts in their elasticity. This work addresses this
challenge through an initial stage of analytical modeling of the thermal
dynamics inside the manipulator in orbit; which is then used for the
development of a first experimental prototype tested with liquid nitrogen and
heat guns. The multi-layered design for Low Earth Orbit (LEO) leverages the
properties of TPU at low infill rates for lightweight inherent flexibility,
silicone rubber ensuring structural integrity, PTFE (Teflon) for unparalleled
thermal stability, and aerogel for insulation. The tendon-actuated servo-driven
gripper is tested in the laboratory by varying the shape and size of objects
during the grasping. The results, based on servomotor force metrics to assess
the flexible manipulator's adaptability and object capture efficiency across
temperature changes, affirm the concept's viability. Forces increase up to
220$\%$ in cryogenic conditions and decrease by no more than 50$\%$ at high
temperatures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Directionality-Aware Mixture Model Parallel Sampling for Efficient
  Linear Parameter Varying Dynamical System Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02609v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02609v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunan Sun, Haihui Gao, Tianyu Li, Nadia Figueroa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Linear Parameter Varying Dynamical System (LPV-DS) is an effective
approach that learns stable, time-invariant motion policies using statistical
modeling and semi-definite optimization to encode complex motions for reactive
robot control. Despite its strengths, the LPV-DS learning approach faces
challenges in achieving a high model accuracy without compromising the
computational efficiency. To address this, we introduce the
Directionality-Aware Mixture Model (DAMM), a novel statistical model that
applies the Riemannian metric on the n-sphere $\mathbb{S}^n$ to efficiently
blend non-Euclidean directional data with $\mathbb{R}^m$ Euclidean states.
Additionally, we develop a hybrid Markov chain Monte Carlo technique that
combines Gibbs Sampling with Split/Merge Proposal, allowing for parallel
computation to drastically speed up inference. Our extensive empirical tests
demonstrate that LPV-DS integrated with DAMM achieves higher reproduction
accuracy, better model efficiency, and near real-time/online learning compared
to standard estimation methods on various datasets. Lastly, we demonstrate its
suitability for incrementally learning multi-behavior policies in real-world
robot experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable Reduced-Rank VAR Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhui Rong, Victor Solo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vector autoregression (VAR) has been widely used in system
identification, econometrics, natural science, and many other areas. However,
when the state dimension becomes large the parameter dimension explodes. So
rank reduced modelling is attractive and is well developed. But a fundamental
requirement in almost all applications is stability of the fitted model. And
this has not been addressed in the rank reduced case. Here, we develop, for the
first time, a closed-form formula for an estimator of a rank reduced transition
matrix which is guaranteed to be stable. We show that our estimator is
consistent and asymptotically statistically efficient and illustrate it in
comparative simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Logic in Computer Science
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chase Termination Beyond Polynomial Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Hanisch, Markus Krötzsch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The chase is a widely implemented approach to reason with tuple-generating
dependencies (tgds), used in data exchange, data integration, and
ontology-based query answering. However, it is merely a semi-decision
procedure, which may fail to terminate. Many decidable conditions have been
proposed for tgds to ensure chase termination, typically by forbidding some
kind of "cycle" in the chase process. We propose a new criterion that
explicitly allows some such cycles, and yet ensures termination of the standard
chase under reasonable conditions. This leads to new decidable fragments of
tgds that are not only syntactically more general but also strictly more
expressive than the fragments defined by prior acyclicity conditions. Indeed,
while known terminating fragments are restricted to PTime data complexity, our
conditions yield decidable languages for any k-ExpTime. We further refine our
syntactic conditions to obtain fragments of tgds for which an optimised chase
procedure decides query entailment in PSpace or k-ExpSpace, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formally Verifying the Safety of Pipelined Moonshot Consensus Protocol 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Praveen, Raghavendra Ramesh, Isaac Doidge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized Finance (DeFi) has emerged as a contemporary competitive as
well as complementary to traditional centralized finance systems. As of 23rd
January 2024, per Defillama approximately USD 55 billion is the total value
locked on the DeFi applications on all blockchains put together.
  A Byzantine Fault Tolerant (BFT) State Machine Replication (SMR) protocol,
popularly known as the consensus protocol, is the central component of a
blockchain. If forks are possible in a consensus protocol, they can be misused
to carry out double spending attacks and can be catastrophic given high volumes
of finance that are transacted on blockchains. Formal verification of the
safety of consensus protocols is the golden standard for guaranteeing that
forks are not possible. However, it is considered complex and challenging to
do. This is reflected by the fact that not many complex consensus protocols are
formally verified except for Tendermint and QBFT.
  We focus on Supra's Pipelined Moonshot consensus protocol. Similar to
Tendermint's formal verification, we too model Pipelined Moonshot using IVy and
formally prove that for all network sizes, as long as the number of Byzantine
validators is less than one thirds, the protocol does not allow forks, thus
proving that Pipelined Moonshot is safe and double spending cannot be done
using forks. The IVy model and proof of safety is available on Github.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An incremental MaxSAT-based model to learn balanced rules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antônio Carlos Souza Ferreira Júnior, Thiago Alves Rocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing advancements in the field of machine learning have led to the
development of numerous applications that effectively address a wide range of
problems with accurate predictions. However, in certain cases, accuracy alone
may not be sufficient. Many real-world problems also demand explanations and
interpretability behind the predictions. One of the most popular interpretable
models that are classification rules. This work aims to propose an incremental
model for learning interpretable and balanced rules based on MaxSAT, called
IMLIB. This new model was based on two other approaches, one based on SAT and
the other on MaxSAT. The one based on SAT limits the size of each generated
rule, making it possible to balance them. We suggest that such a set of rules
seem more natural to be understood compared to a mixture of large and small
rules. The approach based on MaxSAT, called IMLI, presents a technique to
increase performance that involves learning a set of rules by incrementally
applying the model in a dataset. Finally, IMLIB and IMLI are compared using
diverse databases. IMLIB obtained results comparable to IMLI in terms of
accuracy, generating more balanced rules with smaller sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 tables, submitted to BRACIS 2023 (Brazilian Conference on
  Intelligent Systems), accepted version published in Intelligent Systems,
  LNCS, vol 14195</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On a fibrational construction for optics, lenses, and Dialectica
  categories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Capucci, Bruno Gavranović, Abdullah Malik, Francisco Rios, Jonathan Weinberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Categories of lenses/optics and Dialectica categories are both comprised of
bidirectional morphisms of basically the same form. In this work we show how
they can be considered a special case of an overarching fibrational
construction, generalizing Hofstra's construction of Dialectica fibrations and
Spivak's construction of generalized lenses. This construction turns a tower of
Grothendieck fibrations into another tower of fibrations by iteratively
twisting each of the components, using the opposite fibration construction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pp. Project results from the American Mathematical Society's Math
  Research Community on Applied Category Theory 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Robustness in Cyber-Physical Systems:
  Specification-Centric Analysis in the face of System Deviations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changjian Zhang, Parv Kapoor, Romulo Meira-Goes, David Garlan, Eunsuk Kang, Akila Ganlath, Shatadal Mishra, Nejib Ammar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adoption of cyber-physical systems (CPS) is on the rise in complex
physical environments, encompassing domains such as autonomous vehicles, the
Internet of Things (IoT), and smart cities. A critical attribute of CPS is
robustness, denoting its capacity to operate safely despite potential
disruptions and uncertainties in the operating environment. This paper proposes
a novel specification-based robustness, which characterizes the effectiveness
of a controller in meeting a specified system requirement, articulated through
Signal Temporal Logic (STL) while accounting for possible deviations in the
system. This paper also proposes the robustness falsification problem based on
the definition, which involves identifying minor deviations capable of
violating the specified requirement. We present an innovative two-layer
simulation-based analysis framework designed to identify subtle robustness
violations. To assess our methodology, we devise a series of benchmark problems
wherein system parameters can be adjusted to emulate various forms of
uncertainties and disturbances. Initial evaluations indicate that our
falsification approach proficiently identifies robustness violations, providing
valuable insights for comparing robustness between conventional and
reinforcement learning (RL)-based controllers
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Free Doubly-Infinitary Distributive Categories are Cartesian Closed 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10447v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10447v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Lucatelli Nunes, Matthijs Vákár
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We delve into the concept of categories with products that distribute over
coproducts, which we call doubly-infinitary distributive categories. We show
various instances of doubly-infinitary distributive categories aiming for a
comparative analysis with established notions such as extensivity, infinitary
distributiveness, and cartesian closedness. Our exploration reveals that this
condition represents a substantial extension beyond the classical understanding
of infinitary distributive categories. Our main theorem establishes that free
doubly-infinitary distributive categories are cartesian closed. We end the
paper with remarks on non-canonical isomorphisms, open questions, and future
work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, minor, typos, Cantor space</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Box Embeddings for the Description Logic EL++ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11118v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11118v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathias Jackermeier, Jiaoyan Chen, Ian Horrocks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OWL ontologies, whose formal semantics are rooted in Description Logic (DL),
have been widely used for knowledge representation. Similar to Knowledge Graphs
(KGs), ontologies are often incomplete, and maintaining and constructing them
has proved challenging. While classical deductive reasoning algorithms use the
precise formal semantics of an ontology to predict missing facts, recent years
have witnessed growing interest in inductive reasoning techniques that can
derive probable facts from an ontology. Similar to KGs, a promising approach is
to learn ontology embeddings in a latent vector space, while additionally
ensuring they adhere to the semantics of the underlying DL. While a variety of
approaches have been proposed, current ontology embedding methods suffer from
several shortcomings, especially that they all fail to faithfully model
one-to-many, many-to-one, and many-to-many relations and role inclusion axioms.
To address this problem and improve ontology completion performance, we propose
a novel ontology embedding method named Box$^2$EL for the DL EL++, which
represents both concepts and roles as boxes (i.e., axis-aligned
hyperrectangles), and models inter-concept relationships using a bumping
mechanism. We theoretically prove the soundness of Box$^2$EL and conduct an
extensive experimental evaluation, achieving state-of-the-art results across a
variety of datasets on the tasks of subsumption prediction, role assertion
prediction, and approximating deductive reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated license information</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computational Synthetic Cohomology Theory in Homotopy Type Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Ljungström, Anders Mörtberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper discusses the development of synthetic cohomology in Homotopy Type
Theory (HoTT), as well as its computer formalisation. The objectives of this
paper are (1) to generalise previous work on integral cohomology in HoTT by the
current authors and Brunerie (2022) to cohomology with arbitrary coefficients
and (2) to provide the mathematical details of, as well as extend, results
underpinning the computer formalisation of cohomology rings by the current
authors and Lamiaux (2023). With respect to objective (1), we provide new
direct definitions of the cohomology group operations and of the cup product,
which, just as in (Brunerie et al., 2022), enable significant simplifications
of many earlier proofs in synthetic cohomology theory. In particular, the new
definition of the cup product allows us to give the first complete
formalisation of the axioms needed to turn the cohomology groups into a graded
commutative ring. We also establish that this cohomology theory satisfies the
HoTT formulation of the Eilenberg-Steenrod axioms for cohomology and study the
classical Mayer-Vietoris and Gysin sequences. With respect to objective (2), we
characterise the cohomology groups and rings of various spaces, including the
spheres, torus, Klein bottle, real/complex projective planes, and infinite real
projective space. All results have been formalised in Cubical Agda and we
obtain multiple new numbers, similar to the famous `Brunerie number', which can
be used as benchmarks for computational implementations of HoTT. Some of these
numbers are infeasible to compute in Cubical Agda and hence provide new
computational challenges and open problems which are much easier to define than
the original Brunerie number.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: minor typos, updated acknowledgements</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forcing with Language Fragments, Extending Namba Forcing, and Models of
  Theories with Constraints in Interpretation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01213v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01213v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Desmond Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a forcing framework based on the idea of amalgamating language
fragments into a theory with a canonical Henkin model. We then demonstrate the
usefulness of this framework by applying it to both the extended Namba problem
and the analysis of models of certain theories with constraints in
interpretation (TCIs). The foundations for a theory of TCIs and their models
are laid in parallel to the development of our framework, and are of
independent interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>95 pages. Strengthened Lemma 3.29, added details to some proofs and
  refined the language of others</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Orthologic of Epistemic Modals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.02872v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.02872v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wesley H. Holliday, Matthew Mandelkern
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Epistemic modals have peculiar logical features that are challenging to
account for in a broadly classical framework. For instance, while a sentence of
the form $p\wedge\Diamond\neg p$ ('$p$, but it might be that not $p$') appears
to be a contradiction, $\Diamond\neg p$ does not entail $\neg p$, which would
follow in classical logic. Likewise, the classical laws of distributivity and
disjunctive syllogism fail for epistemic modals. Existing attempts to account
for these facts generally either under- or over-correct. Some predict that
$p\wedge\Diamond\neg p$, a so-called epistemic contradiction, is a
contradiction only in an etiolated sense, under a notion of entailment that
does not always allow us to replace $p\wedge\Diamond\neg p$ with a
contradiction; these theories underpredict the infelicity of embedded epistemic
contradictions. Other theories savage classical logic, eliminating not just
rules that intuitively fail but also rules like non-contradiction, excluded
middle, De Morgan's laws, and disjunction introduction, which intuitively
remain valid for epistemic modals. In this paper, we aim for a middle ground,
developing a semantics and logic for epistemic modals that makes epistemic
contradictions genuine contradictions and that invalidates distributivity and
disjunctive syllogism but that otherwise preserves classical laws that
intuitively remain valid. We start with an algebraic semantics, based on
ortholattices instead of Boolean algebras, and then propose a more concrete
possibility semantics, based on partial possibilities related by compatibility.
Both semantics yield the same consequence relation, which we axiomatize. We
then show how to lift an arbitrary possible worlds model for a non-modal
language to a possibility model for a language with epistemic modals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor edits for final version forthcoming in Journal of Philosophical
  Logic</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modal logic, fundamentally 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wesley H. Holliday
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-classical generalizations of classical modal logic have been developed in
the contexts of constructive mathematics and natural language semantics. In
this paper, we discuss a general approach to the semantics of non-classical
modal logics via algebraic representation theorems. We begin with complete
lattices $L$ equipped with an antitone operation $\neg$ sending $1$ to $0$, a
completely multiplicative operation $\Box$, and a completely additive operation
$\Diamond$. Such lattice expansions can be represented by means of a set $X$
together with binary relations $\vartriangleleft$, $R$, and $Q$, satisfying
some first-order conditions, used to represent $(L,\neg)$, $\Box$, and
$\Diamond$, respectively. Indeed, any lattice $L$ equipped with such a $\neg$,
a multiplicative $\Box$, and an additive $\Diamond$ embeds into the lattice of
propositions of a frame $(X,\vartriangleleft,R,Q)$. Building on our recent
study of "fundamental logic", we focus on the case where $\neg$ is dually
self-adjoint ($a\leq \neg b$ implies $b\leq\neg a$) and $\Diamond \neg
a\leq\neg\Box a$. In this case, the representations can be constrained so that
$R=Q$, i.e., we need only add a single relation to $(X,\vartriangleleft)$ to
represent both $\Box$ and $\Diamond$. Using these results, we prove that a
system of fundamental modal logic is sound and complete with respect to an
elementary class of bi-relational structures $(X,\vartriangleleft, R)$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fixed typos. 22 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Typed compositional quantum computation with lenses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14347v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14347v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacques Garrigue, Takafumi Saikawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a type-theoretic framework for describing and proving properties
of quantum computations, in particular those presented as quantum circuits. Our
proposal is based on an observation that, in the polymorphic type system of
Coq, currying on quantum states allows us to apply quantum gates directly
inside a complex circuit. By introducing a discrete notion of lens to control
this currying, we are further able to separate the combinatorics of the circuit
structure from the computational content of gates. We apply our development to
define quantum circuits recursively from the bottom up, and prove their
correctness compositionally.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-24T00:00:00Z">2024-03-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Signal Processing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Site-Specific Beam Alignment in 6G via Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqiang Heng, Yu Zhang, Ahmed Alkhateeb, Jeffrey G. Andrews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beam alignment (BA) in modern millimeter wave standards such as 5G NR and
WiGig (802.11ay) is based on exhaustive and/or hierarchical beam searches over
pre-defined codebooks of wide and narrow beams. This approach is slow and
bandwidth/power-intensive, and is a considerable hindrance to the wide
deployment of millimeter wave bands. A new approach is needed as we move
towards 6G. BA is a promising use case for deep learning (DL) in the 6G air
interface, offering the possibility of automated custom tuning of the BA
procedure for each cell based on its unique propagation environment and user
equipment (UE) location patterns. We overview and advocate for such an approach
in this paper, which we term site-specific beam alignment (SSBA). SSBA largely
eliminates wasteful searches and allows UEs to be found much more quickly and
reliably, without many of the drawbacks of other machine learning-aided
approaches. We first overview and demonstrate new results on SSBA, then
identify the key open challenges facing SSBA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the IEEE Communications Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fusion of Active and Passive Measurements for Robust and Scalable
  Positioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Zhu, Alexander Venus, Erik Leitinger, Stefan Tertinek, Klaus Witrisal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of achieving reliable and robust
positioning of a mobile agent, such as a radio device carried by a person, in
scenarios where direct line-of-sight (LOS) links are obstructed or unavailable.
The human body is considered as an extended object that scatters, attenuates
and blocks the radio signals. We propose a novel particle-based sum-product
algorithm (SPA) that fuses active measurements between the agent and anchors
with passive measurements from pairs of anchors reflected off the body. We
first formulate radio signal models for both active and passive measurements.
Then, a joint tracking algorithm that utilizes both active and passive
measurements is developed for the extended object. The algorithm exploits the
probabilistic data association (PDA) for multiple object-related measurements.
The results demonstrate superior accuracy during and after the obstructed
line-of-sight (OLOS) situation, outperforming conventional methods that solely
rely on active measurements. The proposed joint estimation approach
significantly enhances the localization robustness via radio sensing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Secrecy Enhancement of an Integrated Ground-Aerial Network with a
  Hybrid FSO/THz Feeder Link 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elmehdi Illi, Marwa Qaraqe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High altitude platforms (HAPs)-aided terrestrial-aerial communication
technology based on free-space optical (FSO) and Terahertz (THz) feeder links
has been attracting notable interest recently due to its great potential in
reaching a higher data rate and connectivity. Nonetheless, the presence of
harsh vertical propagation environments and potential aerial eavesdroppers are
two of the main challenges limiting the reliability and security of such a
technology. In this work, a secrecy-enhancing scheme for HAP-aided
ground-aerial communication is proposed. The considered network consists of
HAP-assisted communication between a ground station and a legitimate user under
the threat of an aerial and ground eavesdropper. Thus, the proposed scheme
leverages (i) HAP diversity by exploiting the presence of multiple flying HAPs
and (ii) the use of a hybrid FSO/THz transmission scheme to offer better
resilience against eavesdropping attacks. An analytical secrecy outage
probability (SOP) expression is derived for the scheme in consideration.
Results manifest the notable gain in security of the proposed scheme with
respect to both (i) the single-HAP and (ii) THz feeder-based benchmark ones,
where the proposed scheme's SOP is decreased by four orders of magnitude using
$4$ HAPs with respect to the first benchmark scheme, while a $5$-dB secrecy
gain is manifested with respect to the second benchmark one.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Holography inspired self-controlled reconfigurable intelligent surface 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieao Zhu, Ze Gu, Qian Ma, Linglong Dai, Tie Jun Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Among various promising candidate technologies for the sixth-generation (6G)
wireless communications, recent advances in microwave metasurfaces have sparked
a new research area of reconfigurable intelligent surfaces (RISs). By
controllably reprogramming the wireless propagation channel, RISs are
envisioned to achieve low-cost wireless capacity boosting, coverage extension,
and enhanced energy efficiency. To reprogram the channel, each meta-atom on RIS
needs an external control signal, which is usually generated by base station
(BS). However, BS-controlled RISs require complicated control cables, which
hamper their massive deployments. Here, we eliminate the need for BS control by
proposing a self-controlled RIS (SC-RIS), which is inspired by the optical
holography principle. Different from the existing BS-controlled RISs, each
meta-atom of SC-RIS is integrated with an additional power detector for
holographic recording. By applying the classical Fourier-transform processing
to the measured hologram, SC-RIS is capable of retrieving the user's channel
state information required for beamforming, thus enabling autonomous RIS
beamforming without control cables. Owing to this WiFi-like plug-and-play
capability without the BS control, SC-RISs are expected to enable easy and
massive deployments in the future 6G systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Traditional BS-controlled RISs suffer from complicated control
  cables. To "cut" the control cables, we propose a self-controlled RIS by
  leveraging the holographic interference principle, thus realizing autonomous
  RIS beamforming</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing cognitive function among older adults using machine learning
  and wearable device data: a feasibility study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Collin Sakal, Tingyou Li, Juan Li, Xinyue Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Timely implementation of interventions to slow cognitive decline among older
adults requires accurate monitoring to detect changes in cognitive function.
Data gathered using wearable devices that can continuously monitor factors
known to be associated with cognition could be used to train machine learning
models and develop wearable-based cognitive monitoring systems. Using data from
over 2,400 older adults in the National Health and Nutrition Examination Survey
(NHANES) we developed prediction models to differentiate older adults with
normal cognition from those with poor cognition based on outcomes from three
cognitive tests measuring different domains of cognitive function. During
repeated cross-validation, CatBoost, XGBoost, and Random Forest models
performed best when predicting cognition based on processing speed, working
memory, and attention (median AUCs >0.82) compared to immediate and delayed
recall (median AUCs >0.72) and categorical verbal fluency (median AUC >0.68).
Activity and sleep parameters were also more strongly associated with
processing speed, working memory, and attention compared to other cognitive
subdomains. Our work provides proof of concept that wearable-based cognitive
monitoring systems may be a viable alternative to traditional methods for
monitoring processing speeds, working memory, and attention. We further
identified novel metrics that could be targets in future causal studies seeking
to better understand how sleep and activity parameters influence cognitive
function among older adults.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iterative execution of discrete and inverse discrete Fourier transforms
  with applications for signal denoising via sparsification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09284v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09284v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. Robert Frost
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe a family of iterative algorithms that involve the repeated
execution of discrete and inverse discrete Fourier transforms. One interesting
member of this family is motivated by the discrete Fourier transform
uncertainty principle and involves the application of a sparsification
operation to both the real domain and frequency domain data with convergence
obtained when real domain sparsity hits a stable pattern. This sparsification
variant has practical utility for signal denoising, in particular the recovery
of a periodic spike signal in the presence of Gaussian noise. General
convergence properties and denoising performance relative to existing methods
are demonstrated using simulation studies. An R package implementing this
technique and related resources can be found at
https://hrfrost.host.dartmouth.edu/IterativeFT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modulation and Estimation with a Helper 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04277v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04277v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anatoly Khina, Neri Merhav
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of transmitting a parameter value over an additive white Gaussian
noise (AWGN) channel is considered, where, in addition to the transmitter and
the receiver, there is a helper that observes the noise non-causally and
provides a description of limited rate $R_h$ to the transmitter and/or the
receiver. We derive upper and lower bounds on the optimal achievable
$\alpha$-th moment of the estimation error and show that they coincide for
small values of $\alpha$ and for high values of $R_h$. The upper bound relies
on a recently proposed channel-coding scheme that effectively conveys $R_h$
bits essentially error-free and the rest of the rate - over the same AWGN
channel without help, with the error-free bits being allocated to the most
significant bits of the quantized parameter. We then concentrate on the setting
with a total transmit energy constraint, for which we derive achievability
results for both channel coding and parameter modulation for several scenarios:
when the helper assists only the transmitter or only the receiver and knows the
noise, and when the helper assists the transmitter and/or the receiver and
knows both the noise and the message. In particular, for the message-informed
helper that assists both the receiver and the transmitter, it is shown that the
error probability in the channel-coding task decays doubly exponentially.
Finally, we translate these results to those for continuous-time power-limited
AWGN channels with unconstrained bandwidth. As a byproduct, we show that the
capacity with a message-informed helper that is available only at the
transmitter can exceed the sum of the capacity without help and the help rate
$R_h$, when the helper knows only the noise but not the message.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cooperative Sense and Avoid for UAVs using Secondary Radar 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Mohammadkarimi, Raj Thilak Rajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A cooperative Sense and Avoid (SAA) algorithm for safe navigation of
small-sized UAVs within an airspace is proposed in this paper. The proposed
method relies upon cooperation between the UAV and the surrounding
transponder-equipped aviation obstacles. To do so, the aviation obstacles share
their altitude and identification code with the UAV by using a Mode S operation
of the Secondary Surveillance Radar (SSR) after interrogation. The proposed SAA
algorithm benefits from the estimate of the aviation obstacle's elevation angle
for ranging. This results in more accurate ranging compared to the round-trip
time-based ranging, which is currently used in existing SAA systems. We also
propose a low-complexity and accurate radial velocity estimator for the Mode S
operation of the SSR which is employed in the proposed SAA system. Furthermore,
by considering the Pulse-Position Modulation (PPM) of the transponder reply as
a waveform of pulse radar with random pulse repetition intervals, the maximum
unambiguous radial velocity is obtained. The proposed SAA is equipped with an
intruder identification method that determines the risk level ofthe surrounding
transponder-equipped aviation obstacles. Given the estimated parameters, the
intruder identification method classifies the aviation obstacles into high-,
medium-, and low-risk intruders. The output of the classifier enables the UAV
to plan its path or maneuver for safe navigation accordingly. The root mean
square error (RMSE) of the proposed estimators are analytically derived, and
the effectiveness of our SAA solution is confirmed through simulation
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Ultra-wideband Characterization of Azimuth, Elevation and Time of
  Arrival with Toric Arrays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Ramírez-Arroyo, Antonio Alex-Amor, Rubén Medina, Pablo Padilla, Juan F. Valenzuela-Valdés
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present an analytical framework for the joint
characterization of the 3D direction of arrival (DoA), i.e., azimuth and
elevation components, and time of arrival (ToA) in multipath environments. The
analytical framework is based on the use of nearly frequency-invariant
beamformers (FIB) formed by toric arrays. The frequency response of the toric
array is expanded as a series of phase modes, which leads to azimuth-time and
elevation-time diagrams from which the 3D DoA and the ToA of the incoming waves
can be extracted over a wide bandwidth. Firstly, we discuss some practical
considerations, advantages and limitations of using the analytical method.
Subsequently, we perform a parametric study to analyze the influence of the
method parameters on the quality of the estimation. The method is tested in
single-path and multipath mm-wave environments over a large bandwidth. The
results show that the proposed method improves the quality of the estimation,
i.e., decreases the level of the artifacts, compared to other state-of-art FIB
approaches based on the use of single/concentric circular and elliptical
arrays.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE Transactions on Wireless Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intelligent Surfaces Empowered Wireless Network: Recent Advances and The
  Road to 6G 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16918v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16918v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingqing Wu, Beixiong Zheng, Changsheng You, Lipeng Zhu, Kaiming Shen, Xiaodan Shao, Weidong Mei, Boya Di, Hongliang Zhang, Ertugrul Basar, Lingyang Song, Marco Di Renzo, Zhi-Quan Luo, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent surfaces (ISs) have emerged as a key technology to empower a wide
range of appealing applications for wireless networks, due to their low cost,
high energy efficiency, flexibility of deployment and capability of
constructing favorable wireless channels/radio environments. Moreover, the
recent advent of several new IS architectures further expanded their
electromagnetic functionalities from passive reflection to active
amplification, simultaneous reflection and refraction, as well as holographic
beamforming. However, the research on ISs is still in rapid progress and there
have been recent technological advances in ISs and their emerging applications
that are worthy of a timely review. Thus, we provide in this paper a
comprehensive survey on the recent development and advances of ISs aided
wireless networks. Specifically, we start with an overview on the anticipated
use cases of ISs in future wireless networks such as 6G, followed by a summary
of the recent standardization activities related to ISs. Then, the main design
issues of the commonly adopted reflection-based IS and their state-of-the-art
solutions are presented in detail, including reflection optimization,
deployment, signal modulation, wireless sensing, and integrated sensing and
communications. Finally, recent progress and new challenges in advanced IS
architectures are discussed to inspire futrue research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Movable-Antenna Array Enhanced Beamforming: Achieving Full Array Gain
  with Null Steering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.08787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.08787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lipeng Zhu, Wenyan Ma, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional beamforming with fixed-position antenna (FPA) arrays has a
fundamental trade-off between maximizing the signal power (array gain) over a
desired direction and simultaneously minimizing the interference power over
undesired directions. To overcome this limitation, this letter investigates the
movable antenna (MA) array enhanced beamforming by exploiting the new degree of
freedom (DoF) via antenna position optimization, in addition to the design of
antenna weights. We show that by jointly optimizing the antenna positions
vector (APV) and antenna weights vector (AWV) of a linear MA array, the full
array gain can be achieved over the desired direction while null steering can
be realized over all undesired directions, under certain numbers of MAs and
null-steering directions. The optimal solutions for AWV and APV are derived in
closed form, which reveal that the optimal AWV for MA arrays requires only the
signal phase adjustment with a fixed amplitude. Numerical results validate our
analytical solutions for MA array beamforming and show their superior
performance to the conventional beamforming techniques with FPA arrays.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Movable-Antenna Enhanced Multiuser Communication via Antenna Position
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06978v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06978v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lipeng Zhu, Wenyan Ma, Boyu Ning, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Movable antenna (MA) is a promising technology to improve wireless
communication performance by varying the antenna position in a given finite
area at the transceivers to create more favorable channel conditions. In this
paper, we investigate the MA-enhanced multiple-access channel (MAC) for the
uplink transmission from multiple users each equipped with a single MA to a
base station (BS) with a fixed-position antenna (FPA) array. A field-response
based channel model is used to characterize the multi-path channel between the
antenna array of the BS and each user's MA with a flexible position. To
evaluate the MAC performance gain provided by MAs, we formulate an optimization
problem for minimizing the total transmit power of users, subject to a
minimum-achievable-rate requirement for each user, where the positions of MAs
and the transmit powers of users, as well as the receive combining matrix of
the BS are jointly optimized. To solve this non-convex optimization problem
involving intricately coupled variables, we develop two algorithms based on
zero-forcing (ZF) and minimum mean square error (MMSE) combining methods,
respectively. Specifically, for each algorithm, the combining matrix of the BS
and the total transmit power of users are expressed as a function of the MAs'
position vectors, which are then optimized by using the proposed
multi-directional descent (MDD) framework. It is shown that the proposed
ZF-based and MMSE-based MDD algorithms can converge to high-quality suboptimal
solutions with low computational complexities. Simulation results demonstrate
that the proposed solutions for MA-enhanced multiple access systems can
significantly decrease the total transmit power of users as compared to
conventional FPA systems employing antenna selection under both perfect and
imperfect field-response information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling and Performance Analysis for Movable Antenna Enabled Wireless
  Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lipeng Zhu, Wenyan Ma, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel antenna architecture called movable antenna
(MA) to improve the performance of wireless communication systems. Different
from conventional fixed-position antennas (FPAs) that undergo random wireless
channel variation, the MAs with the capability of flexible movement can be
deployed at positions with more favorable channel conditions to achieve higher
spatial diversity gains. To characterize the general multi-path channel in a
given region or field where the MAs are deployed, a field-response model is
developed by leveraging the amplitude, phase, and angle of arrival/angle of
departure (AoA/AoD) information on each of the multiple channel paths under the
far-field condition. Based on this model, we then analyze the maximum channel
gain achieved by a single receive MA as compared to its FPA counterpart in both
deterministic and stochastic channels. First, in the deterministic channel
case, we show the periodic behavior of the multi-path channel gain in a given
spatial field, which can be exploited for analyzing the maximum channel gain of
the MA. Next, in the case of stochastic channels, the expected value of an
upper bound on the maximum channel gain of the MA in an infinitely large
receive region is derived for different numbers of channel paths. The
approximate cumulative distribution function (CDF) for the maximum channel gain
is also obtained in closed form, which is useful to evaluate the outage
probability of the MA system. Moreover, our results reveal that higher
performance gains by the MA over the FPA can be acquired when the number of
channel paths increases due to more pronounced small-scale fading effects in
the spatial domain. Numerical examples are presented which validate our
analytical results and demonstrate that the MA system can reap considerable
performance gains over the conventional FPA systems with/without antenna
selection (AS).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Movable Antennas for Wireless Communication: Opportunities and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lipeng Zhu, Wenyan Ma, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Movable antenna (MA) technology is a recent development that fully exploits
the wireless channel spatial variation in a confined region by enabling local
movement of the antenna. Specifically, the positions of antennas at the
transmitter and/or receiver can be dynamically changed to obtain better channel
conditions for improving the communication performance. In this article, we
first provide an overview of the promising applications for MA-aided wireless
communication. Then, we present the hardware architecture and channel
characterization for MA systems, based on which the variation of the channel
gain with respect to the MA's position is illustrated. Furthermore, we analyze
the performance advantages of MAs over conventional fixed-position antennas, in
terms of signal power improvement, interference mitigation, flexible
beamforming, and spatial multiplexing. Finally, we discuss the main design
challenges and their potential solutions for MA-aided communication systems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Systems and Control
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dataset</span>s of Great Britain Primary Substations Integrated with Household
  Heating Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Zhou, Chaimaa Essayeh, Thomas Morstyn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing demand for electrified heating, electrified transportation, and
power-intensive data centres challenge distribution networks. If
electrification projects are carried out without considering electrical
distribution infrastructure, there could be unexpected blackouts and financial
losses. Datasets containing real-world distribution network information are
required to address this. On the other hand, social data, such as household
heating composition, are closely coupled with people's lives. Studying the
coupling between the energy system and society is important in promoting social
welfare. To fill these gaps, this paper introduces two datasets. The first is
the main dataset for the distribution networks in Great Britain (GB),
collecting information on firm capacity, peak demands, locations, and parent
transmission nodes (the Grid Supply Point, namely GSP) for all primary
substations (PSs). PSs are a crucial part of the UK distribution network and
are at the lowest voltage level (11 kV) with publicly available data for most
UK Distribution Network Operators (DNOs). Substation firm capacity and peak
demand facilitate an understanding of the remaining room of the existing
network. The parent GSP information helps link the dataset of distribution
networks to datasets of transmission networks. The second dataset extends the
main network dataset, linking each PS to information about the number of
households that use different types of central heating recorded in census data.
The derivation of the second dataset is based on locations of PSs collected in
the main dataset with appropriate assumptions. The derivation process may also
be replicated to integrate other social datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the journal "Data in Brief"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ANN-Based Adaptive NMPC for Uranium Extraction-Scrubbing Operation in
  Spent Nuclear Fuel Treatment Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duc-Tri Vo, Ionela Prodan, Laurent Lefèvre, Vincent Vanel, Sylvain Costenoble, Binh Dinh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the particularities in optimal control of the uranium
extraction-scrubbing operation in the PUREX process. The control problem
requires optimally stabilizing the system at a desired solvent saturation
level, guaranteeing constraints, disturbance rejection, and adapting to set
point variations. A qualified simulator named PAREX was developed by the French
Alternative Energies and Atomic Energy Commission (CEA) to simulate
liquid-liquid extraction operations in the PUREX process. However, since the
mathematical model is complex and is described by a system of nonlinear, stiff,
high-dimensional differential-algebraic equations (DAE), applying optimal
control methods will lead to a large-scale nonlinear programming problem with a
huge computational burden. The solution we propose in this work is to train a
neural network to predict the process outputs using the measurement history.
This neural network architecture, which employs the long short-term memory
(LSTM), linear regression and logistic regression networks, allows reducing the
number of state variables, thus reducing the complexity of the optimization
problems in the control scheme. Furthermore, nonlinear model predictive control
(NMPC) and moving horizon estimation (MHE) problems are developed and solved
using the PSO (Particle Swarm Optimization) algorithm. Simulation results show
that the proposed adaptive optimal control scheme satisfies the requirements of
the control problem and provides promise for experimental testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Control-Coherent Koopman Modeling: A Physical Modeling Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. Harry Asada, Jose A. Solano-Castellanos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The modeling of nonlinear dynamics based on Koopman operator theory, which is
originally applicable only to autonomous systems with no control, is extended
to non-autonomous control system without approximation to input matrix B.
Prevailing methods using a least square estimate of the B matrix may result in
an erroneous input matrix, misinforming the controller about the structure of
the input matrix in a lifted space. Here, a new method for constructing a
Koopman model that comprises the exact input matrix B is presented. A set of
state variables are introduced so that the control inputs are linearly involved
in the dynamics of actuators. With these variables, a lifted linear model with
the exact control matrix, called a Control-Coherent Koopman Model, is
constructed by superposing control input terms, which are linear in local
actuator dynamics, to the Koopman operator of the associated autonomous
nonlinear system. The proposed method is applied to multi degree-of-freedom
robotic arms and multi-cable manipulation systems. Model Predictive Control is
applied to the former. It is demonstrated that the prevailing Dynamic Mode
Decomposition with Control (DMDc) using an approximate control matrix B does
not provide a satisfactory result, while the Control-Coherent Koopman Model
performs well with the correct B matrix.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Automatic Line-System Provisioning with Integrated
  Physical-Parameter-Aware Methodology: Field Verification and Operational
  Feasibility 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hideki Nishizawa, Giacomo Borraccini, Takeo Sasai, Yue-Kai Huang, Toru Mano, Kazuya Anazawa, Masatoshi Namiki, Soichiroh Usui, Tatsuya Matsumura, Yoshiaki Sone, Zehao Wang, Seiji Okamoto, Takeru Inoue, Ezra Ip, Andrea D'Amico, Tingjun Chen, Vittorio Curri, Ting Wang, Koji Asahi, Koichi Takasugi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose methods and an architecture to conduct measurements and optimize
newly installed optical fiber line systems semi-automatically using integrated
physics-aware technologies in a data center interconnection (DCI) transmission
scenario. We demonstrate, for the first time, digital longitudinal monitoring
(DLM) and optical line system (OLS) physical parameter calibration working
together in real-time to extract physical link parameters for transmission
performance optimization. Our methodology has the following advantages over
traditional design: a minimized footprint at user sites, accurate estimation of
the necessary optical network characteristics via complementary telemetry
technologies, and the capability to conduct all operation work remotely. The
last feature is crucial, as it enables remote operation to implement network
design settings for immediate response to quality of transmission (QoT)
degradation and reversion in the case of unforeseen problems. We successfully
performed semi-automatic line system provisioning over field fiber networks
facilities at Duke University, Durham, NC. The tasks of parameter retrieval,
equipment setting optimization, and system setup/provisioning were completed
within 1 hour. The field operation was supervised by on-duty personnel who
could access the system remotely from different time zones. By comparing
Q-factor estimates calculated from the extracted link parameters with measured
results from 400G transceivers, we confirmed that our methodology has a
reduction in the QoT prediction errors (+-0.3 dB) over existing design (+-10.6
dB).
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Logic in Computer Science
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logic-based Explanations for Linear Support Vector Classifiers with
  Reject Option 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Mateus Rocha Filho, Thiago Alves Rocha, Reginaldo Pereira Fernandes Ribeiro, Ajalmar Rêgo da Rocha Neto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Support Vector Classifier (SVC) is a well-known Machine Learning (ML) model
for linear classification problems. It can be used in conjunction with a reject
option strategy to reject instances that are hard to correctly classify and
delegate them to a specialist. This further increases the confidence of the
model. Given this, obtaining an explanation of the cause of rejection is
important to not blindly trust the obtained results. While most of the related
work has developed means to give such explanations for machine learning models,
to the best of our knowledge none have done so for when reject option is
present. We propose a logic-based approach with formal guarantees on the
correctness and minimality of explanations for linear SVCs with reject option.
We evaluate our approach by comparing it to Anchors, which is a heuristic
algorithm for generating explanations. Obtained results show that our proposed
method gives shorter explanations with reduced time cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, submitted to BRACIS 2023 (Brazilian Conference on
  Intelligent Systems), accepted version published in Intelligent Systems,
  LNCS, vol 14195</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Language Models Pretend Solvers? Logic Code Simulation with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minyu Chen, Guoqiang Li, Ling-I Wu, Ruibang Liu, Yuxin Su, Xi Chang, Jianxin Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based large language models (LLMs) have demonstrated significant
potential in addressing logic problems. capitalizing on the great capabilities
of LLMs for code-related activities, several frameworks leveraging logical
solvers for logic reasoning have been proposed recently. While existing
research predominantly focuses on viewing LLMs as natural language logic
solvers or translators, their roles as logic code interpreters and executors
have received limited attention. This study delves into a novel aspect, namely
logic code simulation, which forces LLMs to emulate logical solvers in
predicting the results of logical programs. To further investigate this novel
task, we formulate our three research questions: Can LLMs efficiently simulate
the outputs of logic codes? What strength arises along with logic code
simulation? And what pitfalls? To address these inquiries, we curate three
novel datasets tailored for the logic code simulation task and undertake
thorough experiments to establish the baseline performance of LLMs in code
simulation. Subsequently, we introduce a pioneering LLM-based code simulation
technique, Dual Chains of Logic (DCoL). This technique advocates a dual-path
thinking approach for LLMs, which has demonstrated state-of-the-art performance
compared to other LLM prompt strategies, achieving a notable improvement in
accuracy by 7.06% with GPT-4-Turbo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Many-one reducibility with realizability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takayuki Kihara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we propose a new classification of $\Sigma^0_2$ formulas
under the realizability interpretation of many-one reducibility (i.e., Levin
reducibility). For example, ${\sf Fin}$, the decision of being eventually zero
for sequences, is many-one/Levin complete among $\Sigma^0_2$ formulas of the
form $\exists n\forall m\geq n.\varphi(m,x)$, where $\varphi$ is decidable. The
decision of boundedness for sequences ${\sf BddSeq}$ and posets ${\sf PO}_{\sf
top}$ are many-one/Levin complete among $\Sigma^0_2$ formulas of the form
$\exists n\forall m\geq n\forall k.\varphi(m,k,x)$, where $\varphi$ is
decidable. However, unlike the classical many-one reducibility, none of the
above is $\Sigma^0_2$-complete. The decision of non-density of linear order
${\sf NonDense}$ is truly $\Sigma^0_2$-complete.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Term rewriting on nestohedra 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre-Louis Curien, Guillaume Laplante-Anfossi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We define term rewriting systems on the vertices and faces of nestohedra, and
show that the former are confluent and terminating. While the associated poset
on vertices generalizes Barnard--McConville's flip order for
graph-associahedra, the preorder on faces likely generalizes the facial weak
order for permutahedra. Moreover, we define and study contextual families of
nestohedra, whose local confluence diagrams satisfy a certain uniformity
condition. Among them are associahedra and operahedra, whose associated proofs
of confluence for their rewriting systems reproduce proofs of categorical
coherence theorems for monoidal categories and categorified operads.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lemur: Integrating Large Language Models in Automated Program
  Verification <span class="chip">ICLR'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04870v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04870v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoze Wu, Clark Barrett, Nina Narodytska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The demonstrated code-understanding capability of LLMs raises the question of
whether they can be used for automated program verification, a task that
demands high-level abstract reasoning about program properties that is
challenging for verification tools. We propose a general methodology to combine
the power of LLMs and automated reasoners for automated program verification.
We formally describe this methodology as a set of derivation rules and prove
its soundness. We instantiate the calculus as a sound automated verification
procedure, which led to practical improvements on a set of synthetic and
competition benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR'24</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-23T00:00:00Z">2024-03-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Signal Processing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy Efficient Design of Active STAR-RIS-Aided SWIPT Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajad Faramarzi, Hosein Zarini, Sepideh Javadi, Mohammad Robat Mili, Rui Zhang, George K. Karagiannidis, Naofal Al-Dhahir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the downlink transmission of a multi-antenna base
station (BS) supported by an active simultaneously transmitting and
reconfigurable intelligent surface (STAR-RIS) to serve single-antenna users via
simultaneous wireless information and power transfer (SWIPT). In this context,
we formulate an energy efficiency maximisation problem that jointly optimises
the gain, element selection and phase shift matrices of the active STAR-RIS,
the transmit beamforming of the BS and the power splitting ratio of the users.
With respect to the highly coupled and non-convex form of this problem, an
alternating optimisation solution approach is proposed, using tools from convex
optimisation and reinforcement learning. Specifically, semi-definite relaxation
(SDR), difference of concave functions (DC), and fractional programming
techniques are employed to transform the non-convex optimisation problem into a
convex form for optimising the BS beamforming vector and the power splitting
ratio of the SWIPT. Then, by integrating meta-learning with the modified deep
deterministic policy gradient (DDPG) and soft actor-critical (SAC) methods, a
combinatorial reinforcement learning network is developed to optimise the
element selection, gain and phase shift matrices of the active STAR-RIS. Our
simulations show the effectiveness of the proposed resource allocation scheme.
Furthermore, our proposed active STAR-RIS-based SWIPT system outperforms its
passive counterpart by 57% on average.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Channel-Resilient CSI-Based RF Fingerprinting using Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqi Kong, He Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces DeepCRF, a deep learning framework designed for channel
state information-based radio frequency fingerprinting (CSI-RFF). The
considered CSI-RFF is built on micro-CSI, a recently discovered radio-frequency
(RF) fingerprint that manifests as micro-signals appearing on the channel state
information (CSI) curves of commercial WiFi devices. Micro-CSI facilitates
CSI-RFF which is more streamlined and easily implementable compared to existing
schemes that rely on raw I/Q samples. The primary challenge resides in the
precise extraction of micro-CSI from the inherently fluctuating CSI
measurements, a process critical for reliable RFF. The construction of a
framework that is resilient to channel variability is essential for the
practical deployment of CSI-RFF techniques. DeepCRF addresses this challenge
with a thoughtfully trained convolutional neural network (CNN). This network's
performance is significantly enhanced by employing effective and strategic data
augmentation techniques, which bolster its ability to generalize to novel,
unseen channel conditions. Furthermore, DeepCRF incorporates supervised
contrastive learning to enhance its robustness against noises. Our evaluations
demonstrate that DeepCRF significantly enhances the accuracy of device
identification across previously unencountered channels. It outperforms both
the conventional model-based methods and standard CNN that lack our specialized
training and enhancement strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages,5 figures, INFOCOM WKSHPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Block Orthogonal Sparse Superposition Codes for $ \sf{L}^3 $
  Communications: Low Error Rate, Low Latency, and Low Power Consumption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghwa Han, Bowhyung Lee, Min Jang, Donghun Lee, Seho Myung, Namyoon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Block orthogonal sparse superposition (BOSS) code is a class of joint coded
modulation methods, which can closely achieve the finite-blocklength capacity
with a low-complexity decoder at a few coding rates under Gaussian channels.
However, for fading channels, the code performance degrades considerably
because coded symbols experience different channel fading effects. In this
paper, we put forth novel joint demodulation and decoding methods for BOSS
codes under fading channels. For a fast fading channel, we present a minimum
mean square error approximate maximum a posteriori (MMSE-A-MAP) algorithm for
the joint demodulation and decoding when channel state information is available
at the receiver (CSIR). We also propose a joint demodulation and decoding
method without using CSIR for a block fading channel scenario. We refer to this
as the non-coherent sphere decoding (NSD) algorithm. Simulation results
demonstrate that BOSS codes with MMSE-A-MAP decoding outperform CRC-aided polar
codes, while NSD decoding achieves comparable performance to quasi-maximum
likelihood decoding with significantly reduced complexity. Both decoding
algorithms are suitable for parallelization, satisfying low-latency
constraints. Additionally, real-time simulations on a software-defined radio
testbed validate the feasibility of using BOSS codes for low-power
transmission.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Physical Layer Security in Dual-Function Radar-Communication
  Systems with Hybrid Beamforming Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingyun Xu, Bowen Wang, Huiyong Li, Ziyang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this letter, we investigate enhancing the physical layer security (PLS)
for the dual-function radar-communication (DFRC) system with hybrid beamforming
(HBF) architecture, where the base station (BS) achieves downlink communication
and radar target detection simultaneously. We consider an eavesdropper
intercepting the information transmitted from the BS to the downlink
communication users with imperfectly known channel state information.
Additionally, the location of the radar target is also imperfectly known by the
BS. To enhance PLS in the considered DFRC system, we propose a novel HBF
architecture, which introduces a new integrated sensing and security (I2S)
symbol. The secure HBF design problem for DFRC is formulated by maximizing the
minimum legitimate user communication rate subject to radar
signal-to-interference-plus-noise ratio, eavesdropping rate, hardware and power
constraints. To solve this non-convex problem, we propose an alternating
optimization based method to jointly optimize transmit and receive beamformers.
Numerical simulation results validate the effectiveness of the proposed
algorithm and show the superiority of the proposed I2S-aided HBF architecture
for achieving DFRC and enhancing PLS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arbitrary Discrete Fourier Analysis and Its Application in Replayed
  Speech Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01130v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01130v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shih-Kuang Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a group of finite sequences and its variants were proposed to
use in conducting signal analysis; we called the developed signal analysis
methods arbitrary discrete Fourier analysis (ADFA), Mel-scale discrete Fourier
analysis (MDFA) and constant Q analysis (CQA). The effectiveness of three
signal analysis methods were then validated by testing their performance on a
replayed speech detection benchmark (i.e., the ASVspoof 2019 Physical Access)
along with a state-of-the-art model. Comparable performance to the best
reported systems were shown by the experimental results with three signal
analysis methods. Furthermore, the CQA method shown its efficiency with less
computation time in compared to the convention method constant Q transform
(CQT), which is commonly used in spoofed and fake speech detection and music
processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/shihkuanglee/ADFA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Theoretical Analysis of the Radio Map Estimation Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15106v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15106v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Romero, Tien Ngoc Ha, Raju Shrestha, Massimo Franceschetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radio maps provide radio frequency metrics, such as the received signal
strength, at every location of a geographic area. These maps, which are
estimated using a set of measurements collected at multiple positions, find a
wide range of applications in wireless communications, including the prediction
of coverage holes, network planning, resource allocation, and path planning for
mobile robots. Although a vast number of estimators have been proposed, the
theoretical understanding of the radio map estimation (RME) problem has not
been addressed. The present work aims at filling this gap along two directions.
First, the complexity of the set of radio map functions is quantified by means
of lower and upper bounds on their spatial variability, which offers valuable
insight into the required spatial distribution of measurements and the
estimators that can be used. Second, the reconstruction error for power maps in
free space is upper bounded for three conventional spatial interpolators. The
proximity coefficient, which is a decreasing function of the distance from the
transmitters to the mapped region, is proposed to quantify the complexity of
the RME problem. Numerical experiments assess the tightness of the obtained
bounds and the validity of the main takeaways in complex environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A learning-based solution approach to the application placement problem
  in mobile edge computing under uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11259v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11259v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha-Hossein Hejazi, Zahra Ghadimkhani, Arezoo Borji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Placing applications in mobile edge computing servers presents a complex
challenge involving many servers, users, and their requests. Existing
algorithms take a long time to solve high-dimensional problems with significant
uncertainty scenarios. Therefore, an efficient approach is required to maximize
the quality of service while considering all technical constraints. One of
these approaches is machine learning, which emulates optimal solutions for
application placement in edge servers. Machine learning models are expected to
learn how to allocate user requests to servers based on the spatial positions
of users and servers. In this study, the problem is formulated as a two-stage
stochastic programming. A sufficient amount of training records is generated by
varying parameters such as user locations, their request rates, and solving the
optimization model. Then, based on the distance features of each user from the
available servers and their request rates, machine learning models generate
decision variables for the first stage of the stochastic optimization model,
which is the user-to-server request allocation, and are employed as independent
decision agents that reliably mimic the optimization model. Support Vector
Machines (SVM) and Multi-layer Perceptron (MLP) are used in this research to
achieve practical decisions from the stochastic optimization models. The
performance of each model has shown an execution effectiveness of over 80%.
This research aims to provide a more efficient approach for tackling
high-dimensional problems and scenarios with uncertainties in mobile edge
computing by leveraging machine learning models for optimal decision-making in
request allocation to edge servers. These results suggest that machine-learning
models can significantly improve solution times compared to conventional
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling Sparse Graph Sequences and Signals Using Generalized Graphons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08124v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08124v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Ji, Xingchao Jian, Wee Peng Tay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphons are limit objects of sequences of graphs and are used to analyze the
behavior of large graphs. Recently, graphon signal processing has been
developed to study signal processing on large graphs. A major limitation of
this approach is that any sparse sequence of graphs inevitably converges to the
zero graphon, rendering the resulting signal processing theory trivial and
inadequate for sparse graph sequences. To overcome this limitation, we propose
a new signal processing framework that leverages the concept of generalized
graphons and introduces the stretched cut distance as a measure to compare
these graphons. Our framework focuses on the sampling of graph sequences from
generalized graphons and explores the convergence properties of associated
operators, spectra, and signals. Our signal processing framework provides a
comprehensive approach to analyzing and processing signals on graph sequences,
even if they are sparse. Finally, we discuss the practical implications of our
theory for real-world large networks through numerical experiments.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Logic in Computer Science
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAT Encoding of Partial Ordering Models for Graph Coloring Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Faber, Adalat Jabrayilov, Petra Mutzel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we suggest new SAT encodings of the partial-ordering based ILP
model for the graph coloring problem (GCP) and the bandwidth coloring problem
(BCP). The GCP asks for the minimum number of colors that can be assigned to
the vertices of a given graph such that each two adjacent vertices get
different colors. The BCP is a generalization, where each edge has a weight
that enforces a minimal "distance" between the assigned colors, and the goal is
to minimize the "largest" color used. For the widely studied GCP, we
experimentally compare our new SAT encoding to the state-of-the-art approaches
on the DIMACS benchmark set. Our evaluation confirms that this SAT encoding is
effective for sparse graphs and even outperforms the state-of-the-art on some
DIMACS instances. For the BCP, our theoretical analysis shows that the
partial-ordering based SAT and ILP formulations have an asymptotically smaller
size than that of the classical assignment-based model. Our practical
evaluation confirms not only a dominance compared to the assignment-based
encodings but also to the state-of-the-art approaches on a set of benchmark
instances. Up to our knowledge, we have solved several open instances of the
BCP from the literature for the first time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Concepts Definable in First-Order Logic with Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1909.03820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1909.03820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steffen van Bergerem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study Boolean classification problems over relational background
structures in the logical framework introduced by Grohe and Tur\'an (TOCS
2004). It is known (Grohe and Ritzert, LICS 2017) that classifiers definable in
first-order logic over structures of polylogarithmic degree can be learned in
sublinear time, where the degree of the structure and the running time are
measured in terms of the size of the structure. We generalise the results to
the first-order logic with counting FOCN, which was introduced by Kuske and
Schweikardt (LICS 2017) as an expressive logic generalising various other
counting logics. Specifically, we prove that classifiers definable in FOCN over
classes of structures of polylogarithmic degree can be consistently learned in
sublinear time. This can be seen as a first step towards extending the learning
framework to include numerical aspects of machine learning. We extend the
result to agnostic probably approximately correct (PAC) learning for classes of
structures of degree at most $(\log \log n)^c$ for some constant $c$. Moreover,
we show that bounding the degree is crucial to obtain sublinear-time learning
algorithms. That is, we prove that, for structures of unbounded degree,
learning is not possible in sublinear time, even for classifiers definable in
plain first-order logic.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-22T00:00:00Z">2024-03-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Signal Processing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Subpopulation Bias for Fair Network Topology Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madeline Navarro, Samuel Rey, Andrei Buciulea, Antonio G. Marques, Santiago Segarra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider fair network topology inference from nodal observations.
Real-world networks often exhibit biased connections based on sensitive nodal
attributes. Hence, different subpopulations of nodes may not share or receive
information equitably. We thus propose an optimization-based approach to
accurately infer networks while discouraging biased edges. To this end, we
present bias metrics that measure topological demographic parity to be applied
as convex penalties, suitable for most optimization-based graph learning
methods. Moreover, we encourage equitable treatment for any number of
subpopulations of differing sizes. We validate our method on synthetic and
real-world simulations using networks with both biased and unbiased
connections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast real-time arbitrary waveform generation using graphic processing
  units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juntian Tu, Sarthak Subhankar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time Arbitrary Waveform Generation (AWG) is essential in various
engineering and research applications, and often requires complex bespoke
hardware and software. This paper introduces an AWG framework using an NVIDIA
Graphics Processing Unit (GPU) and a commercially available high-speed
Digital-to-Analog Converter (DAC) card, both running on a desktop personal
computer (PC). The GPU accelerates the "embarrassingly" data parallel additive
waveform synthesis framework for AWG, and the DAC reconstructs the generated
waveform in the analog domain at high speed. The AWG framework is programmed
using the developer-friendly Compute Unified Device Architecture (CUDA) runtime
application programming interface from NVIDIA and is readily customizable, and
scalable with additional parallel hardware. We present and characterize two
different pathways for computing modulated radio-frequency (rf) waveforms: one
pathway offers high-complexity simultaneous chirping of 1000 individual
Nyquist-limited single-frequency tones for 35 ms at a sampling rate of 560
MB/s, and the other pathway allows simultaneous continuous chirping of 194
individual Nyquist-limited single-frequency tones at 100 MB/s, or 20 individual
tones at 560 MB/s. This AWG framework is designed for fast on-the-fly
rearrangement of a large stochastically-loaded optical tweezer array of single
atoms or molecules into a defect-free array needed for quantum simulation and
quantum computation applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information Rates of Successive Interference Cancellation for Optical
  Fiber 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Jäger, Gerhard Kramer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Successive interference cancellation (SIC) is used to approach the achievable
information rates (AIRs) of joint detection and decoding for long-haul optical
fiber links. The AIRs of memoryless ring constellations are compared to those
of circularly symmetric complex Gaussian modulation for surrogate channel
models with correlated phase noise. Simulations are performed for 1000 km of
standard single-mode fiber with ideal Raman amplification. In this setup, 32
rings and 16 SIC-stages with Gaussian message-passing receivers achieve the AIR
peaks of previous work. The computational complexity scales in proportion to
the number of SIC-stages, where one stage has the complexity of separate
detection and decoding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to walk on new ground: Calibration-free decoding for c-VEP BCI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J. Thielen, J. Sosulski, M. Tangermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores two zero-training methods aimed at enhancing the
usability of brain-computer interfaces (BCIs) by eliminating the need for a
calibration session. We introduce a novel method rooted in the event-related
potential (ERP) domain, unsupervised mean maximization (UMM), to the fast
code-modulated visual evoked potential (c-VEP) stimulus protocol. We compare
UMM to the state-of-the-art c-VEP zero-training method that uses canonical
correlation analysis (CCA). The comparison includes instantaneous
classification and classification with cumulative learning from previously
classified trials for both CCA and UMM. Our study shows the effectiveness of
both methods in navigating the complexities of a c-VEP dataset, highlighting
their differences and distinct strengths. This research not only provides
insights into the practical implementation of calibration-free BCI methods but
also paves the way for further exploration and refinement. Ultimately, the
fusion of CCA and UMM holds promise for enhancing the accessibility and
usability of BCI systems across various application domains and a multitude of
stimulus protocols.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 9th Graz Brain-Computer Interface Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Task-agnostic Trait of <span class="highlight-title">Self-supervised</span> Learning in the
  Context of Detecting Mental Disorders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Kumar Gupta, Rohit Sinha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has been investigated to generate
task-agnostic representations across various domains. However, such
investigation has not been conducted for detecting multiple mental disorders.
The rationale behind the existence of a task-agnostic representation lies in
the overlapping symptoms among multiple mental disorders. Consequently, the
behavioural data collected for mental health assessment may carry a mixed bag
of attributes related to multiple disorders. Motivated by that, in this study,
we explore a task-agnostic representation derived through SSL in the context of
detecting major depressive disorder (MDD) and post-traumatic stress disorder
(PTSD) using audio and video data collected during interactive sessions. This
study employs SSL models trained by predicting multiple fixed targets or masked
frames. We propose a list of fixed targets to make the generated representation
more efficient for detecting MDD and PTSD. Furthermore, we modify the
hyper-parameters of the SSL encoder predicting fixed targets to generate global
representations that capture varying temporal contexts. Both these innovations
are noted to yield improved detection performances for considered mental
disorders and exhibit task-agnostic traits. In the context of the SSL model
predicting masked frames, the generated global representations are also noted
to exhibit task-agnostic traits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Channel Orthogonalization with Reconfigurable Surfaces: General Models,
  Theoretical Limits, and Effective Configuration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Vidal Alegría, Johan Thunberg, Ove Edfors
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We envision a future in which multi-antenna technology effectively exploits
the spatial domain as a set of non-interfering orthogonal resources, allowing
for flexible resource allocation and efficient modulation/demodulation.
Reconfigurable intelligent surface (RIS) has emerged as a promising technology
which allows shaping the propagation environment for improved performance. This
paper studies the ability of three extended types of reconfigurable surface
(RS), including the recently proposed beyond diagonal RIS (BD-RIS), to achieve
perfectly orthogonal channels in a general multi-user multiple-input
multiple-output (MU-MIMO) scenario. We propose practical implementations for
the three types of RS consisting of passive components, and obtain the
corresponding restrictions on their reconfigurability. We then use these
restrictions to derive closed-form conditions for achieving arbitrary
(orthogonal) channels. We also study the problem of optimal orthogonal channel
selection for achieving high channel gain without active amplification at the
RS, and we propose some methods with satisfying performance. Finally, we
provide efficient channel estimation and RS configuration techniques such that
all the computation, including the channel selection, may be performed at the
base station (BS). The numerical results showcase the potential and
practicality of RS channel orthogonalization, thus taking a step towards
orthogonal spatial domain multiplexing (OSDM).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures. This work has been submitted to the IEEE for
  possible publication, copyright information may be affected upon publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Resource Allocation for STAR-RIS Assisted SWIPT Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyu Zhu, Xidong Mu, Li Guo, Ao Huang, Shibiao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A simultaneously transmitting and reflecting reconfigurable intelligent
surface (STAR-RIS) assisted simultaneous wireless information and power
transfer (SWIPT) system is proposed. More particularly, an STAR-RIS is deployed
to assist in the information/power transfer from a multi-antenna access point
(AP) to multiple single-antenna information users (IUs) and energy users (EUs),
where two practical STAR-RIS operating protocols, namely energy splitting (ES)
and time switching (TS), are employed. Under the imperfect channel state
information (CSI) condition, a multi-objective optimization problem (MOOP)
framework, that simultaneously maximizes the minimum data rate and minimum
harvested power, is employed to investigate the fundamental rate-energy
trade-off between IUs and EUs. To obtain the optimal robust resource allocation
strategy, the MOOP is first transformed into a single-objective optimization
problem (SOOP) via the {\epsilon}-constraint method, which is then reformulated
by approximating semi-infinite inequality constraints with the S-procedure. For
ES, an alternating optimization (AO)-based algorithm is proposed to jointly
design AP active beamforming and STAR-RIS passive beamforming, where a penalty
method is leveraged in STAR-RIS beamforming design. Furthermore, the developed
algorithm is extended to optimize the time allocation policy and beamforming
vectors in a two-layer iterative manner for TS. Numerical results reveal that:
1) deploying STAR-RISs achieves a significant performance gain over
conventional RISs, especially in terms of harvested power for EUs; 2) the ES
protocol obtains a better user fairness performance when focusing only on IUs
or EUs, while the TS protocol yields a better balance between IUs and EUs; 3)
the imperfect CSI affects IUs more significantly than EUs, whereas TS can
confer a more robust design to attenuate these effects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uplink soft handover for LEO constellations: how strong the
  inter-satellite link should be 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houcem Ben Salem, Alberto Tarable, Alessandro Nordio, Behrooz Makki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a constellation of low-earth-orbit (LEO) satellites connected to
a handheld device on the ground. Due to the very large orbital speed, an
effective handover strategy becomes of paramount importance. In particular, we
study the benefits of soft handover in the uplink from the physical-layer point
of view. We give a realistic model for both the ground-to-satellite and the
inter-satellite links, following the 3GPP channel model for the former. We
suppose that, during handover from a serving satellite to a target satellite,
one of the two satellites forwards the received signal from the ground user to
the other, thus acting as a relay. We quantify through simulations the loss of
hard handover, compared to soft handover. For the latter, we test both
amplify-and-forward (AF) and decode-and-forward (DF) relaying techniques and
verify that, at least in the simulated conditions, DF does not repay, in terms
of block error rate (BLER), the increase of complexity with respect to AF.
Also, we study the effect of the LEO constellation size on the network BLER.
Finally, we show that, with soft handover, the impact of misalignment on the
inter-satellite link is severe, especially at optical frequencies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coexisting Passive RIS and Active Relay Assisted NOMA Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Huang, Li Guo, Xidong Mu, Chao Dong, Yuanwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel coexisting passive reconfigurable intelligent surface (RIS) and
active decode-and-forward (DF) relay assisted non-orthogonal multiple access
(NOMA) transmission framework is proposed. In particular, two communication
protocols are conceived, namely Hybrid NOMA (H-NOMA) and Full NOMA (F-NOMA).
Based on the proposed two protocols, both the sum rate maximization and max-min
rate fairness problems are formulated for jointly optimizing the power
allocation at the access point and relay as well as the passive beamforming
design at the RIS. To tackle the non-convex problems, an alternating
optimization (AO) based algorithm is first developed, where the transmit power
and the RIS phase-shift are alternatingly optimized by leveraging the
two-dimensional search and rank-relaxed difference-of-convex (DC) programming,
respectively. Then, a two-layer penalty based joint optimization (JO) algorithm
is developed to jointly optimize the resource allocation coefficients within
each iteration. Finally, numerical results demonstrate that: i) the proposed
coexisting RIS and relay assisted transmission framework is capable of
achieving a significant user performance improvement than conventional schemes
without RIS or relay; ii) compared with the AO algorithm, the JO algorithm
requires less execution time at the cost of a slight performance loss; and iii)
the H-NOMA and F-NOMA protocols are generally preferable for ensuring user rate
fairness and enhancing user sum rate, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STAR-RIS Assisted Downlink Active and Uplink Backscatter Communications
  with NOMA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Huang, Xidong Mu, Li Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A simultaneously transmitting and reflecting reconfigurable intelligent
surface (STAR-RIS) assisted downlink (DL) active and uplink (UL) backscatter
communication (BackCom) framework is proposed. More particularly, a full-duplex
(FD) base station (BS) communicates with the DL users via the STAR-RIS's
transmission link, while exciting and receiving the information from the UL
BackCom devices with the aid of the STAR-RIS's reflection link. Non-orthogonal
multiple access (NOMA) is exploited in both DL and UL communications for
improving the spectrum efficiency. The system weighted sum rate maximization
problem is formulated for jointly optimizing the FD BS active receive and
transmit beamforming, the STAR- RIS passive beamforming, and the DL NOMA
decoding orders, subject to the DL user's individual rate constraint. To tackle
this challenging non-convex problem, we propose an alternating optimization
(AO) based algorithm for the joint active and passive beamforming design with a
given DL NOMA decoding order. To address the potential high computational
complexity required for exhaustive searching all the NOMA decoding orders, an
efficient NOMA user ordering scheme is further developed. Finally, numerical
results demonstrate that: i) compared with the baseline schemes employing
conventional RISs or space division multiple access, the proposed scheme
achieves higher performance gains; and ii) higher UL rate gain is obtained at a
cost of DL performance degradation, as a remedy, a more flexible performance
tradeoff can be achieved by introducing the STAR-RIS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Range-Angle Estimation for FDA-MIMO System With Frequency Offset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengjiang Sun, Peng Chen, Zhenxin Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frequency diverse array multiple-input multiple-output (FDA-MIMO) radar
differs from the traditional phased array (PA) radar, and can form
range-angle-dependent beampattern and differentiate between closely spaced
targets sharing the same angle but occupying distinct range cells. In the
FDA-MIMO radar, target range estimation is achieved by employing a subtle
frequency variation between adjacent array antennas, so the estimation
performance is degraded severely in a practical scenario with frequency offset.
In this paper, the range-angle estimation problem for FDA-MIMO radar is
considered with frequency offsets in both transmitting and receiving arrays.
First, we build a system model for the FDA-MIMO radar with transmitting and
receiving frequency offsets. Then, the frequency offset is transferred into an
equalized additional noise. The noise characteristics are analyzed in detail
theoretically, together with the influence on the range-angle estimation.
Moreover, since the effect of the transmitting frequency offset is similar to
additional colored noise, denoising algorithms are introduced to mitigate the
performance deterioration caused by the frequency offset. Finally,
Cram\'{e}r-Rao lower bounds (CRLB) for the range-angle estimation are derived
in the scenario with the frequency offsets. Simulation results show the
analysis of frequency offset and the corresponding estimation performance using
different algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Primary Rate Maximization in Movable Antennas Empowered Symbiotic Radio
  Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Lyu, Hao Liu, Wenqing Hong, Shimin Gong, Feng Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a movable antenna (MA) empowered scheme for
symbiotic radio (SR) communication systems. Specifically, multiple antennas at
the primary transmitter (PT) can be flexibly moved to favorable locations to
boost the channel conditions of the primary and secondary transmissions. The
primary transmission is achieved by the active transmission from the PT to the
primary user (PU), while the backscatter device (BD) takes a ride over the
incident signal from the PT to passively send the secondary signal to the PU.
Under this setup, we consider a primary rate maximization problem by jointly
optimizing the transmit beamforming and the positions of MAs at the PT under a
practical bit error rate constraint on the secondary transmission. Then, an
alternating optimization framework with the utilization of the successive
convex approximation, semi-definite processing and simulated annealing (SA)
modified particle swarm optimization (SA-PSO) methods is proposed to find the
solution of the transmit beamforming and MAs' positions. Finally, numerical
results are provided to demonstrate the performance improvement provided by the
proposed MA empowered scheme and the proposed algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE VTC-Spring 2024. 6 Pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secure Outage Analysis for RIS-Aided MISO Systems with Randomly Located
  Eavesdroppers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Shi, Jindan Xu, Wei Xu, Chau Yuen, A. Lee Swindlehurst, Xiaohu You, Chunming Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the physical layer security of an RIS-assisted
multiple-antenna communication system with randomly located eavesdroppers. The
exact distributions of the received signal-to-noise-ratios (SNRs) at the
legitimate user and the eavesdroppers located according to a Poisson point
process (PPP) are derived, and a closed-form expression for the secrecy outage
probability (SOP) is obtained. It is revealed that the secrecy performance is
mainly affected by the number of RIS reflecting elements, and the impact of the
transmit antennas and transmit power at the base station is marginal. In
addition, when the locations of the randomly located eavesdroppers are unknown,
deploying the RIS closer to the legitimate user rather than to the base station
is shown to be more efficient. We also perform an analytical study
demonstrating that the secrecy diversity order depends on the path loss
exponent of the RIS-to-ground links. Finally, numerical simulations are
conducted to verify the accuracy of these theoretical observations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2023 IEEE Globecom Workshops (GC Wkshps). arXiv admin
  note: substantial text overlap with arXiv:2312.16814</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Coded Federated Learning: Privacy Preservation and Straggler
  Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxi Li, Ming Xiao, Mikael Skoglund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we address the problem of federated learning in the presence
of stragglers. For this problem, a coded federated learning framework has been
proposed, where the central server aggregates gradients received from the
non-stragglers and gradient computed from a privacy-preservation global coded
dataset to mitigate the negative impact of the stragglers. However, when
aggregating these gradients, fixed weights are consistently applied across
iterations, neglecting the generation process of the global coded dataset and
the dynamic nature of the trained model over iterations. This oversight may
result in diminished learning performance. To overcome this drawback, we
propose a new method named adaptive coded federated learning (ACFL). In ACFL,
before the training, each device uploads a coded local dataset with additive
noise to the central server to generate a global coded dataset under privacy
preservation requirements. During each iteration of the training, the central
server aggregates the gradients received from the non-stragglers and the
gradient computed from the global coded dataset, where an adaptive policy for
varying the aggregation weights is designed. Under this policy, we optimize the
performance in terms of privacy and learning, where the learning performance is
analyzed through convergence analysis and the privacy performance is
characterized via mutual information differential privacy. Finally, we perform
simulations to demonstrate the superiority of ACFL compared with the
non-adaptive methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beamforming Design for Semantic-Bit Coexisting Communication System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maojun Zhang, Guangxu Zhu, Richeng Jin, Xiaoming Chen, Qingjiang Shi, Caijun Zhong, Kaibin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic communication (SemCom) is emerging as a key technology for future
sixth-generation (6G) systems. Unlike traditional bit-level communication
(BitCom), SemCom directly optimizes performance at the semantic level, leading
to superior communication efficiency. Nevertheless, the task-oriented nature of
SemCom renders it challenging to completely replace BitCom. Consequently, it is
desired to consider a semantic-bit coexisting communication system, where a
base station (BS) serves SemCom users (sem-users) and BitCom users (bit-users)
simultaneously. Such a system faces severe and heterogeneous inter-user
interference. In this context, this paper provides a new semantic-bit
coexisting communication framework and proposes a spatial beamforming scheme to
accommodate both types of users. Specifically, we consider maximizing the
semantic rate for semantic users while ensuring the quality-of-service (QoS)
requirements for bit-users. Due to the intractability of obtaining the exact
closed-form expression of the semantic rate, a data driven method is first
applied to attain an approximated expression via data fitting. With the
resulting complex transcendental function, majorization minimization (MM) is
adopted to convert the original formulated problem into a multiple-ratio
problem, which allows fractional programming (FP) to be used to further
transform the problem into an inhomogeneous quadratically constrained quadratic
programs (QCQP) problem. Solving the problem leads to a semi-closed form
solution with undetermined Lagrangian factors that can be updated by a fixed
point algorithm. Extensive simulation results demonstrate that the proposed
beamforming scheme significantly outperforms conventional beamforming
algorithms such as zero-forcing (ZF), maximum ratio transmission (MRT), and
weighted minimum mean-square error (WMMSE).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ User Training with Error Augmentation for Electromyogram-based Gesture
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07289v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07289v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunus Bicer, Niklas Smedemark-Margulies, Basak Celik, Elifnur Sunger, Ryan Orendorff, Stephanie Naufel, Tales Imbiriba, Deniz Erdoğmuş, Eugene Tunik, Mathew Yarossi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We designed and tested a system for real-time control of a user interface by
extracting surface electromyographic (sEMG) activity from eight electrodes in a
wrist-band configuration. sEMG data were streamed into a machine-learning
algorithm that classified hand gestures in real-time. After an initial model
calibration, participants were presented with one of three types of feedback
during a human-learning stage: veridical feedback, in which predicted
probabilities from the gesture classification algorithm were displayed without
alteration, modified feedback, in which we applied a hidden augmentation of
error to these probabilities, and no feedback. User performance was then
evaluated in a series of minigames, in which subjects were required to use
eight gestures to manipulate their game avatar to complete a task. Experimental
results indicated that, relative to baseline, the modified feedback condition
led to significantly improved accuracy and improved gesture class separation.
These findings suggest that real-time feedback in a gamified user interface
with manipulation of feedback may enable intuitive, rapid, and accurate task
acquisition for sEMG-based gesture recognition applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 10 figures. V2: Fix latex characters in author name. V3:
  Add published DOI and Copyright notice</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Communication of Measurement Anomalies in the
  Smart Grid 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02324v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02324v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Ravi, Anna Scaglione, Sean Peisert, Parth Pradhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a framework based on differential privacy (DP) for
querying electric power measurements to detect system anomalies or bad data.
Our DP approach conceals consumption and system matrix data, while
simultaneously enabling an untrusted third party to test hypotheses of
anomalies, such as the presence of bad data, by releasing a randomized
sufficient statistic for hypothesis-testing. We consider a measurement model
corrupted by Gaussian noise and a sparse noise vector representing the attack,
and we observe that the optimal test statistic is a chi-square random variable.
To detect possible attacks, we propose a novel DP chi-square noise mechanism
that ensures the test does not reveal private information about power
injections or the system matrix. The proposed framework provides a robust
solution for detecting bad data while preserving the privacy of sensitive power
system data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model-informed ECG Dual Attention Network for Heart
  Failure Risk Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Chen, Lei Li, Marcel Beetz, Abhirup Banerjee, Ramneek Gupta, Vicente Grau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heart failure (HF) poses a significant public health challenge, with a rising
global mortality rate. Early detection and prevention of HF could significantly
reduce its impact. We introduce a novel methodology for predicting HF risk
using 12-lead electrocardiograms (ECGs). We present a novel, lightweight
dual-attention ECG network designed to capture complex ECG features essential
for early HF risk prediction, despite the notable imbalance between low and
high-risk groups. This network incorporates a cross-lead attention module and
twelve lead-specific temporal attention modules, focusing on cross-lead
interactions and each lead's local dynamics. To further alleviate model
overfitting, we leverage a large language model (LLM) with a public ECG-Report
dataset for pretraining on an ECG-report alignment task. The network is then
fine-tuned for HF risk prediction using two specific cohorts from the UK
Biobank study, focusing on patients with hypertension (UKB-HYP) and those who
have had a myocardial infarction (UKB-MI).The results reveal that LLM-informed
pre-training substantially enhances HF risk prediction in these cohorts. The
dual-attention design not only improves interpretability but also predictive
accuracy, outperforming existing competitive methods with C-index scores of
0.6349 for UKB-HYP and 0.5805 for UKB-MI. This demonstrates our method's
potential in advancing HF risk assessment with clinical complex ECG data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under journal revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coverage and Rate Analysis for Integrated Sensing and Communication
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Gan, Chongwen Huang, Zhaohui Yang, Xiaoming Chen, Jiguang He, Zhaoyang Zhang, Chau Yuen, Yong Liang Guan, Mérouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrated sensing and communication (ISAC) is increasingly recognized as a
pivotal technology for next-generation cellular networks, offering mutual
benefits in both sensing and communication capabilities. This advancement
necessitates a re-examination of the fundamental limits within networks where
these two functions coexist via shared spectrum and infrastructures. However,
traditional stochastic geometry-based performance analyses are confined to
either communication or sensing networks separately. This paper bridges this
gap by introducing a generalized stochastic geometry framework in ISAC
networks. Based on this framework, we define and calculate the coverage and
ergodic rate of sensing and communication performance under resource
constraints. Then, we shed light on the fundamental limits of ISAC networks by
presenting theoretical results for the coverage rate of the unified
performance, taking into account the coupling effects of dual functions in
coexistence networks. Further, we obtain the analytical formulations for
evaluating the ergodic sensing rate constrained by the maximum communication
rate, and the ergodic communication rate constrained by the maximum sensing
rate. Extensive numerical results validate the accuracy of all theoretical
derivations, and also indicate that denser networks significantly enhance ISAC
coverage. Specifically, increasing the base station density from $1$
$\text{km}^{-2}$ to $10$ $\text{km}^{-2}$ can boost the ISAC coverage rate from
$1.4\%$ to $39.8\%$. Further, results also reveal that with the increase of the
constrained sensing rate, the ergodic communication rate improves
significantly, but the reverse is not obvious.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solution-Set Geometry and Regularization Path of a Nonconvexly
  Regularized Convex Sparse Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhang, Isao Yamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generalized minimax concave (GMC) penalty is a nonconvex sparse
regularizer which can preserve the overall-convexity of the regularized
least-squares problem. In this paper, we focus on a significant instance of the
GMC model termed scaled GMC (sGMC), and present various notable findings on its
solution-set geometry and regularization path. Our investigation indicates that
while the sGMC penalty is a nonconvex extension of the LASSO penalty (i.e., the
$\ell_1$-norm), the sGMC model preserves many celebrated properties of the
LASSO model, hence can serve as a less biased surrogate of LASSO without losing
its advantages. Specifically, for a fixed regularization parameter $\lambda$,
we show that the solution-set geometry, solution uniqueness and sparseness of
the sGMC model can be characterized in a similar elegant way to the LASSO model
(see, e.g., Osborne et al. 2000, R. J. Tibshirani 2013). For a varying
$\lambda$, we prove that the sGMC solution set is a continuous polytope-valued
mapping of $\lambda$. Most noticeably, our study indicates that similar to
LASSO, the minimum $\ell_2$-norm regularization path of the sGMC model is
continuous and piecewise linear in $\lambda$. Based on these theoretical
results, an efficient regularization path algorithm is proposed for the sGMC
model, extending the well-known least angle regression (LARS) algorithm for
LASSO. We prove the correctness and finite termination of the proposed
algorithm under a mild assumption, and confirm its
correctness-in-general-situation, efficiency, and practical utility through
numerical experiments. Many results in this study also contribute to the
theoretical research of LASSO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>53 pages, 10 figures. Submitted to journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detection Is Tracking: Point Cloud Multi-Sweep Deep Learning Models
  Revisited 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingji Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional tracking paradigm takes in instantaneous measurements such as
range and bearing, and produces object tracks across time. In applications such
as autonomous driving, lidar measurements in the form of point clouds are
usually passed through a "virtual sensor" realized by a deep learning model, to
produce "measurements" such as bounding boxes, which are in turn ingested by a
tracking module to produce object tracks. Very often multiple lidar sweeps are
accumulated in a buffer to merge and become the input to the virtual sensor. We
argue in this paper that such an input already contains temporal information,
and therefore the virtual sensor output should also contain temporal
information, not just instantaneous values for the time corresponding to the
end of the buffer. In particular, we present the deep learning model called
MULti-Sweep PAired Detector (MULSPAD) that produces, for each detected object,
a pair of bounding boxes at both the end time and the beginning time of the
input buffer. This is achieved with fairly straightforward changes in commonly
used lidar detection models, and with only marginal extra processing, but the
resulting symmetry is satisfying. Such paired detections make it possible not
only to construct rudimentary trackers fairly easily, but also to construct
more sophisticated trackers that can exploit the extra information conveyed by
the pair and be robust to choices of motion models and object birth/death
models. We have conducted preliminary training and experimentation using Waymo
Open Dataset, which shows the efficacy of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>My previous employer Motional is requiring a review and approval
  process before I can publish this paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning-Based One-Bit Maximum Likelihood Detection for Massive MIMO
  Systems: Dithering-Aided Adaptive Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunseong Cho, Jinseok Choi, Brian L. Evans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a learning-based detection framework for uplink
massive multiple-input and multiple-output (MIMO) systems with one-bit
analog-to-digital converters. The learning-based detection only requires
counting the occurrences of the quantized outputs of -1 and +1 for estimating a
likelihood probability at each antenna. Accordingly, the key advantage of this
approach is to perform maximum likelihood detection without explicit channel
estimation which has been one of the primary challenges of one-bit quantized
systems. However, due to the quasi-deterministic reception in the high
signal-to-noise ratio (SNR) regime, one-bit observations in the high SNR regime
are biased to either +1 or -1, and thus, the learning requires excessive
training to estimate the small likelihood probabilities. To address this
drawback, we propose a dither-and-learning technique to estimate likelihood
functions from dithered signals. First, we add a dithering signal to
artificially decrease the SNR and then infer the likelihood function from the
quantized dithered signals by using an SNR estimate derived from a deep neural
network-based estimator which is trained offline. We extend our technique by
developing an adaptive dither-and-learning method that updates the dithering
power according to the patterns observed in the quantized dithered signals. The
proposed framework is also applied to channel-coded MIMO systems by computing a
bit-wise and user-wise log-likelihood ratio from the refined likelihood
probabilities. Simulation results validate the performance of the proposed
methods in both uncoded and coded systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Transactions on Vehicular
  Technologies</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Predictive Maintenance: Systems, Purposes and Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1912.07383v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1912.07383v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianwen Zhu, Yongyi Ran, Xin Zhou, Yonggang Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper highlights the importance of maintenance techniques in the coming
industrial revolution, reviews the evolution of maintenance techniques, and
presents a comprehensive literature review on the latest advancement of
maintenance techniques, i.e., Predictive Maintenance (PdM), with emphasis on
system architectures, optimization objectives, and optimization methods. In
industry, any outages and unplanned downtime of machines or systems would
degrade or interrupt a company's core business, potentially resulting in
significant penalties and immeasurable reputation and economic loss. Existing
traditional maintenance approaches, such as Reactive Maintenance (RM) and
Preventive Maintenance (PM), suffer from high prevent and repair costs,
inadequate or inaccurate mathematical degradation processes, and manual feature
extraction. The incoming fourth industrial revolution is also demanding for a
new maintenance paradigm to reduce the maintenance cost and downtime, and
increase system availability and reliability. Predictive Maintenance (PdM) is
envisioned the solution. In this survey, we first provide a high-level view of
the PdM system architectures including PdM 4.0, Open System Architecture for
Condition Based Monitoring (OSA-CBM), and cloud-enhanced PdM system. Then, we
review the specific optimization objectives, which mainly comprise cost
minimization, availability/reliability maximization, and multi-objective
optimization. Furthermore, we present the optimization methods to achieve the
aforementioned objectives, which include traditional Machine Learning (ML)
based and Deep Learning (DL) based approaches. Finally, we highlight the future
research directions that are critical to promote the application of DL
techniques in the context of PdM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 23 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Secrecy Performance of RIS-Assisted MISO Systems over Rician Channels
  with Spatially Random Eavesdroppers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Shi, Jindan Xu, Wei Xu, Chau Yuen, A. Lee Swindlehurst, Chunming Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconfigurable intelligent surface (RIS) technology is emerging as a
promising technique for performance enhancement for next-generation wireless
networks. This paper investigates the physical layer security of an
RIS-assisted multiple-antenna communication system in the presence of random
spatially distributed eavesdroppers. The RIS-to-ground channels are assumed to
experience Rician fading. Using stochastic geometry, exact distributions of the
received signal-to-noise-ratios (SNRs) at the legitimate user and the
eavesdroppers located according to a Poisson point process (PPP) are derived,
and closed-form expressions for the secrecy outage probability (SOP) and the
ergodic secrecy capacity (ESC) are obtained to provide insightful guidelines
for system design. First, the secrecy diversity order is obtained as
$\frac{2}{\alpha_2}$, where $\alpha_2$ denotes the path loss exponent of the
RIS-to-ground links. Then, it is revealed that the secrecy performance is
mainly affected by the number of RIS reflecting elements, $N$, and the impact
of the number of transmit antennas and transmit power at the base station is
marginal. In addition, when the locations of the randomly located eavesdroppers
are unknown, deploying the RIS closer to the legitimate user rather than to the
base station is shown to be more efficient. Moreover, it is also found that the
density of randomly located eavesdroppers, $\lambda_e$, has an additive effect
on the asymptotic ESC performance given by
$\log_2{\left({1}/{\lambda_e}\right)}$. Finally, numerical simulations are
conducted to verify the accuracy of these theoretical observations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Wireless Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiscale Hodge Scattering Networks for Data Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Saito, Stefan C. Schonsheck, Eugene Shvarts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose new scattering networks for signals measured on simplicial
complexes, which we call \emph{Multiscale Hodge Scattering Networks} (MHSNs).
Our construction is based on multiscale basis dictionaries on simplicial
complexes, i.e., the $\kappa$-GHWT and $\kappa$-HGLET, which we recently
developed for simplices of dimension $\kappa \in \mathbb{N}$ in a given
simplicial complex by generalizing the node-based Generalized Haar-Walsh
Transform (GHWT) and Hierarchical Graph Laplacian Eigen Transform (HGLET). The
$\kappa$-GHWT and the $\kappa$-HGLET both form redundant sets (i.e.,
dictionaries) of multiscale basis vectors and the corresponding expansion
coefficients of a given signal. Our MHSNs use a layered structure analogous to
a convolutional neural network (CNN) to cascade the moments of the modulus of
the dictionary coefficients. The resulting features are invariant to reordering
of the simplices (i.e., node permutation of the underlying graphs).
Importantly, the use of multiscale basis dictionaries in our MHSNs admits a
natural pooling operation that is akin to local pooling in CNNs, and which may
be performed either locally or per-scale. These pooling operations are harder
to define in both traditional scattering networks based on Morlet wavelets, and
geometric scattering networks based on Diffusion Wavelets. As a result, we are
able to extract a rich set of descriptive yet robust features that can be used
along with very simple machine learning methods (i.e., logistic regression or
support vector machines) to achieve high-accuracy classification systems with
far fewer parameters to train than most modern graph neural networks. Finally,
we demonstrate the usefulness of our MHSNs in three distinct types of problems:
signal classification, domain (i.e., graph/simplex) classification, and
molecular dynamics prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 Pages, Comments Welcome</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Logic in Computer Science
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ (Towards a) Statistical Probabilistic Lazy Lambda Calculus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Radha Jagadeesan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the desiderata on a model for statistical probabilistic programming
languages. We argue that they can be met by a combination of traditional tools,
namely open bisimulation and probabilistic simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flip-width: Cops and Robber on dense graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00352v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00352v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Szymon Toruńczyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We define new graph parameters, called flip-width, that generalize treewidth,
degeneracy, and generalized coloring numbers for sparse graphs, and
clique-width and twin-width for dense graphs. The flip-width parameters are
defined using variants of the Cops and Robber game, in which the robber has
speed bounded by a fixed constant $r\in\mathbb N\cup\{\infty\}$, and the cops
perform flips (or perturbations) of the considered graph. We then propose a new
notion of tameness of a graph class, called bounded flip-width, which is a
dense counterpart of classes of bounded expansion of Ne\v{s}etril and Ossona de
Mendez, and includes classes of bounded twin-width of Bonnet, Kim,
Thomass{\'e}, and Watrigant. This unifies Sparsity Theory and Twin-width
Theory, providing a common language for studying the central notions of the two
theories, such as weak coloring numbers and twin-width -- corresponding to
winning strategies of one player -- or dense shallow minors, rich divisions, or
well-linked sets, corresponding to winning strategies of the other player. We
prove that boundedness of flip-width is preserved by first-order
interpretations, or transductions, generalizing previous results concerning
classes of bounded expansion and bounded twin-width. We provide an algorithm
approximating the flip-width of a given graph, which runs in slicewise
polynomial time (XP) in the size of the graph. Finally, we propose a more
general notion of tameness, called almost bounded flip-width, which is a dense
counterpart of nowhere dense classes. We conjecture, and provide evidence, that
classes with almost bounded flip-width coincide with monadically dependent (or
monadically NIP) classes, introduced by Shelah in model theory. We also provide
evidence that classes of almost bounded flip-width characterise the hereditary
graph classes for which the model-checking problem is fixed-parameter
tractable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>80 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inferentialist Resource Semantics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander V. Gheorghiu, Tao Gu, David J. Pym
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In systems modelling, a system typically comprises located resources relative
to which processes execute. One important use of logic in informatics is in
modelling such systems for the purpose of reasoning (perhaps automated) about
their behaviour and properties. To this end, one requires an interpretation of
logical formulae in terms of the resources and states of the system; such an
interpretation is called a resource semantics of the logic. This paper shows
how inferentialism -- the view that meaning is given in terms of inferential
behaviour -- enables a versatile and expressive framework for resource
semantics. Specifically, how inferentialism seamlessly incorporates the
assertion-based approach of the logic of Bunched Implications, foundational in
program verification (e.g., as the basis of Separation Logic), and the renowned
number-of-uses reading of Linear Logic. This integration enables reasoning
about shared and separated resources in intuitive and familiar ways, as well as
about the composition and interfacing of system components.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Note on a Translation from First-Order Logic into the Calculus of
  Relations Preserving Validity and Finite Validity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoshiki Nakamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this note, we give a linear-size translation from formulas of first-order
logic into equations of the calculus of relations preserving validity and
finite validity. Our translation also gives a linear-size conservative
reduction from formulas of first-order logic into formulas of the
three-variable fragment of first-order logic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Metric Temporal Logic for Continuous Stochastic Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.00984v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.00984v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitsumasa Ikeda, Yoriyuki Yamagata, Takayuki Kihara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we prove measurability of event for which a general
continuous-time stochastic process satisfies continuous-time Metric Temporal
Logic (MTL) formula. Continuous-time MTL can define temporal constrains for
physical system in natural way. Then there are several researches that deal
with probability of continuous MTL semantics for stochastic processes. However,
proving measurability for such events is by no means an obvious task, even
though it is essential. The difficulty comes from the semantics of "until
operator", which is defined by logical sum of uncountably many propositions.
Given the difficulty involved in proving the measurability of such an event
using classical measure-theoretic methods, we employ a theorem from stochastic
analysis. This theorem is utilized to prove the measurability of hitting times
for stochastic processes, and it stands as a profound result within the theory
of capacity. Next, we provide an example that illustrates the failure of
probability approximation when discretizing the continuous semantics of MTL
formulas with respect to time. Additionally, we prove that the probability of
the discretized semantics converges to that of the continuous semantics when we
impose restrictions on diamond operators to prevent nesting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revised based on comments from anonymous LMCS referees</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-21T00:00:00Z">2024-03-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Signal Processing
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crowdsourced Multilingual Speech Intelligibility Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Lechler, Kamil Wojcicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of generative audio features, there is an increasing need for
rapid evaluation of their impact on speech intelligibility. Beyond the existing
laboratory measures, which are expensive and do not scale well, there has been
comparatively little work on crowdsourced assessment of intelligibility.
Standards and recommendations are yet to be defined, and publicly available
multilingual test materials are lacking. In response to this challenge, we
propose an approach for a crowdsourced intelligibility assessment. We detail
the test design, the collection and public release of the multilingual speech
data, and the results of our early experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RIS-Aided Cooperative Mobile Edge Computing: Computation Efficiency
  Maximization via Joint Uplink and Downlink Resource Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenrong Liu, Zongze Li, Yi Gong, Yik-Chung Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In mobile edge computing (MEC) systems, the wireless channel condition is a
critical factor affecting both the communication power consumption and
computation rate of the offloading tasks. This paper exploits the idea of
cooperative transmission and employing reconfigurable intelligent surface (RIS)
in MEC to improve the channel condition and maximize computation efficiency
(CE). The resulting problem couples various wireless resources in both uplink
and downlink, which calls for the joint design of the user association,
receive/downlink beamforming vectors, transmit power of users, task partition
strategies for local computing and offloading, and uplink/downlink phase shifts
at the RIS. To tackle the challenges brought by the combinatorial optimization
problem, the group sparsity structure of the beamforming vectors determined by
user association is exploited. Furthermore, while the CE does not explicitly
depend on the downlink phase shifts, instead of simply finding a feasible
solution, we exploit the hidden relationship between them and convert this
relationship into an explicit form for optimization. Then the resulting problem
is solved via the alternating maximization framework, and the nonconvexity of
each subproblem is handled individually. Simulation results show that
cooperative transmission and RIS deployment can significantly improve the CE
and demonstrate the importance of optimizing the downlink phase shifts with an
explicit form.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication in IEEE Transactions on
  Wireless Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiple and Gyro-Free Inertial <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeev Yampolsky, Yair Stolero, Nitzan Pri-Hadash, Dan Solodar, Shira Massas, Itai Savin, Itzik Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An inertial navigation system (INS) utilizes three orthogonal accelerometers
and gyroscopes to determine platform position, velocity, and orientation. There
are countless applications for INS, including robotics, autonomous platforms,
and the internet of things. Recent research explores the integration of
data-driven methods with INS, highlighting significant innovations, improving
accuracy and efficiency. Despite the growing interest in this field and the
availability of INS datasets, no datasets are available for gyro-free INS
(GFINS) and multiple inertial measurement unit (MIMU) architectures. To fill
this gap and to stimulate further research in this field, we designed and
recorded GFINS and MIMU datasets using 54 inertial sensors grouped in nine
inertial measurement units. These sensors can be used to define and evaluate
different types of MIMU and GFINS architectures. The inertial sensors were
arranged in three different sensor configurations and mounted on a mobile robot
and a passenger car. In total, the dataset contains 35 hours of inertial data
and corresponding ground truth trajectories. The data and code are freely
accessible through our GitHub repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 16 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Galileo OSNMA Time To First Authenticated Fix 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleix Galan, Ignacio Fernandez-Hernandez, Wim De Wilde, Sofie Pollin, Gonzalo Seco-Granados
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Galileo is the first global navigation satellite system to authenticate their
civilian signals through the Open Service Galileo Message Authentication
(OSNMA) protocol. However, OSNMA delays the time to obtain a first position and
time fix, the so-called Time To First Authentication Fix (TTFAF). Reducing the
TTFAF as much as possible is crucial to integrate the technology seamlessly
into the current products. In the cases where the receiver already has
cryptographic data available, the so-called hot start mode and focus of this
article, the currently available implementations achieve an average TTFAF of
around 100 seconds in ideal environments. In this work, we dissect the TTFAF
process, propose two main optimizations to reduce the TTFAF, and benchmark them
in three distinct scenarios (open-sky, soft urban, and hard urban) with
recorded real data. Moreover, we evaluate the optimizations using the synthetic
scenario from the official OSNMA test vectors. The first block of optimizations
centers on extracting as much information as possible from broken sub-frames by
processing them at page level and combining redundant data from multiple
satellites. The second block of optimizations aims to reconstruct missed
navigation data by using fields in the authentication tags belonging to the
same sub-frame as the authentication key. Combining both optimizations improves
the TTFAF substantially for all considered scenarios. We obtain an average
TTFAF of 60.9 and 68.8 seconds for the test vectors and the open-sky scenario,
respectively, with a best-case of 44.0 seconds in both. Likewise, the urban
scenarios see a drastic reduction of the average TTFAF between the
non-optimized and optimized cases, from 127.5 to 87.5 seconds in the soft urban
scenario and from 266.1 to 146.1 seconds in the hard urban scenario. These
optimizations are available as part of the open-source OSNMAlib library on
GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible. 12 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimization for MIMO Integrated Sensing and Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lie-Liang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fundamentals of MIMO communications and MIMO sensing are firstly analyzed
with regard to channel and sensing capacities. It is shown that the different
objectives of communications and sensing lead to different signaling waveforms
required for achieving their capacities. Hence, the optimization of integrated
sensing and communications (ISAC) is relied on a trade-off expected between the
performance of communications and that of sensing. Following this observation,
the design and resource optimization in general MIMO ISAC systems are discussed
along with the analysis of some existing ISAC schemes. Furthermore, the design
of ISAC in mmWave communications is addressed. Specifically, the principle of
sensing in mmWave systems is established, and a range of optimization
alternatives for ISAC design in mmWave systems are reviewed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Book chapter, 22 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bistatic Doppler Frequency Estimation with Asynchronous Moving Devices
  for Integrated Sensing and Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianmaria Ventura, Zaman Bhalli, Michele Rossi, Jacopo Pegoraro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this letter, we present for the first time a method to estimate the
bistatic Doppler frequency of a target with clock asynchronous and mobile
Integrated Sensing And Communication (ISAC) devices. Existing approaches have
separately tackled the presence of phase offsets due to clock asynchrony or the
additional Doppler shift due to device movement. However, in real ISAC
scenarios, these two sources of phase nuisance are concurrently present, making
the estimation of the target's Doppler frequency particularly challenging. Our
method solves the problem using the sole wireless signal at the receiver, by
computing Channel Impulse Response (CIR) phase differences across different
multipath components and subsequent time instants. In this way, we cancel out
phase offsets. Then, we construct a system of equations that allows
disentangling the target's Doppler frequency from that of the moving device.
The proposed method is validated via simulation, exploring the impact of
different system parameters. Numerical results show that our approach is a
viable way of estimating Doppler frequency in bistatic asynchronous ISAC
scenarios with mobile devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constellation Shaping under Phase Noise Impairment for Sub-THz
  Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12433v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12433v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dileepa Marasinghe, Le Hang Nguyen, Jafar Mohammadi, Yejian Chen, Thorsten Wild, Nandana Rajatheva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large untapped spectrum in the sub-THz allows for ultra-high throughput
communication to realize many seemingly impossible applications in 6G. One of
the challenges in radio communications in sub-THz is the hardware impairments.
Specifically, phase noise is one key hardware impairment, which is accentuated
as we increase the frequency and bandwidth. Furthermore, the moderate output
power of the sub-THz power amplifier demands limits on peak to average power
ratio (PAPR) signal design. Single carrier frequency domain equalization
(SC-FDE) has been identified as a suitable candidate for sub-THz, although some
challenges such as phase noise and PAPR still remain to be tackled. In this
work, we design a phase noise robust, modest PAPR SC waveform by geometrically
shaping the constellation under practical conditions. We formulate the waveform
optimization problem in its augmented Lagrangian form and use a
back-propagation-inspired technique to obtain a constellation design that is
numerically robust to phase noise, while maintaining a relatively low PAPR
compared to the conventional waveforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE ICC 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Logic in Computer Science
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PECR: A formal system based on computability logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        G. Pantelis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PECR is a formal system designed to explore the properties of computability
of programs on a real-world computer. As such PECR incorporates the finite
resources of the machine upon which a program is to be executed. The main
features of the formal system will be presented and its practical applications
will be discussed. Of particular interest is the implementation of the formal
system to the exploration of the laws of nature that lead to rigorous
constructions of computer models of real-world phenomena.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fully Evaluated Left-Sequential Logics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alban Ponse, Daan J. C. Staudt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a family of two-valued "fully evaluated left-sequential logics"
(FELs), of which Free FEL (defined by Staudt in 2012) is most distinguishing
(weakest) and immune to atomic side effects. Next is Memorising FEL, in which
evaluations of subexpressions are memorised. The following stronger logic is
Conditional FEL (inspired by Guzm\'an and Squier's Conditional logic, 1990).
The strongest FEL is static FEL, a sequential version of propositional logic.
We use evaluation trees as a simple, intuitive semantics and provide complete
axiomatisations for closed terms (left-sequential propositional expressions).
  For each FEL except Static FEL, we also define its three-valued version, with
a constant U for "undefinedness" and again provide complete, independent
aziomatisations, each one containing two additional axioms for U on top of the
axiomatisations of the two-valued case. In this setting, the strongest FEL is
equivalent to Bochvar's strict logic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 5 tables, three appendices. As mentioned, the text of
  Section 2 on pp.6-14, the quote on p.30 and Appendix A are taken from
  arXiv:1206.1936 (written by Staudt)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complexity of the Model Checking problem for inquisitive propositional
  and modal logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca Grilletti, Ivano Ciardelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this paper is to study the complexity of the model checking
problem MC for inquisitive propositional logic InqB and for inquisitive modal
logic InqM, that is, the problem of deciding whether a given finite structure
for the logic satisfies a given formula. In recent years, this problem has been
thoroughly investigated for several variations of dependence and teams logics,
systems closely related to inquisitive logic. Building upon some ideas
presented by Yang, we prove that the model checking problems for InqB and InqM
are both AP-complete.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining Type Checking and Set Constraint Solving to Improve Automated
  Software Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.01713v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.01713v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximiliano Cristiá, Gianfranco Rossi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we show how prescritive type checking and constraint solving
can be combined to increase automation during software verification. We do so
by defining a type system and implementing a typechecker for {log} (read
`setlog'), a Constraint Logic Programming (CLP) language and satisfiability
solver based on set theory. Hence, we proceed as follows: a) a type system for
{log} is defined; b) the constraint solver is proved to be safe w.r.t. the type
system; c) the implementation of a concrete typechecker is presented; d) the
integration of type checking and set constraint solving to increase automation
during software verification is discussed; and f) two industrial-strength case
studies are presented where this combination is used with very good results.
  Under consideration in Theory and Practice of Logic Programming (TPLP)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under consideration in Theory and Practice of Logic Programming
  (TPLP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The addition of temporal neighborhood makes the logic of prefixes and
  sub-intervals EXPSPACE-complete 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07881v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07881v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        L. Bozzelli, A. Montanari, A. Peron, P. Sala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A classic result by Stockmeyer gives a non-elementary lower bound to the
emptiness problem for star-free generalized regular expressions. This result is
intimately connected to the satisfiability problem for interval temporal logic,
notably for formulas that make use of the so-called chop operator. Such an
operator can indeed be interpreted as the inverse of the concatenation
operation on regular languages, and this correspondence enables reductions
between non-emptiness of star-free generalized regular expressions and
satisfiability of formulas of the interval temporal logic of chop under the
homogeneity assumption. In this paper, we study the complexity of the
satisfiability problem for suitable weakenings of the chop interval temporal
logic, that can be equivalently viewed as fragments of Halpern and Shoham
interval logic. We first consider the logic $\mathsf{BD}_{hom}$ featuring
modalities $B$, for \emph{begins}, corresponding to the prefix relation on
pairs of intervals, and $D$, for \emph{during}, corresponding to the infix
relation. The homogeneous models of $\mathsf{BD}_{hom}$ naturally correspond to
languages defined by restricted forms of regular expressions, that use union,
complementation, and the inverses of the prefix and infix relations. Such a
fragment has been recently shown to be PSPACE-complete . In this paper, we
study the extension $\mathsf{BD}_{hom}$ with the temporal neighborhood modality
$A$ (corresponding to the Allen relation \emph{Meets}), and prove that it
increases both its expressiveness and complexity. In particular, we show that
the resulting logic $\mathsf{BDA}_{hom}$ is EXPSPACE-complete.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-20T00:00:00Z">2024-03-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Logic in Computer Science
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The equational theory of the Weihrauch lattice with multiplication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eike Neumann, Arno Pauly, Cécilia Pradic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the equational theory of the Weihrauch lattice with multiplication,
meaning the collection of equations between terms built from variables, the
lattice operations $\sqcup$, $\sqcap$, the product $\times$, and the finite
parallelization $(-)^*$ which are true however we substitute Weihrauch degrees
for the variables. We provide a combinatorial description of these in terms of
a reducibility between finite graphs, and moreover, show that deciding which
equations are true in this sense is complete for the third level of the
polynomial hierarchy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Database Dependencies and Formal Concept Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaume Baixeries
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This is an account of the characterization of database dependencies with
Formal Concept Analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyper Strategy Logic <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raven Beutner, Bernd Finkbeiner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Strategy logic (SL) is a powerful temporal logic that enables strategic
reasoning in multi-agent systems. SL supports explicit (first-order)
quantification over strategies and provides a logical framework to express many
important properties such as Nash equilibria, dominant strategies, etc. While
in SL the same strategy can be used in multiple strategy profiles, each such
profile is evaluated w.r.t. a path-property, i.e., a property that considers
the single path resulting from a particular strategic interaction. In this
paper, we present Hyper Strategy Logic (HyperSL), a strategy logic where the
outcome of multiple strategy profiles can be compared w.r.t. a hyperproperty,
i.e., a property that relates multiple paths. We show that HyperSL can capture
important properties that cannot be expressed in SL, including
non-interference, quantitative Nash equilibria, optimal adversarial planning,
and reasoning under imperfect information. On the algorithmic side, we identify
an expressive fragment of HyperSL with decidable model checking and present a
model-checking algorithm. We contribute a prototype implementation of our
algorithm and report on encouraging experimental results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAMAS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming Differentiable Logics with Coq Formalisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reynald Affeldt, Alessandro Bruni, Ekaterina Komendantskaya, Natalia Ślusarz, Kathrin Stark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For performance and verification in machine learning, new methods have
recently been proposed that optimise learning systems to satisfy formally
expressed logical properties. Among these methods, differentiable logics (DLs)
are used to translate propositional or first-order formulae into loss functions
deployed for optimisation in machine learning. At the same time, recent
attempts to give programming language support for verification of neural
networks showed that DLs can be used to compile verification properties to
machine-learning backends. This situation is calling for stronger guarantees
about the soundness of such compilers, the soundness and compositionality of
DLs, and the differentiability and performance of the resulting loss functions.
In this paper, we propose an approach to formalise existing DLs using the
Mathematical Components library in the Coq proof assistant. Thanks to this
formalisation, we are able to give uniform semantics to otherwise disparate
DLs, give formal proofs to existing informal arguments, find errors in previous
work, and provide formal proofs to missing conjectured properties. This work is
meant as a stepping stone for the development of programming language support
for verification of machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference Submission (ITP'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OSVAuto: semi-automatic verifier for functional specifications of
  operating systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulun Wu, Bohua Zhan, Bican Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the design and implementation of a tool for semi-automatic
verification of functional specifications of operating system modules. Such
verification tasks are traditionally done in interactive theorem provers, where
the functionalities of the module are specified at abstract and concrete levels
using data such as structures, algebraic datatypes, arrays, maps and so on. In
this work, we provide encodings to SMT for these commonly occurring data types.
This allows verification conditions to be reduced into a form suitable for SMT
solvers. The use of SMT solvers combined with a tactic language allows
semi-automatic verification of the specification. We apply the tool to verify
functional specification for key parts of the uC-OS/II operating system, based
on earlier work giving full verification of the system in Coq. We demonstrate a
large reduction in the amount of human effort due to increased level of
automation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning about distributive laws in a concurrent refinement algebra 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Larissa A. Meinicke, Ian J. Hayes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributive laws are important for algebraic reasoning in arithmetic and
logic. They are equally important for algebraic reasoning about concurrent
programs. In existing theories such as Concurrent Kleene Algebra, only partial
correctness is handled, and many of its distributive laws are weak, in the
sense that they are only refinements in one direction, rather than equalities.
The focus of this paper is on strengthening our theory to support the proof of
strong distributive laws that are equalities, and in doing so come up with laws
that are quite general. Our concurrent refinement algebra supports total
correctness by allowing both finite and infinite behaviours. It supports the
rely/guarantee approach of Jones by encoding rely and guarantee conditions as
rely and guarantee commands. The strong distributive laws may then be used to
distribute rely and guarantee commands over sequential compositions and into
(and out of) iterations. For handling data refinement of concurrent programs,
strong distributive laws are essential.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 1 Figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanized HOL Reasoning in Set Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Guilloud, Sankalp Gambhir, Andrea Gilot, Viktor Kunčak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a mechanized embedding of higher-order logic (HOL) and algebraic
data types (ADT) into first-order logic with ZFC axioms. We implement this in
the Lisa proof assistant for schematic first-order logic and its library based
on axiomatic set theory. HOL proof steps are implemented as proof producing
tactics in Lisa, and the types are interpreted as sets, with function (or
arrow) types coinciding with set-theoretic function spaces. The embedded HOL
proofs, as opposed to being a layer over the existing proofs, are interoperable
with the existing library. This yields a form of soft type system supporting
top-level polymorphism and ADTs over set theory, and offer tools to reason
about functions in set theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Semantic Search Engine for Mathlib4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxiong Gao, Haocheng Ju, Jiedong Jiang, Zihan Qin, Bin Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The interactive theorem prover, Lean, enables the verification of formal
mathematical proofs and is backed by an expanding community. Central to this
ecosystem is its mathematical library, mathlib4, which lays the groundwork for
the formalization of an expanding range of mathematical theories. However,
searching for theorems in mathlib4 can be challenging. To successfully search
in mathlib4, users often need to be familiar with its naming conventions or
documentation strings. Therefore, creating a semantic search engine that can be
used easily by individuals with varying familiarity with mathlib4 is very
important. In this paper, we present a semantic search engine for mathlib4 that
accepts informal queries and finds the relevant theorems. We also establish a
benchmark for assessing the performance of various search engines for mathlib4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Expressive Power of <span class="highlight-title">Transformer</span>s with Chain of Thought <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07923v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07923v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Merrill, Ashish Sabharwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent theoretical work has identified surprisingly simple reasoning
problems, such as checking if two nodes in a graph are connected or simulating
finite-state machines, that are provably unsolvable by standard transformers
that answer immediately after reading their input. However, in practice,
transformers' reasoning can be improved by allowing them to use a "chain of
thought" or "scratchpad", i.e., generate and condition on a sequence of
intermediate tokens before answering. Motivated by this, we ask: Does such
intermediate generation fundamentally extend the computational power of a
decoder-only transformer? We show that the answer is yes, but the amount of
increase depends crucially on the amount of intermediate generation. For
instance, we find that transformer decoders with a logarithmic number of
decoding steps (w.r.t. the input length) push the limits of standard
transformers only slightly, while a linear number of decoding steps, assuming a
slight generalization to standard pre-norm, adds a clear new ability (under
standard complexity conjectures): recognizing all regular languages. Our
results also imply that linear steps keep transformer decoders within
context-sensitive languages, and polynomial steps with generalized pre-norm
make them recognize exactly the class of polynomial-time solvable problems --
the first exact characterization of a type of transformers in terms of standard
complexity classes. Together, our results provide a nuanced framework for
understanding how the length of a transformer's chain of thought or scratchpad
impacts its reasoning power.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9-page preprint. Updated March 20 after ICLR acceptance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-accessible localizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.06670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.06670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J. Daniel Christensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a 2005 paper, Casacuberta, Scevenels and Smith construct a homotopy
idempotent functor $E$ on the category of simplicial sets with the property
that whether it can be expressed as localization with respect to a map $f$ is
independent of the ZFC axioms. We show that this construction can be carried
out in homotopy type theory. More precisely, we give a general method of
associating to a suitable (possibly large) family of maps, a reflective
subuniverse of any universe $\mathcal{U}$. When specialized to an appropriate
family, this produces a localization which when interpreted in the
$\infty$-topos of spaces agrees with the localization corresponding to $E$. Our
approach generalizes the approach of [CSS] in two ways. First, by working in
homotopy type theory, our construction can be interpreted in any
$\infty$-topos. Second, while the local objects produced by [CSS] are always
1-types, our construction can produce $n$-types, for any $n$. This is new, even
in the $\infty$-topos of spaces. In addition, by making use of universes, our
proof is very direct.
  Along the way, we prove many results about "small" types that are of
independent interest. As an application, we give a new proof that separated
localizations exist. We also give results that say when a localization with
respect to a family of maps can be presented as localization with respect to a
single map, and show that the simplicial model satisfies a strong form of the
axiom of choice which implies that sets cover and that the law of excluded
middle holds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages; to appear in Journal of Topology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Algorithms for Verification of Markov Decision Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09184v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09184v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomáš Brázdil, Krishnendu Chatterjee, Martin Chmelik, Vojtěch Forejt, Jan Křetínský, Marta Kwiatkowska, Tobias Meggendorfer, David Parker, Mateusz Ujma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a general framework for applying learning algorithms and
heuristical guidance to the verification of Markov decision processes (MDPs).
The primary goal of our techniques is to improve performance by avoiding an
exhaustive exploration of the state space, instead focussing on particularly
relevant areas of the system, guided by heuristics. Our work builds on the
previous results of Br{\'{a}}zdil et al., significantly extending it as well as
refining several details and fixing errors.
  The presented framework focuses on probabilistic reachability, which is a
core problem in verification, and is instantiated in two distinct scenarios.
The first assumes that full knowledge of the MDP is available, in particular
precise transition probabilities. It performs a heuristic-driven partial
exploration of the model, yielding precise lower and upper bounds on the
required probability. The second tackles the case where we may only sample the
MDP without knowing the exact transition dynamics. Here, we obtain
probabilistic guarantees, again in terms of both the lower and upper bounds,
which provides efficient stopping criteria for the approximation. In
particular, the latter is an extension of statistical model-checking (SMC) for
unbounded properties in MDPs. In contrast to other related approaches, we do
not restrict our attention to time-bounded (finite-horizon) or discounted
properties, nor assume any particular structural properties of the MDP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finite Hil<span class="highlight-title">bert</span> systems for Weak Kleene logics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03265v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03265v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vitor Greati, Sérgio Marcelino, Umberto Rivieccio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple-conclusion Hilbert-style systems allow us to finitely axiomatize
every logic defined by a finite matrix. Having obtained such axiomatizations
for Paraconsistent Weak Kleene and Bochvar-Kleene logics, we modify them by
replacing the multiple-conclusion rules with carefully selected
single-conclusion ones. In this way we manage to introduce the first finite
Hilbert-style single-conclusion axiomatizations for these logics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Corrections on Def.2, Def.3 (PWK system) and Rem. 4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a theory of natural directed paths 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02792v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02792v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philippe Gaucher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the abstract setting of presheaf category on a thick category of
cubes. Precubical sets, symmetric transverse sets, symmetric precubical sets
and the new category of (non-symmetric) transverse sets are examples of this
structure. All these presheaf categories share the same metric and homotopical
properties from a directed homotopy point of view. This enables us to extend
Raussen's notion of natural $d$-path for each of them. Finally, we adapt
Ziemia\'{n}ski's notion of cube chain to this abstract setting and we prove
that it has the expected behavior on precubical sets. As an application, we
verify that the formalization of the parallel composition with synchronization
of process algebra using the coskeleton functor of the category of symmetric
transverse sets has a category of cube chains with the correct homotopy type.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages; Section 3 relies on arXiv:2209.02667; v2: some maps of cube
  chains were missing (see Theorem 4.5 and Corollary 4.6); v3 new numbering of
  the theorems in arXiv:2209.02667</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-03-28T05:25:10.067537608Z">
            2024-03-28 05:25:10 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
